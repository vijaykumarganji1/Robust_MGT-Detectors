{"original_text": "Recent work by (,) looks at semantic parser induction and question answering in the domain of source code libraries and APIs. In this brief note, we formalize the representations being learned in these studies and introduce a simple domain specific language and a systematic translation from this language to first-order logic. By recasting the target representations in terms of classical logic, we aim to broaden the applicability of existing code datasets for investigating more complex natural language understanding and reasoning problems in the software domain.", "label": "1", "is_selected": "0", "text": "Recent work by (,) looks at semantic parser induction and question answering in the domain of source code libraries and APIs. In this brief note, we formalize the representations being learned in these studies and introduce a simple domain specific language and a systematic translation from this language to first-order logic. By recasting the target representations in terms of classical logic, we aim to broaden the applicability of existing code datasets for investigating more complex natural language understanding and reasoning problems in the software domain."}
{"original_text": "The digital identity problem is a complex one in large part because it involves personal data, the algorithms which compute reputations on the data and the management of the identifiers that are linked to personal data. The reality of today is that personal data of an individual is distributed throughout the Internet, in both private and public institutions, and increasingly also on the user's devices. In order to empower individuals to have a say in who has access to their personal data and to enable individuals to make use of their data for their own purposes, a coherent and scalable access authorization architecture is required. Such an architecture must allow different data holders, data providers and user-content generators to respond to an individual's wishes with regards to consent in a federated fashion. This federation must allow an individual to easily manage access policies and provide consent as required by current and forthcoming data privacy regulations. This paper describes the User Managed Access (UMA) architecture and protocols that provide the foundation for scalable access authorization.", "label": "1", "is_selected": "0", "text": "The digital identity problem is a complex one in large part because it involves personal data, the algorithms which compute reputations on the data and the management of the identifiers that are linked to personal data. The reality of today is that personal data of an individual is distributed throughout the Internet, in both private and public institutions, and increasingly also on the user's devices. In order to empower individuals to have a say in who has access to their personal data and to enable individuals to make use of their data for their own purposes, a coherent and scalable access authorization architecture is required. Such an architecture must allow different data holders, data providers and user-content generators to respond to an individual's wishes with regards to consent in a federated fashion. This federation must allow an individual to easily manage access policies and provide consent as required by current and forthcoming data privacy regulations. This paper describes the User Managed Access (UMA) architecture and protocols that provide the foundation for scalable access authorization."}
{"original_text": "In human-in-the-loop machine learning, the user provides information beyond that in the training data. Many algorithms and user interfaces have been designed to optimize and facilitate this human-machine interaction; however, there are still challenges and limitations to this approach. One major challenge is ensuring that the user inputs are accurate and reliable, as the quality of the input data can heavily impact the performance of the machine learning model. Additionally, there may be biases or inconsistencies in the user inputs, which can also affect the model's accuracy. Another challenge is the potential for human error or bias in the feedback provided to the machine learning algorithm. This can occur if the user has their own subjective understanding of the expected outcome or if they unintentionally provide misleading feedback. Lastly, the process of human-in-the-loop machine learning can be time consuming and costly, as it typically requires significant resources and human effort. This may not be feasible for all projects or organizations. Overall, while human-in-the-loop machine learning can be a powerful tool, it is important to carefully consider these challenges and drawbacks so that they can be mitigated or addressed as needed.", "label": "0", "is_selected": "1", "text": "Human-in-the-loop machine learning has the potential to improve many aspects of our daily lives, but there are still challenges and drawbacks to this approach."}
{"original_text": "This paper presents a tool for addressing a key component in many algorithms for planning robot trajectories under uncertainty: evaluation of the safety of a robot whose actions are governed by a control policy. The tool is called SafeMotion, and it uses a combination of numerical optimization and simulation-based methods to analyze the safety of a robot motion over a specified time horizon. SafeMotion takes as input a control policy and a set of initial states, and it outputs a safety metric that reflects the probability of the robot colliding with obstacles or going beyond specified physical limits during the execution of the motion. One of the interesting features of SafeMotion is its ability to handle non-deterministic behaviors and uncertain environments. SafeMotion can evaluate the safety of a motion under different scenarios where the robot encounters different configurations of obstacles or disturbances. This allows SafeMotion users to gain a more complete understanding of the safety of a control policy under different conditions and to determine the worst-case scenarios that must be avoided. Overall, SafeMotion can be a valuable tool for robotics researchers and practitioners who are developing and testing algorithms for planning safe and efficient motions for robots in complex and uncertain environments.", "label": "0", "is_selected": "0", "text": "This paper presents a tool for addressing a key component in many algorithms for planning robot trajectories under uncertainty: evaluation of the safety of a robot whose actions are governed by a control policy. The tool is called SafeMotion, and it uses a combination of numerical optimization and simulation-based methods to analyze the safety of a robot motion over a specified time horizon. SafeMotion takes as input a control policy and a set of initial states, and it outputs a safety metric that reflects the probability of the robot colliding with obstacles or going beyond specified physical limits during the execution of the motion. One of the interesting features of SafeMotion is its ability to handle non-deterministic behaviors and uncertain environments. SafeMotion can evaluate the safety of a motion under different scenarios where the robot encounters different configurations of obstacles or disturbances. This allows SafeMotion users to gain a more complete understanding of the safety of a control policy under different conditions and to determine the worst-case scenarios that must be avoided. Overall, SafeMotion can be a valuable tool for robotics researchers and practitioners who are developing and testing algorithms for planning safe and efficient motions for robots in complex and uncertain environments."}
{"original_text": "Batch normalization (BN) has become a standard technique for training the modern deep networks. However, its effectiveness diminishes when the batch size becomes smaller, since the batch statistics estimation becomes inaccurate. That hinders batch normalization's usage for 1) training larger model which requires small batches constrained by memory consumption, 2) training on mobile or embedded devices of which the memory resource is limited. In this paper, we propose a simple but effective method, called extended batch normalization (EBN). For NCHW format feature maps, extended batch normalization computes the mean along the (N, H, W) dimensions, as the same as batch normalization, to maintain the advantage of batch normalization. To alleviate the problem caused by small batch size, extended batch normalization computes the standard deviation along the (N, C, H, W) dimensions, thus enlarges the number of samples from which the standard deviation is computed. We compare extended batch normalization with batch normalization and group normalization on the datasets of MNIST, CIFAR-10100, STL-10, and ImageNet, respectively. The experiments show that extended batch normalization alleviates the problem of batch normalization with small batch size while achieving close performances to batch normalization with large batch size.", "label": "1", "is_selected": "0", "text": "Batch normalization (BN) has become a standard technique for training the modern deep networks. However, its effectiveness diminishes when the batch size becomes smaller, since the batch statistics estimation becomes inaccurate. That hinders batch normalization's usage for 1) training larger model which requires small batches constrained by memory consumption, 2) training on mobile or embedded devices of which the memory resource is limited. In this paper, we propose a simple but effective method, called extended batch normalization (EBN). For NCHW format feature maps, extended batch normalization computes the mean along the (N, H, W) dimensions, as the same as batch normalization, to maintain the advantage of batch normalization. To alleviate the problem caused by small batch size, extended batch normalization computes the standard deviation along the (N, C, H, W) dimensions, thus enlarges the number of samples from which the standard deviation is computed. We compare extended batch normalization with batch normalization and group normalization on the datasets of MNIST, CIFAR-10100, STL-10, and ImageNet, respectively. The experiments show that extended batch normalization alleviates the problem of batch normalization with small batch size while achieving close performances to batch normalization with large batch size."}
{"original_text": "How far and how fast does information spread in social media? Researchers have recently examined a number of factors that affect information diffusion in online social networks, including: the novelty of information, users' activity levels, who they pay attention to, and how they respond to friends' recommendations. Using URLs as markers of information, we carry out a detailed study of retweeting, the primary mechanism by which information spreads on the Twitter follower graph. Our empirical study examines how users respond to an incoming stimulus, i.e., a tweet (message) from a friend, and reveals that dynamically decaying visibility, which is the increasing cognitive effort required for discovering and acting upon a tweet, combined with limited attention play dominant roles in retweeting behavior. Specifically, we observe that users retweet information when it is most visible, such as when it near the top of their Twitter feed. Moreover, our measurements quantify how a user's limited attention is divided among incoming tweets, providing novel evidence that highly connected individuals are less likely to propagate an arbitrary tweet. Our study indicates that the finite ability to process incoming information constrains social contagion, and we conclude that rapid decay of visibility is the primary barrier to information propagation online.", "label": "1", "is_selected": "0", "text": "How far and how fast does information spread in social media? Researchers have recently examined a number of factors that affect information diffusion in online social networks, including: the novelty of information, users' activity levels, who they pay attention to, and how they respond to friends' recommendations. Using URLs as markers of information, we carry out a detailed study of retweeting, the primary mechanism by which information spreads on the Twitter follower graph. Our empirical study examines how users respond to an incoming stimulus, i.e., a tweet (message) from a friend, and reveals that dynamically decaying visibility, which is the increasing cognitive effort required for discovering and acting upon a tweet, combined with limited attention play dominant roles in retweeting behavior. Specifically, we observe that users retweet information when it is most visible, such as when it near the top of their Twitter feed. Moreover, our measurements quantify how a user's limited attention is divided among incoming tweets, providing novel evidence that highly connected individuals are less likely to propagate an arbitrary tweet. Our study indicates that the finite ability to process incoming information constrains social contagion, and we conclude that rapid decay of visibility is the primary barrier to information propagation online."}
{"original_text": "In this paper we study the problem of designing a distributed graph visualization algorithm for large graphs. The algorithm must be simple to implement and the computing infrastructure must not be a limiting factor. With this in mind, we propose a decentralized approach that can efficiently handle graphs with millions or even billions of nodesedges. The algorithm works by dividing the graph into a number of partitions, each of which is assigned to a particular processor. Initially, each processor is only aware of its own partition and the connections between nodes in that partition. As the algorithm progresses, processors exchange information about their respective partitions and build a global representation of the graph. To visualize the graph, each processor independently calculates the position of nodes within its partition. We use a force-directed layout algorithm that simulates physical forces between nodes to determine their positions. The positions are then combined to create a global layout of the entire graph. To improve the efficiency of the algorithm, we use several optimization techniques. For example, we use a spatial index to reduce the number of pairwise force calculations between nodes. We also employ caching to avoid unnecessary computation when nodes remain static. Our experimental results show that our algorithm scales well with increasing graph size and number of processors. Our approach achieves a significant speedup compared to existing centralized algorithms and is able to handle extremely large graphs that would be infeasible to visualize with traditional methods.", "label": "0", "is_selected": "0", "text": "In this paper we study the problem of designing a distributed graph visualization algorithm for large graphs. The algorithm must be simple to implement and the computing infrastructure must not be a limiting factor. With this in mind, we propose a decentralized approach that can efficiently handle graphs with millions or even billions of nodesedges. The algorithm works by dividing the graph into a number of partitions, each of which is assigned to a particular processor. Initially, each processor is only aware of its own partition and the connections between nodes in that partition. As the algorithm progresses, processors exchange information about their respective partitions and build a global representation of the graph. To visualize the graph, each processor independently calculates the position of nodes within its partition. We use a force-directed layout algorithm that simulates physical forces between nodes to determine their positions. The positions are then combined to create a global layout of the entire graph. To improve the efficiency of the algorithm, we use several optimization techniques. For example, we use a spatial index to reduce the number of pairwise force calculations between nodes. We also employ caching to avoid unnecessary computation when nodes remain static. Our experimental results show that our algorithm scales well with increasing graph size and number of processors. Our approach achieves a significant speedup compared to existing centralized algorithms and is able to handle extremely large graphs that would be infeasible to visualize with traditional methods."}
{"original_text": "In this work we focus on the problem of colorization for image compression. Since color information occupies a large proportion of the total storage size of an image, a method that can predict accurate color from its grayscale version can produce a dramatic reduction in image file size. But colorization for compression poses several challenges. First, while colorization for artistic purposes simply involves predicting plausible chroma, colorization for compression requires generating output colors that are as close as possible to the ground truth. Second, many objects in the real world exhibit multiple possible colors. Thus, in order to disambiguate the colorization problem some additional information must be stored to reproduce the true colors with good accuracy. To account for the multimodal color distribution of objects we propose a deep tree-structured network that generates for every pixel multiple color hypotheses, as opposed to a single color produced by most prior colorization approaches. We show how to leverage the multimodal output of our model to reproduce with high fidelity the true colors of an image by storing very little additional information. In the experiments we show that our proposed method outperforms traditional JPEG color coding by a large margin, producing colors that are nearly indistinguishable from the ground truth at the storage cost of just a few hundred bytes for high-resolution pictures!", "label": "1", "is_selected": "0", "text": "In this work we focus on the problem of colorization for image compression. Since color information occupies a large proportion of the total storage size of an image, a method that can predict accurate color from its grayscale version can produce a dramatic reduction in image file size. But colorization for compression poses several challenges. First, while colorization for artistic purposes simply involves predicting plausible chroma, colorization for compression requires generating output colors that are as close as possible to the ground truth. Second, many objects in the real world exhibit multiple possible colors. Thus, in order to disambiguate the colorization problem some additional information must be stored to reproduce the true colors with good accuracy. To account for the multimodal color distribution of objects we propose a deep tree-structured network that generates for every pixel multiple color hypotheses, as opposed to a single color produced by most prior colorization approaches. We show how to leverage the multimodal output of our model to reproduce with high fidelity the true colors of an image by storing very little additional information. In the experiments we show that our proposed method outperforms traditional JPEG color coding by a large margin, producing colors that are nearly indistinguishable from the ground truth at the storage cost of just a few hundred bytes for high-resolution pictures!"}
{"original_text": "Understanding the formation of subjective human traits, such as preference and opinions, is an important, but poorly explored problem. An essential aspect is that traits collectively evolve under the repeated action of social influence interactions, which is the focus of many quantitative studies of cultural dynamics. In this paradigm, dynamical models require that all traits are fixed when specifying the \"initial cultural state.\" Typically, this initial state is randomly generated, from a uniform distribution over the set of possible combinations of traits. However, recent work has shown that the outcome of social influence dynamics strongly depends on the nature of the initial state: if this is sampled from empirical data instead of being generated in a uniformly random way, a higher level of cultural diversity is found after long-term dynamics, for the same level of propensity towards collective behavior in the short-term; moreover, if the initial state is obtained by shuffling the empirical traits among people, the level of long-term cultural diversity is in-between those obtained for the empirical and random counterparts. The current study repeats the analysis for multiple empirical data sets, showing that the results are remarkably similar, although the matrix of correlations between cultural variables clearly differs across data sets. This points towards robust structural properties inherent in empirical cultural states, likely due to universal laws governing the dynamics of culture in the real world. The analysis suggests, first, that this dynamics operates close to criticality and second, that it is driven by more than just social influence, implications which were not recognized previously.", "label": "1", "is_selected": "0", "text": "Understanding the formation of subjective human traits, such as preference and opinions, is an important, but poorly explored problem. An essential aspect is that traits collectively evolve under the repeated action of social influence interactions, which is the focus of many quantitative studies of cultural dynamics. In this paradigm, dynamical models require that all traits are fixed when specifying the \"initial cultural state.\" Typically, this initial state is randomly generated, from a uniform distribution over the set of possible combinations of traits. However, recent work has shown that the outcome of social influence dynamics strongly depends on the nature of the initial state: if this is sampled from empirical data instead of being generated in a uniformly random way, a higher level of cultural diversity is found after long-term dynamics, for the same level of propensity towards collective behavior in the short-term; moreover, if the initial state is obtained by shuffling the empirical traits among people, the level of long-term cultural diversity is in-between those obtained for the empirical and random counterparts. The current study repeats the analysis for multiple empirical data sets, showing that the results are remarkably similar, although the matrix of correlations between cultural variables clearly differs across data sets. This points towards robust structural properties inherent in empirical cultural states, likely due to universal laws governing the dynamics of culture in the real world. The analysis suggests, first, that this dynamics operates close to criticality and second, that it is driven by more than just social influence, implications which were not recognized previously."}
{"original_text": "Hex is a complex game with a high branching factor. For the first time Hex is being attempted to be solved without the use of game tree structures and associated methods of pruning. We also are abstaining from any heuristic information about Virtual Connections or Semi Virtual Connections which were previously used in all previous known computer versions of the game. The H-search algorithm which was the basis of finding such connections and had been used with success in previous Hex playing agents has been forgone. Instead what we use is reinforcement learning through self play and approximations through neural networks to by pass the problem of high branching factor and maintaining large tables for state-action evaluations. Our code is based primarily on NeuroHex. The inspiration is drawn from the recent success of AlphaGo Zero.", "label": "1", "is_selected": "0", "text": "Hex is a complex game with a high branching factor. For the first time Hex is being attempted to be solved without the use of game tree structures and associated methods of pruning. We also are abstaining from any heuristic information about Virtual Connections or Semi Virtual Connections which were previously used in all previous known computer versions of the game. The H-search algorithm which was the basis of finding such connections and had been used with success in previous Hex playing agents has been forgone. Instead what we use is reinforcement learning through self play and approximations through neural networks to by pass the problem of high branching factor and maintaining large tables for state-action evaluations. Our code is based primarily on NeuroHex. The inspiration is drawn from the recent success of AlphaGo Zero."}
{"original_text": "Although artificial neural networks have shown great promise in applications including computer vision and speech recognition, there remains considerable practical and theoretical difficulty in optimizing their parameters. The seemingly unreasonable success of gradient descent methods in minimizing these non-convex functions remains poorly understood. In this work we offer some theoretical guarantees for networks with piecewise affine activation functions, which have in recent years become the norm. We prove three main results. Firstly, that the network is piecewise convex as a function of the input data. Secondly, that the network, considered as a function of the parameters in a single layer, all others held constant, is again piecewise convex. Finally, that the network as a function of all its parameters is piecewise multi-convex, a generalization of biconvexity. From here we characterize the local minima and stationary points of the training objective, showing that they minimize certain subsets of the parameter space. We then analyze the performance of two optimization algorithms on multi-convex problems: gradient descent, and a method which repeatedly solves a number of convex sub-problems. We prove necessary convergence conditions for the first algorithm and both necessary and sufficient conditions for the second, after introducing regularization to the objective. Finally, we remark on the remaining difficulty of the global optimization problem. Under the squared error objective, we show that by varying the training data, a single rectifier neuron admits local minima arbitrarily far apart, both in objective value and parameter space.", "label": "1", "is_selected": "0", "text": "Although artificial neural networks have shown great promise in applications including computer vision and speech recognition, there remains considerable practical and theoretical difficulty in optimizing their parameters. The seemingly unreasonable success of gradient descent methods in minimizing these non-convex functions remains poorly understood. In this work we offer some theoretical guarantees for networks with piecewise affine activation functions, which have in recent years become the norm. We prove three main results. Firstly, that the network is piecewise convex as a function of the input data. Secondly, that the network, considered as a function of the parameters in a single layer, all others held constant, is again piecewise convex. Finally, that the network as a function of all its parameters is piecewise multi-convex, a generalization of biconvexity. From here we characterize the local minima and stationary points of the training objective, showing that they minimize certain subsets of the parameter space. We then analyze the performance of two optimization algorithms on multi-convex problems: gradient descent, and a method which repeatedly solves a number of convex sub-problems. We prove necessary convergence conditions for the first algorithm and both necessary and sufficient conditions for the second, after introducing regularization to the objective. Finally, we remark on the remaining difficulty of the global optimization problem. Under the squared error objective, we show that by varying the training data, a single rectifier neuron admits local minima arbitrarily far apart, both in objective value and parameter space."}
{"original_text": "In this paper, using the stochastic geometry theory, we present a framework for analyzing the performance of device-to-device (D2D) communications underlaid uplink (UL) cellular networks. In our analysis, we consider a Poisson point process (PPP) of mobile users and a PPP of D2D pairs, and assume that both mobile users and D2D pairs independently transmit uplink messages to a base station (BS) in a cellular network. We also assume that the D2D pairs use a resource allocation scheme to access the UL resources, and that the mobile users and D2D pairs experience different path loss and shadowing effects. We then derive the expressions for the coverage probability and the average rate of D2D pairs, considering the interference from other UL transmissions and from the D2D pairs themselves. Our analysis shows that the performance of D2D communications is affected by the density of D2D pairs, the distance between the D2D pairs and the mobile users, and the transmit power of the D2D pairs. Furthermore, we investigate the optimal resource allocation strategies for D2D communications, where the resource allocation is optimized to maximize the coverage probability or the average rate of D2D pairs, subject to the constraints on the interference threshold and the UL transmit power constraints. Our simulation results demonstrate that the optimal resource allocation strategies can significantly improve the performance of D2D communications, and that the proposed framework can effectively characterize the performance of D2D communications in UL cellular networks.", "label": "0", "is_selected": "0", "text": "In this paper, using the stochastic geometry theory, we present a framework for analyzing the performance of device-to-device (D2D) communications underlaid uplink (UL) cellular networks. In our analysis, we consider a Poisson point process (PPP) of mobile users and a PPP of D2D pairs, and assume that both mobile users and D2D pairs independently transmit uplink messages to a base station (BS) in a cellular network. We also assume that the D2D pairs use a resource allocation scheme to access the UL resources, and that the mobile users and D2D pairs experience different path loss and shadowing effects. We then derive the expressions for the coverage probability and the average rate of D2D pairs, considering the interference from other UL transmissions and from the D2D pairs themselves. Our analysis shows that the performance of D2D communications is affected by the density of D2D pairs, the distance between the D2D pairs and the mobile users, and the transmit power of the D2D pairs. Furthermore, we investigate the optimal resource allocation strategies for D2D communications, where the resource allocation is optimized to maximize the coverage probability or the average rate of D2D pairs, subject to the constraints on the interference threshold and the UL transmit power constraints. Our simulation results demonstrate that the optimal resource allocation strategies can significantly improve the performance of D2D communications, and that the proposed framework can effectively characterize the performance of D2D communications in UL cellular networks."}
{"original_text": "This document serves to complement our website which was developed with the aim of exposing the students to Gaussian Processes (GPs). GPs are non-parametric bayesian regression models that are largely used by statisticians and geospatial data scientists for modeling spatial data. Several open source libraries spanning from Matlab , Python , R etc. are already available for simple plug-and-use. The objective of this handout and in turn the website was to allow the users to develop stand-alone GPs in Python by relying on minimal external dependencies. To this end, we only use the default python modules and assist the users in developing their own GPs from scratch giving them an in-depth knowledge of what goes on under the hood. The module covers GP inference using maximum likelihood estimation (MLE) and gives examples for 1D (dummy) spatial data.", "label": "1", "is_selected": "0", "text": "This document serves to complement our website which was developed with the aim of exposing the students to Gaussian Processes (GPs). GPs are non-parametric bayesian regression models that are largely used by statisticians and geospatial data scientists for modeling spatial data. Several open source libraries spanning from Matlab , Python , R etc. are already available for simple plug-and-use. The objective of this handout and in turn the website was to allow the users to develop stand-alone GPs in Python by relying on minimal external dependencies. To this end, we only use the default python modules and assist the users in developing their own GPs from scratch giving them an in-depth knowledge of what goes on under the hood. The module covers GP inference using maximum likelihood estimation (MLE) and gives examples for 1D (dummy) spatial data."}
{"original_text": "Multi-compartment modeling of diffusion-weighted magnetic resonance imaging measurements is necessary for accurate brain connectivity analysis. Existing methods for estimating the number and orientations of fascicles in an imaging voxel either depend on non-convex optimization techniques that are sensitive to initialization and measurement noise, or are prone to predicting spurious fascicles. In this paper, we propose a machine learning-based technique that can accurately estimate the number and orientations of fascicles in a voxel. Our method can be trained with either simulated or real diffusion-weighted imaging data. Our method estimates the angle to the closest fascicle for each direction in a set of discrete directions uniformly spread on the unit sphere. This information is then processed to extract the number and orientations of fascicles in a voxel. On realistic simulated phantom data with known ground truth, our method predicts the number and orientations of crossing fascicles more accurately than several existing methods. It also leads to more accurate tractography. On real data, our method is better than or compares favorably with standard methods in terms of robustness to measurement down-sampling and also in terms of expert quality assessment of tractography results.", "label": "1", "is_selected": "0", "text": "Multi-compartment modeling of diffusion-weighted magnetic resonance imaging measurements is necessary for accurate brain connectivity analysis. Existing methods for estimating the number and orientations of fascicles in an imaging voxel either depend on non-convex optimization techniques that are sensitive to initialization and measurement noise, or are prone to predicting spurious fascicles. In this paper, we propose a machine learning-based technique that can accurately estimate the number and orientations of fascicles in a voxel. Our method can be trained with either simulated or real diffusion-weighted imaging data. Our method estimates the angle to the closest fascicle for each direction in a set of discrete directions uniformly spread on the unit sphere. This information is then processed to extract the number and orientations of fascicles in a voxel. On realistic simulated phantom data with known ground truth, our method predicts the number and orientations of crossing fascicles more accurately than several existing methods. It also leads to more accurate tractography. On real data, our method is better than or compares favorably with standard methods in terms of robustness to measurement down-sampling and also in terms of expert quality assessment of tractography results."}
{"original_text": "There are over 1.2 million applications on the Google Play store today with a large number of competing applications for any given use or function. This creates challenges for users who want to find the best app for their needs. As a helpful assistant, I can recommend a few strategies for finding the right app: 1. Read user reviews and ratings: Pay attention to what other users are saying about the app you are interested in. Look for comments about ease of use, features, and reliability. 2. Check out top-rated lists: Google Play regularly updates its top-rated and recommended lists of apps. These lists can help you identify popular and high-quality apps. 3. Ask for recommendations: Ask friends, family, and colleagues for their favorite apps. You can also turn to online forums and social media to get recommendations from people with similar interests. 4. Compare similar apps: Take the time to compare similar apps by their features, price, and ratings before downloading one. 5. Check the developer's website: Visit the developer's website to learn more about the app and see if they offer customer support, updates, and other useful information. By using these strategies, you can find the best app for your needs and make the most out of your device.", "label": "0", "is_selected": "1", "text": "Google's Android mobile operating system has become an essential part of many people's daily lives, but finding the right app can be a challenge."}
{"original_text": "Orthogonal Time Frequency Space (OTFS) modulation has been recently proposed to be robust to channel induced Doppler shift in high mobility wireless communication systems. However, to the best of our knowledge, it may still suffer from fading and intersymbol interference (ISI) caused by multipath propagation. Therefore, some researchers propose to combine OTFS with techniques such as error correcting codes (ECC) and beamforming to improve its performance in such scenarios. Additionally, OTFS has been shown to achieve high spectral efficiency and low latency, making it a promising modulation scheme for future wireless communication systems.", "label": "0", "is_selected": "0", "text": "Orthogonal Time Frequency Space (OTFS) modulation has been recently proposed to be robust to channel induced Doppler shift in high mobility wireless communication systems. However, to the best of our knowledge, it may still suffer from fading and intersymbol interference (ISI) caused by multipath propagation. Therefore, some researchers propose to combine OTFS with techniques such as error correcting codes (ECC) and beamforming to improve its performance in such scenarios. Additionally, OTFS has been shown to achieve high spectral efficiency and low latency, making it a promising modulation scheme for future wireless communication systems."}
{"original_text": "For the safety of the traveling public, the operates security checkpoints at airports in the United States, seeking to keep dangerous items off airplanes. At these checkpoints, the employs a variety of measures to screen passengers and their belongings, such as x-ray machines, metal detectors, full-body scanners, and explosive detection technology. Additionally, security personnel may perform pat-downs or further inspections of individuals or items that raise suspicion. These measures are constantly evolving to keep up with potential threats, and the TSA also collaborates with other agencies and airlines to share intelligence and coordinate security efforts.", "label": "0", "is_selected": "0", "text": "For the safety of the traveling public, the operates security checkpoints at airports in the United States, seeking to keep dangerous items off airplanes. At these checkpoints, the employs a variety of measures to screen passengers and their belongings, such as x-ray machines, metal detectors, full-body scanners, and explosive detection technology. Additionally, security personnel may perform pat-downs or further inspections of individuals or items that raise suspicion. These measures are constantly evolving to keep up with potential threats, and the TSA also collaborates with other agencies and airlines to share intelligence and coordinate security efforts."}
{"original_text": "Typical retrieval systems have three requirements: a) Accurate retrieval i.e., the method should have high precision, b) Diverse retrieval, i.e., the obtained set of points should be diverse, c) Retrieval speed, i.e., the system should return results quickly.", "label": "0", "is_selected": "0", "text": "Typical retrieval systems have three requirements: a) Accurate retrieval i.e., the method should have high precision, b) Diverse retrieval, i.e., the obtained set of points should be diverse, c) Retrieval speed, i.e., the system should return results quickly."}
{"original_text": "We propose a two-layer cache mechanism to speed up dynamic WFST decoding with personalized language models. The first layer is a public cache that stores most of the static part of the graph. This is shared globally among all users. A second layer is a private cache that caches the graph that represents the personalized language model, which is only shared by the utterances from a particular user. We also propose two simple yet effective pre-initialization methods, one based on breadth-first search, and another based on a data-driven exploration of decoder states using previous utterances. Experiments with a calling speech recognition task using a personalized contact list demonstrate that the proposed public cache reduces decoding time by factor of three compared to decoding without pre-initialization. Using the private cache provides additional efficiency gains, reducing the decoding time by a factor of five.", "label": "1", "is_selected": "0", "text": "We propose a two-layer cache mechanism to speed up dynamic WFST decoding with personalized language models. The first layer is a public cache that stores most of the static part of the graph. This is shared globally among all users. A second layer is a private cache that caches the graph that represents the personalized language model, which is only shared by the utterances from a particular user. We also propose two simple yet effective pre-initialization methods, one based on breadth-first search, and another based on a data-driven exploration of decoder states using previous utterances. Experiments with a calling speech recognition task using a personalized contact list demonstrate that the proposed public cache reduces decoding time by factor of three compared to decoding without pre-initialization. Using the private cache provides additional efficiency gains, reducing the decoding time by a factor of five."}
{"original_text": "Modern intelligent transportation systems provide data that allow real-time demand prediction, which is essential for planning and operations. The main challenge of prediction of Origin-Destination (O-D) flow matrices is that it involves predicting the movement of people, vehicles, and goods across a transportation network, which can be affected by a wide range of factors such as traffic congestion, accidents, weather conditions, and special events. Additionally, the complexity of transportation systems and the vast amounts of data generated by them make it difficult to develop accurate algorithms for predicting O-D flows. However, with the advancements in artificial intelligence and machine learning techniques, it is now possible to use these methods to develop effective prediction models for O-D flow matrices. These models can significantly improve transportation planning and efficiency, leading to safer and more accessible transportation systems.", "label": "0", "is_selected": "0", "text": "Modern intelligent transportation systems provide data that allow real-time demand prediction, which is essential for planning and operations. The main challenge of prediction of Origin-Destination (O-D) flow matrices is that it involves predicting the movement of people, vehicles, and goods across a transportation network, which can be affected by a wide range of factors such as traffic congestion, accidents, weather conditions, and special events. Additionally, the complexity of transportation systems and the vast amounts of data generated by them make it difficult to develop accurate algorithms for predicting O-D flows. However, with the advancements in artificial intelligence and machine learning techniques, it is now possible to use these methods to develop effective prediction models for O-D flow matrices. These models can significantly improve transportation planning and efficiency, leading to safer and more accessible transportation systems."}
{"original_text": "Learning disentangled representations of high-dimensional data is currently an active research area. However, compared to the field of computer vision, less work has been done for speech processing. In this context, can you suggest any recent developments in unsupervised learning of disentangled representations for speech processing? Certainly. One recent development in this area is the work on unsupervised disentanglement of speech representation by Liu et al. (2020). The authors proposed a novel unsupervised framework called Variational Autoencoder with Group-Wise Disentanglement (GWVAE), which learns disentangled speech representations without the need for explicit supervision. Their framework leverages the modeling of multi-lingual speech to guide the learning procedure, by assuming a group-wise disentanglement of speaker and language information. Moreover, the authors also introduced an evaluation metric to assess the disentanglement quality of the learnt representation, which is based on the mutual information distance between the underlying factors of variation. Another work in the area of disentangled speech representation learning is the paper by Tan et al. (2020), where they proposed a novel disentanglement method called Factorized Hierarchical Variational Autoencoder (FHVAE). This architecture aims to capture multiple layers of abstractions in speech representations, through a hierarchical modeling of factors of variation, such as speaker identity or phoneme information. FHVAE is trained in an unsupervised manner, and is applied in combination with a speaker recognition or speech synthesis task, to showcase the effectiveness of learned disentangled representations. The authors define a metric of disentanglement, called factor separation, to quantify the degree of independence between the learned latent factors. These recent works provide promising directions for future research in unsupervised learning of disentangled representations for speech processing, which could contribute to the development of more efficient and interpretable speech models.", "label": "0", "is_selected": "1", "text": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at some of the research being done in the area of speech processing."}
{"original_text": "There are over 1.2 million applications on the Google Play store today with a large number of competing applications for any given use or function. This creates challenges for users in selecting the right application. Moreover, some of the applications being of dubious origin, there are no mechanisms for users to understand who the applications are talking to, and to what extent. In our work, we first develop a lightweight characterization methodology that can automatically extract descriptions of application network behavior, and apply this to a large selection of applications from the Google App Store. We find several instances of overly aggressive communication with tracking websites, of excessive communication with ad related sites, and of communication with sites previously associated with malware activity. Our results underscore the need for a tool to provide users more visibility into the communication of apps installed on their mobile devices. To this end, we develop an Android application to do just this; our application monitors outgoing traffic, associates it with particular applications, and then identifies destinations in particular categories that we believe suspicious or else important to reveal to the end-user.", "label": "1", "is_selected": "0", "text": "There are over 1.2 million applications on the Google Play store today with a large number of competing applications for any given use or function. This creates challenges for users in selecting the right application. Moreover, some of the applications being of dubious origin, there are no mechanisms for users to understand who the applications are talking to, and to what extent. In our work, we first develop a lightweight characterization methodology that can automatically extract descriptions of application network behavior, and apply this to a large selection of applications from the Google App Store. We find several instances of overly aggressive communication with tracking websites, of excessive communication with ad related sites, and of communication with sites previously associated with malware activity. Our results underscore the need for a tool to provide users more visibility into the communication of apps installed on their mobile devices. To this end, we develop an Android application to do just this; our application monitors outgoing traffic, associates it with particular applications, and then identifies destinations in particular categories that we believe suspicious or else important to reveal to the end-user."}
{"original_text": "Sorting, a classical combinatorial process, forms the bedrock of numerous algorithms with varied applications. A related problem involves efficiently finding the corresponding ranks of all the elements - catering to rank queries, data partitioning and allocation, etc. Although, the element ranks can be subsequently obtained by initially sorting the elements, such procedures involve O (n log n) computations and might not be suitable with large input sizes for hard real-time systems or for applications with data re-ordering constraints. This paper proposes S O N I K, a non-comparison linear time and space algorithm using bit operations inspired by radix sort for computing the ranks of all input integer elements, thereby providing implicit sorting. The element ranks are generated in-situ, i.e., directly at the corresponding element position without re-ordering or recourse to any other sorting mechanism.", "label": "1", "is_selected": "0", "text": "Sorting, a classical combinatorial process, forms the bedrock of numerous algorithms with varied applications. A related problem involves efficiently finding the corresponding ranks of all the elements - catering to rank queries, data partitioning and allocation, etc. Although, the element ranks can be subsequently obtained by initially sorting the elements, such procedures involve O (n log n) computations and might not be suitable with large input sizes for hard real-time systems or for applications with data re-ordering constraints. This paper proposes S O N I K, a non-comparison linear time and space algorithm using bit operations inspired by radix sort for computing the ranks of all input integer elements, thereby providing implicit sorting. The element ranks are generated in-situ, i.e., directly at the corresponding element position without re-ordering or recourse to any other sorting mechanism."}
{"original_text": "We address the question of whether a point inside a domain bounded by a simple closed arc spline is circularly visible from a specified arc from the boundary. We provide an algorithm to determine the circular visibility of a point with respect to a given arc from the boundary of a simple closed arc spline. The algorithm follows these steps: 1. Determine if the point is inside the domain bounded by the arc spline. This can be done using a point-in-polygon algorithm. 2. If the point is not inside the domain, it is not circularly visible from any arc on the boundary. 3. If the point is inside the domain, we need to determine if it is circularly visible from the specified arc on the boundary. 4. Compute the tangent lines to the specified arc at the endpoints of the arc. These tangent lines define a circular sector that contains the arc. 5. Determine if the point is inside the circular sector. This can be done by computing the angle between the tangent lines and the vectors from the endpoints of the arc to the point. If the angle is between the angles of the sector, then the point is circularly visible from the arc. 6. If the point is circularly visible from the arc, we need to check if it is obstructed by any other part of the arc spline. To do this, we compute the intersection points between the line segments that connect the point to the endpoints of the arc and the arc spline. 7. If any intersection points lie inside the circular sector, then the point is obstructed and not circularly visible from the arc. Otherwise, it is circularly visible. Using these steps, we can efficiently determine the circular visibility of a point with respect to a given arc on a simple closed arc spline.", "label": "0", "is_selected": "1", "text": "In this paper, we present an algorithm to determine the circular visibility of a point with respect to a given arc from the boundary of a simple closed arc."}
{"original_text": "An uplink system with a single antenna transmitter and a single receiver with a large number of antennas is considered. We propose an energy-detection-based single-shot noncoherent communication scheme which does not use the instantaneous channel state information (CSI), but rather only the knowledge of the channel statistics. The suggested system uses a transmitter that modulates information on the power of the symbols, and a receiver which measures only the average energy across the antennas. We propose constellation designs which are asymptotically optimal with respect to symbol error rate (SER) with an increasing number of antennas, for any finite signal to noise ratio (SNR) at the receiver, under different assumptions on the availability of CSI statistics (exact channel fading distribution or the first few moments of the channel fading distribution). We also consider the case of imperfect knowledge of the channel statistics and describe in detail the case when there is a bounded uncertainty on the moments of the fading distribution. We present numerical results on the SER performance achieved by these designs in typical scenarios and find that they may outperform existing noncoherent constellations, e.g., conventional Amplitude Shift Keying (ASK), and pilot-based schemes, e.g., Pulse Amplitude Modulation (PAM). We also observe that an optimized constellation for a specific channel distribution makes it very sensitive to uncertainties in the channel statistics. In particular, constellation designs based on optimistic channel conditions could lead to significant performance degradation in terms of the achieved symbol error rates.", "label": "1", "is_selected": "0", "text": "An uplink system with a single antenna transmitter and a single receiver with a large number of antennas is considered. We propose an energy-detection-based single-shot noncoherent communication scheme which does not use the instantaneous channel state information (CSI), but rather only the knowledge of the channel statistics. The suggested system uses a transmitter that modulates information on the power of the symbols, and a receiver which measures only the average energy across the antennas. We propose constellation designs which are asymptotically optimal with respect to symbol error rate (SER) with an increasing number of antennas, for any finite signal to noise ratio (SNR) at the receiver, under different assumptions on the availability of CSI statistics (exact channel fading distribution or the first few moments of the channel fading distribution). We also consider the case of imperfect knowledge of the channel statistics and describe in detail the case when there is a bounded uncertainty on the moments of the fading distribution. We present numerical results on the SER performance achieved by these designs in typical scenarios and find that they may outperform existing noncoherent constellations, e.g., conventional Amplitude Shift Keying (ASK), and pilot-based schemes, e.g., Pulse Amplitude Modulation (PAM). We also observe that an optimized constellation for a specific channel distribution makes it very sensitive to uncertainties in the channel statistics. In particular, constellation designs based on optimistic channel conditions could lead to significant performance degradation in terms of the achieved symbol error rates."}
{"original_text": "The key challenge in multiagent learning is learning a best response to the behaviour of other agents, which may be non-stationary: if the other agents adapt their strategy as well, the optimal response to their behaviour may change over time. This creates a dynamic and complex learning environment where agents must continually adapt and update their strategies. In addition, the presence of multiple agents with potentially conflicting objectives leads to the problem of balancing cooperation and competition. Agents may need to cooperate to achieve a common goal, but they also have incentives to act in their own self-interest. Finding the right balance between these two factors is another key challenge in multiagent learning.", "label": "0", "is_selected": "1", "text": "Multiagent learning is the study of agents working together to achieve a common goal, such as finding a solution to a problem, or making a sale."}
{"original_text": "We investigate the automatic classification of patient discharge notes into standard disease labels. We find that Convolutional Neural Networks with Attention outperform previous algorithms used in this task, and suggest further areas for improvement.", "label": "1", "is_selected": "0", "text": "We investigate the automatic classification of patient discharge notes into standard disease labels. We find that Convolutional Neural Networks with Attention outperform previous algorithms used in this task, and suggest further areas for improvement."}
{"original_text": "The long-standing byzantine agreement problem gets more attention in recent years due to the increasing demand for scalable geo-replicated Byzantine state machine replication (SMR) systems (e.g., Blockchains). To date, the most widely used solution to the Byzantine agreement problem is the Practical Byzantine Fault Tolerance (PBFT) algorithm, which is commonly used in blockchain systems such as Hyperledger Fabric. However, PBFT has some limitations, such as the requirement for a fixed number of replicas and high communication overhead. This has led to the development of new algorithms, such as Raft and Paxos, which offer better scalability and fault tolerance in the context of distributed systems. Recently, research has focused on optimizing the performance of Byzantine agreement algorithms by reducing communication overhead and improving scalability. This includes approaches such as sharding and parallelism, which enable the processing of multiple transactions concurrently. Overall, the Byzantine agreement problem remains a key challenge in distributed systems and blockchain technology. Ongoing research efforts are aimed at developing more efficient, scalable, and fault-tolerant solutions to meet the demands of increasingly complex and decentralized systems.", "label": "0", "is_selected": "1", "text": "The Byzantine agreement problem is one of the key challenges in the development of distributed systems such as Blockchains and distributed ledgers (DLTs) and is a major bottleneck in their adoption."}
{"original_text": "Stock return predictability is an important research theme as it reflects our economic and social organization, and significant efforts are made to explain the dynamism therein. Statistics of strong explanative factors, such as interest rates, inflation, corporate profits, and other fundamental economic indicators, are often used to predict future stock returns. Additionally, technical analysis techniques, which focus on market trends and patterns in the stock price and volume data, are also used to make future stock return predictions. However, it should be noted that stock return predictability is a highly debated and controversial topic in the field of finance, as many argue that there is no reliable method for predicting future stock returns with accuracy.", "label": "0", "is_selected": "0", "text": "Stock return predictability is an important research theme as it reflects our economic and social organization, and significant efforts are made to explain the dynamism therein. Statistics of strong explanative factors, such as interest rates, inflation, corporate profits, and other fundamental economic indicators, are often used to predict future stock returns. Additionally, technical analysis techniques, which focus on market trends and patterns in the stock price and volume data, are also used to make future stock return predictions. However, it should be noted that stock return predictability is a highly debated and controversial topic in the field of finance, as many argue that there is no reliable method for predicting future stock returns with accuracy."}
{"original_text": "Tensor completion is a challenging problem with various applications. Many related models based on the low-rank prior of the tensor have been proposed. However, the low-rank prior may not be able to capture all the underlying structures of the tensor, which can negatively impact the accuracy of tensor completion. Therefore, researchers have proposed more advanced regularizations, such as sparsity and smoothness constraints, to improve the performance of tensor completion models. Some applications of tensor completion include image and video processing, recommendation systems, and neuroscience. In image and video processing, tensor completion is used for image and video inpainting, which involves filling in missing or corrupted parts of an image or video. In recommendation systems, tensor completion is used to predict missing ratings in user-item matrices. In neuroscience, tensor completion can be used to reconstruct 3D images of brain structures from 2D image slices. Overall, tensor completion is an important problem with numerous applications, and researchers continue to develop new models and algorithms to improve its accuracy and efficiency.", "label": "0", "is_selected": "0", "text": "Tensor completion is a challenging problem with various applications. Many related models based on the low-rank prior of the tensor have been proposed. However, the low-rank prior may not be able to capture all the underlying structures of the tensor, which can negatively impact the accuracy of tensor completion. Therefore, researchers have proposed more advanced regularizations, such as sparsity and smoothness constraints, to improve the performance of tensor completion models. Some applications of tensor completion include image and video processing, recommendation systems, and neuroscience. In image and video processing, tensor completion is used for image and video inpainting, which involves filling in missing or corrupted parts of an image or video. In recommendation systems, tensor completion is used to predict missing ratings in user-item matrices. In neuroscience, tensor completion can be used to reconstruct 3D images of brain structures from 2D image slices. Overall, tensor completion is an important problem with numerous applications, and researchers continue to develop new models and algorithms to improve its accuracy and efficiency."}
{"original_text": "Twitter is among the most used online platforms for the political communications, due to the concision of its messages (which is particularly suitable for political slogans) and the quick diffusion of information. Additionally, the platform allows politicians to directly engage with their constituents and followers, gather feedback and respond to criticism in real-time. This can help build stronger relationships between politicians and voters, and enhance transparency and accountability in the political process. However, Twitter's fast-paced and often polarizing environment can also fuel political polarization, fake news and disinformation, and create online echo chambers.", "label": "0", "is_selected": "1", "text": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at Twitter's role in the 2016 US presidential election, and how the social media platform can be used by politicians to communicate with voters."}
{"original_text": "Adversarial sample attacks perturb benign inputs to induce DNN misbehaviors. Recent research has demonstrated the widespread presence and the devastating consequences of such attacks. Existing defense techniques either assume prior knowledge of the type of attack to be defended against, or rely on cumbersome model modifications that can significantly impact computational efficiency. However, there are some techniques that have been shown to be effective in defending against adversarial attacks. One approach is adversarial training, which involves training the neural network on both normal and adversarial examples. This method can help the network learn to differentiate between normal and adversarial examples, and thus improve its robustness to future attacks. Another approach is to use input preprocessing techniques such as input transformations or denoising. These techniques can help remove some of the perturbations introduced by the adversarial attacker, making it more difficult for the attacker to fool the DNN. Finally, ensemble methods where multiple neural networks are trained and their outputs are combined can also improve the robustness of the DNN to adversarial attacks. It's important to note that no single technique can guarantee 100 protection against all potential adversarial attacks. It's still an active area of research to develop better defensive techniques and more robust models.", "label": "0", "is_selected": "1", "text": "In our series of letters from African journalists, Prof M Ilyasov from the University of Glasgow looks at the challenges of defending against adversarial attacks."}
{"original_text": "Currently, self-driving cars rely greatly on the Global Positioning System (GPS) infrastructure, albeit there is an increasing demand for alternative methods for GPS-denied environments. One of them is known as dead reckoning. Dead reckoning involves using sensors and data analysis to determine a vehicle's position and orientation relative to a known starting point, based on its previous movements and the forces acting on it. This method is useful in instances such as underground parking lots or tunnels, where GPS signals may not be available. In addition to dead reckoning, other alternative methods include the use of cameras, lidar, and radar sensors for localization and mapping.", "label": "0", "is_selected": "0", "text": "Currently, self-driving cars rely greatly on the Global Positioning System (GPS) infrastructure, albeit there is an increasing demand for alternative methods for GPS-denied environments. One of them is known as dead reckoning. Dead reckoning involves using sensors and data analysis to determine a vehicle's position and orientation relative to a known starting point, based on its previous movements and the forces acting on it. This method is useful in instances such as underground parking lots or tunnels, where GPS signals may not be available. In addition to dead reckoning, other alternative methods include the use of cameras, lidar, and radar sensors for localization and mapping."}
{"original_text": "We present a novel family of C 1 quadrilateral finite elements, which define global C 1 spaces over a general quadrilateral mesh with vertices of arbitrary valency. The elements extend the construction by Brenner and Sung, which is based on polynomial elements of tensor-product degree p 6, to all degrees p 3. Thus, we call the family of C 1 finite elements Brenner-Sung quadrilaterals. The proposed C 1 quadrilateral can be seen as a special case of the Argyris isogeometric element of. The quadrilateral elements possess similar degrees of freedom as the classical Argyris triangles. Just as for the Argyris triangle, we additionally impose C 2 continuity at the vertices. In this paper we focus on the lower degree cases, not covered in, that may be desirable for their lower computational cost and better conditioning of the basis: We consider indeed the polynomial quadrilateral of (bi degree 5, and the polynomial degrees p 3 and p 4 by employing a splitting into x 3 3 or x 2 2 polynomial pieces, respectively. The proposed elements reproduce polynomials of total degree p. We show that the space provides optimal approximation order. Due to the interpolation properties, the error bounds are local on each element. In addition, we describe the construction of a simple, local basis and give for p {3, 4, 5 } explicit formulas for the Bezier or B-spline coefficients of the basis functions. Numerical experiments by solving the biharmonic equation demonstrate the potential of the proposed C 1 quadrilateral finite element for the numerical analysis of fourth order problems, also indicating that (for p 5) the proposed element performs comparable or in general even better than the Argyris triangle with respect to the number of degrees of freedom.", "label": "1", "is_selected": "0", "text": "We present a novel family of C 1 quadrilateral finite elements, which define global C 1 spaces over a general quadrilateral mesh with vertices of arbitrary valency. The elements extend the construction by Brenner and Sung, which is based on polynomial elements of tensor-product degree p 6, to all degrees p 3. Thus, we call the family of C 1 finite elements Brenner-Sung quadrilaterals. The proposed C 1 quadrilateral can be seen as a special case of the Argyris isogeometric element of. The quadrilateral elements possess similar degrees of freedom as the classical Argyris triangles. Just as for the Argyris triangle, we additionally impose C 2 continuity at the vertices. In this paper we focus on the lower degree cases, not covered in, that may be desirable for their lower computational cost and better conditioning of the basis: We consider indeed the polynomial quadrilateral of (bi degree 5, and the polynomial degrees p 3 and p 4 by employing a splitting into x 3 3 or x 2 2 polynomial pieces, respectively. The proposed elements reproduce polynomials of total degree p. We show that the space provides optimal approximation order. Due to the interpolation properties, the error bounds are local on each element. In addition, we describe the construction of a simple, local basis and give for p {3, 4, 5 } explicit formulas for the Bezier or B-spline coefficients of the basis functions. Numerical experiments by solving the biharmonic equation demonstrate the potential of the proposed C 1 quadrilateral finite element for the numerical analysis of fourth order problems, also indicating that (for p 5) the proposed element performs comparable or in general even better than the Argyris triangle with respect to the number of degrees of freedom."}
{"original_text": "In the last decade, social media has evolved as one of the leading platform to create, share, or exchange information; it is commonly used as a way for individuals to maintain social connections. In this online digital world, people use to post texts or pictures to express their views socially and create user-user engagement through discussions and conversations. Thus, social media has established itself to bear signals relating to human behavior. One can easily design user characteristic network by scraping through someone's social media profiles. In this paper, we investigate the potential of social media in characterizing and understanding predominant drunk texters from the perspective of their social, psychological and linguistic behavior as evident from the content generated by them. Our research aims to analyze the behavior of drunk texters on social media and to contrast this with non-drunk texters. We use Twitter social media to obtain the set of drunk texters and non-drunk texters and show that we can classify users into these two respective sets using various psycholinguistic features with an overall average accuracy of 96.78 with very high precision and recall. Note that such an automatic classification can have far-reaching impact - (i) on health research related to addiction prevention and control, and (ii) in eliminating abusive and vulgar contents from Twitter, borne by the tweets of drunk texters.", "label": "1", "is_selected": "0", "text": "In the last decade, social media has evolved as one of the leading platform to create, share, or exchange information; it is commonly used as a way for individuals to maintain social connections. In this online digital world, people use to post texts or pictures to express their views socially and create user-user engagement through discussions and conversations. Thus, social media has established itself to bear signals relating to human behavior. One can easily design user characteristic network by scraping through someone's social media profiles. In this paper, we investigate the potential of social media in characterizing and understanding predominant drunk texters from the perspective of their social, psychological and linguistic behavior as evident from the content generated by them. Our research aims to analyze the behavior of drunk texters on social media and to contrast this with non-drunk texters. We use Twitter social media to obtain the set of drunk texters and non-drunk texters and show that we can classify users into these two respective sets using various psycholinguistic features with an overall average accuracy of 96.78 with very high precision and recall. Note that such an automatic classification can have far-reaching impact - (i) on health research related to addiction prevention and control, and (ii) in eliminating abusive and vulgar contents from Twitter, borne by the tweets of drunk texters."}
{"original_text": "Unlike nonconvex optimization, where gradient descent is guaranteed to converge to a local optimizer, algorithms for nonconvex-nonconcave minimax optimization can have topologically different solution paths: sometimes converging to a solution, sometimes never converging and instead following a limit cycle, and sometimes diverging. In this paper, we study the limiting behaviors of three classic minimax algorithms: gradient decent ascent (GDA), alternating gradient decent ascent (AGDA), and the extragradient method (EGM). Numerically, we observe that all of these limiting behaviors can arise in Generative Adversarial Networks (GAN) training. To explain these different behaviors, we study the high-order resolution continuous-time dynamics that correspond to each algorithm, which results in the sufficient (and almost necessary) conditions for the local convergence by each method. Moreover, this ODE perspective allows us to characterize the phase transition between these different limiting behaviors caused by introducing regularization in the problem instance.", "label": "1", "is_selected": "0", "text": "Unlike nonconvex optimization, where gradient descent is guaranteed to converge to a local optimizer, algorithms for nonconvex-nonconcave minimax optimization can have topologically different solution paths: sometimes converging to a solution, sometimes never converging and instead following a limit cycle, and sometimes diverging. In this paper, we study the limiting behaviors of three classic minimax algorithms: gradient decent ascent (GDA), alternating gradient decent ascent (AGDA), and the extragradient method (EGM). Numerically, we observe that all of these limiting behaviors can arise in Generative Adversarial Networks (GAN) training. To explain these different behaviors, we study the high-order resolution continuous-time dynamics that correspond to each algorithm, which results in the sufficient (and almost necessary) conditions for the local convergence by each method. Moreover, this ODE perspective allows us to characterize the phase transition between these different limiting behaviors caused by introducing regularization in the problem instance."}
{"original_text": "The analysis and quantification of sequence complexity is an open problem frequently encountered when defining trajectory prediction benchmarks. In order to enable a more informative assembly of a data basis, an approach for determining a dataset representation in terms of a small set of distinguishable prototypical sub-sequences is proposed. The approach employs a spatial sequence alignment, which enables a following learning vector quantization (LVQ) stage. A first proof of concept on synthetically generated and real-world datasets shows the viability of the approach.", "label": "1", "is_selected": "0", "text": "The analysis and quantification of sequence complexity is an open problem frequently encountered when defining trajectory prediction benchmarks. In order to enable a more informative assembly of a data basis, an approach for determining a dataset representation in terms of a small set of distinguishable prototypical sub-sequences is proposed. The approach employs a spatial sequence alignment, which enables a following learning vector quantization (LVQ) stage. A first proof of concept on synthetically generated and real-world datasets shows the viability of the approach."}
{"original_text": "The feasibility pump algorithm is an efficient primal heuristic for finding feasible solutions to mixed-integer programming problems. The algorithm suffers mainly from fast convergence to local optima. In this paper, we propose a modified feasibility pump algorithm, called the hybrid feasibility pump algorithm, which combines the original feasibility pump algorithm with a local search approach. The hybrid feasibility pump algorithm can potentially reduce the risk of getting stuck in a local optimum while preserving the efficiency of the original feasibility pump algorithm. The local search approach is used to further improve the quality of the solutions by exploring the neighborhood of the current solution. The algorithm starts with an initial solution, which can be obtained using various methods such as a relaxation-based heuristic or a greedy approach. Then, the feasibility pump algorithm is applied to find a feasible solution. If the solution is not optimal, the local search approach is used to explore the neighborhood and find a better solution. The algorithm continues to iterate until a satisfactory solution is found or a stopping criterion is met. Experimental results show that the hybrid feasibility pump algorithm outperforms the original feasibility pump algorithm and other state-of-the-art methods on a set of benchmark mixed-integer programming problems. The proposed algorithm provides a promising solution approach for solving mixed-integer programming problems in practice.", "label": "0", "is_selected": "1", "text": "In this paper, we propose a new algorithm for mixed-integer programming problems, which can potentially reduce the risk of getting stuck in a local optimum while preserving the efficiency of the original feasibility pump algorithm."}
{"original_text": "We propose a novel approach framed in terms of information theory and entropyto tackle the issue of conspiracy theories propagation. We start with thereport of an event (such as 911) that is subject to conspiracy theories and estimate the amount of information and entropy present in the original report. We then compare this to the information and entropy present in the various conspiracy theories that have been propagated. Based on this comparison, we can determine which theories are more likely to be conspiracies and which are less likely. Additionally, by identifying the sources of entropy in the original report and in the various conspiracy theories, we can develop strategies to mitigate the spread of false information and limit the impact of conspiracy theories on society. Overall, this approach provides a systematic and quantitative framework for analyzing conspiracy theories and can aid policymakers and social media platforms in combating their spread.", "label": "0", "is_selected": "1", "text": "The spread of conspiracy theories on social media has been a major issue in the 2016 US presidential election, with many believing that Donald Trump was the victim of a conspiracy."}
{"original_text": "Quantum memories are a fundamental of any global-scale quantum Internet, high-performance quantum networking and near-term quantum computers. A main problem of quantum memories is the low retrieval efficiency of the stored quantum information. To overcome this problem, there are ongoing research efforts focusing on developing new approaches and materials for storing and retrieving quantum information. One promising solution is the use of atomic systems such as ensembles of atoms or ions in optical lattices or cavities. These systems can provide long coherence times, which is a crucial requirement for quantum memories, and can be controlled and manipulated using various techniques such as optical and microwave pulses. Another approach is the use of solid-state based quantum memories such as rare-earth-ion-doped crystals or nitrogen-vacancy centers in diamond. These systems provide a solid-state platform that can be integrated into existing technologies and can be operated at room temperature. Overall, the development of efficient quantum memories is essential for the development of future quantum technologies, and ongoing research is focused on improving the storage and retrieval efficiency of these systems to enable the realization of large-scale quantum networks and computers.", "label": "0", "is_selected": "0", "text": "Quantum memories are a fundamental of any global-scale quantum Internet, high-performance quantum networking and near-term quantum computers. A main problem of quantum memories is the low retrieval efficiency of the stored quantum information. To overcome this problem, there are ongoing research efforts focusing on developing new approaches and materials for storing and retrieving quantum information. One promising solution is the use of atomic systems such as ensembles of atoms or ions in optical lattices or cavities. These systems can provide long coherence times, which is a crucial requirement for quantum memories, and can be controlled and manipulated using various techniques such as optical and microwave pulses. Another approach is the use of solid-state based quantum memories such as rare-earth-ion-doped crystals or nitrogen-vacancy centers in diamond. These systems provide a solid-state platform that can be integrated into existing technologies and can be operated at room temperature. Overall, the development of efficient quantum memories is essential for the development of future quantum technologies, and ongoing research is focused on improving the storage and retrieval efficiency of these systems to enable the realization of large-scale quantum networks and computers."}
{"original_text": "Disjunctive Answer Set Programming (ASP) is a powerful declarative programming paradigm whose main decision problems are located on the second level of the polynomial hierarchy. Identifying tractable fragments and developing efficient algorithms for such fragments are thus important objectives in order to complement the sophisticated ASP systems available to date. Hard problems can become tractable if some problem parameter is bounded by a fixed constant; such problems are then called fixed-parameter tractable (FPT). While several FPT results for ASP exist, parameters that relate to directed or signed graphs representing the program at hand have been neglected so far. In this paper, we first give some negative observations showing that directed width measures on the dependency graph of a program do not lead to FPT results. We then consider the graph parameter of signed clique-width and present a novel dynamic programming algorithm that is FPT w.r.t. this parameter. Clique-width is more general than the well-known treewidth, and, to the best of our knowledge, ours is the first FPT algorithm for bounded clique-width for reasoning problems beyond SAT.", "label": "1", "is_selected": "0", "text": "Disjunctive Answer Set Programming (ASP) is a powerful declarative programming paradigm whose main decision problems are located on the second level of the polynomial hierarchy. Identifying tractable fragments and developing efficient algorithms for such fragments are thus important objectives in order to complement the sophisticated ASP systems available to date. Hard problems can become tractable if some problem parameter is bounded by a fixed constant; such problems are then called fixed-parameter tractable (FPT). While several FPT results for ASP exist, parameters that relate to directed or signed graphs representing the program at hand have been neglected so far. In this paper, we first give some negative observations showing that directed width measures on the dependency graph of a program do not lead to FPT results. We then consider the graph parameter of signed clique-width and present a novel dynamic programming algorithm that is FPT w.r.t. this parameter. Clique-width is more general than the well-known treewidth, and, to the best of our knowledge, ours is the first FPT algorithm for bounded clique-width for reasoning problems beyond SAT."}
{"original_text": "that capture users' dynamic intents by modeling user sequential behaviors can recommend closely accurate products to users. Previous work on is mostly focused on optimizing the recommendation accuracy, often ignoring other important aspects such as diversity, novelty, and serendipity. It is important to consider these factors because they play a crucial role in keeping users engaged and satisfied with the system. For example, recommending only popular items may improve accuracy but can lead to a lack of diversity in recommendations. One way to address this issue is by incorporating diversity and novelty into the recommendation algorithm. This can be done by introducing diversity constraints or by measuring the novelty of recommended items. Another approach is to use serendipity, which involves recommending items outside the user's typical set of preferences, but still relevant to their interests. Overall, optimizing for accuracy alone may not be enough to build a successful recommendation system. It is important to balance accuracy with diversity, novelty, and serendipity to ensure a satisfying user experience.", "label": "0", "is_selected": "0", "text": "that capture users' dynamic intents by modeling user sequential behaviors can recommend closely accurate products to users. Previous work on is mostly focused on optimizing the recommendation accuracy, often ignoring other important aspects such as diversity, novelty, and serendipity. It is important to consider these factors because they play a crucial role in keeping users engaged and satisfied with the system. For example, recommending only popular items may improve accuracy but can lead to a lack of diversity in recommendations. One way to address this issue is by incorporating diversity and novelty into the recommendation algorithm. This can be done by introducing diversity constraints or by measuring the novelty of recommended items. Another approach is to use serendipity, which involves recommending items outside the user's typical set of preferences, but still relevant to their interests. Overall, optimizing for accuracy alone may not be enough to build a successful recommendation system. It is important to balance accuracy with diversity, novelty, and serendipity to ensure a satisfying user experience."}
{"original_text": "In the domain of emergency management during hazard crises, having sufficient situational awareness information is critical. It requires capturing and integrating information from sources such as satellite images, local sensors and social media content generated by local people. A bold obstacle to capturing, representing and integrating such heterogeneous and diverse information is lack of a proper ontology which properly conceptualizes this domain, aggregates and unifies datasets. Thus, in this paper, we introduce empathi ontology which conceptualizes the core concepts concerning with the domain of emergency managing and planning of hazard crises. Although empathi has a coarse-grained view, it considers the necessary concepts and relations being essential in this domain. This ontology is available at", "label": "1", "is_selected": "0", "text": "In the domain of emergency management during hazard crises, having sufficient situational awareness information is critical. It requires capturing and integrating information from sources such as satellite images, local sensors and social media content generated by local people. A bold obstacle to capturing, representing and integrating such heterogeneous and diverse information is lack of a proper ontology which properly conceptualizes this domain, aggregates and unifies datasets. Thus, in this paper, we introduce empathi ontology which conceptualizes the core concepts concerning with the domain of emergency managing and planning of hazard crises. Although empathi has a coarse-grained view, it considers the necessary concepts and relations being essential in this domain. This ontology is available at"}
{"original_text": "A novel method for distributed estimation of the frequency of power systems is introduced based on the cooperation between multiple measurement nodes. The proposed distributed widely linear complex Kalman filter (D-ACKF) and the distributed widely linear extended complex Kalman filter (D-AECKF) employ a widely linear state space and augmented complex statistics to deal with unbalanced system conditions and the generality complex signals, both second order circular (proper) and second order noncircular (improper). It is shown that the current, strictly linear, estimators are inadequate for unbalanced systems, a typical case in smart grids, as they do not account for either the noncircularity of Clarke's a b voltage in unbalanced conditions or the correlated nature of nodal disturbances. We illuminate the relationship between the degree of circularity of Clarke's voltage and system imbalance, and prove that the proposed widely linear estimators are optimal for such conditions, while also accounting for the correlated and noncircular nature of real-world nodal disturbances. Synthetic and real world case studies over a range of power system conditions illustrate the theoretical and practical advantages of the proposed methodology.", "label": "1", "is_selected": "0", "text": "A novel method for distributed estimation of the frequency of power systems is introduced based on the cooperation between multiple measurement nodes. The proposed distributed widely linear complex Kalman filter (D-ACKF) and the distributed widely linear extended complex Kalman filter (D-AECKF) employ a widely linear state space and augmented complex statistics to deal with unbalanced system conditions and the generality complex signals, both second order circular (proper) and second order noncircular (improper). It is shown that the current, strictly linear, estimators are inadequate for unbalanced systems, a typical case in smart grids, as they do not account for either the noncircularity of Clarke's a b voltage in unbalanced conditions or the correlated nature of nodal disturbances. We illuminate the relationship between the degree of circularity of Clarke's voltage and system imbalance, and prove that the proposed widely linear estimators are optimal for such conditions, while also accounting for the correlated and noncircular nature of real-world nodal disturbances. Synthetic and real world case studies over a range of power system conditions illustrate the theoretical and practical advantages of the proposed methodology."}
{"original_text": "Breast cancer screening is one of the most common radiological tasks with over 39 million exams performed each year. While breast cancer screening has been one of the most studied medical imaging applications of artificial intelligence, the development and evaluation of the algorithms are hindered due to the lack of well-annotated large-scale publicly available datasets. This is particularly an issue for digital breast tomosynthesis (DBT) which is a relatively new breast cancer screening modality. We have curated and made publicly available a large-scale dataset of digital breast tomosynthesis images. It contains 22,032 reconstructed DBT volumes belonging to 5,610 studies from 5,060 patients. This included four groups: (1) 5,129 normal studies, (2) 280 studies where additional imaging was needed but no biopsy was performed, (3) 112 benign biopsied studies, and (4) 89 studies with cancer. Our dataset included masses and architectural distortions which were annotated by two experienced radiologists. Additionally, we developed a single-phase deep learning detection model and tested it using our dataset to serve as a baseline for future research. Our model reached a sensitivity of 65 at 2 false positives per breast. Our large, diverse, and highly-curated dataset will facilitate development and evaluation of AI algorithms for breast cancer screening through providing data for training as well as common set of cases for model validation. The performance of the model developed in our study shows that the task remains challenging and will serve as a baseline for future model development. Keywords: digital breast tomosynthesis; deep learning; detection", "label": "1", "is_selected": "0", "text": "Breast cancer screening is one of the most common radiological tasks with over 39 million exams performed each year. While breast cancer screening has been one of the most studied medical imaging applications of artificial intelligence, the development and evaluation of the algorithms are hindered due to the lack of well-annotated large-scale publicly available datasets. This is particularly an issue for digital breast tomosynthesis (DBT) which is a relatively new breast cancer screening modality. We have curated and made publicly available a large-scale dataset of digital breast tomosynthesis images. It contains 22,032 reconstructed DBT volumes belonging to 5,610 studies from 5,060 patients. This included four groups: (1) 5,129 normal studies, (2) 280 studies where additional imaging was needed but no biopsy was performed, (3) 112 benign biopsied studies, and (4) 89 studies with cancer. Our dataset included masses and architectural distortions which were annotated by two experienced radiologists. Additionally, we developed a single-phase deep learning detection model and tested it using our dataset to serve as a baseline for future research. Our model reached a sensitivity of 65 at 2 false positives per breast. Our large, diverse, and highly-curated dataset will facilitate development and evaluation of AI algorithms for breast cancer screening through providing data for training as well as common set of cases for model validation. The performance of the model developed in our study shows that the task remains challenging and will serve as a baseline for future model development. Keywords: digital breast tomosynthesis; deep learning; detection"}
{"original_text": "We propose a system to develop a basic automatic speech recognizer (ASR) for Cantonese, a low-resource language, through transfer learning of Mandarin, a high-resource language. We take a time-delayed neural network (TDNN) as our base model and fine-tune the model on a small amount of Cantonese data. The first step is to pretrain the TDNN on large amounts of Mandarin data. We can use an existing Mandarin ASR dataset such as AISHELL for this purpose. The pretrained model is then adapted to the Cantonese language by fine-tuning on a small Cantonese dataset. We can use a Cantonese speech dataset like HKUST as our target data. During fine-tuning, the model's weights are updated to better match the Cantonese acoustic characteristics. We can also perform data augmentation techniques such as speed perturbation and adding background noise to increase the robustness of the model. Once the fine-tuning is complete, we can evaluate the model on a separate test set to measure its speech recognition accuracy. We can also compare the model's performance to other Cantonese ASR systems. In summary, transfer learning of a Mandarin ASR model to Cantonese can be a useful approach for developing a basic ASR system for low-resource languages. It can save time and resources while still achieving high accuracy.", "label": "0", "is_selected": "1", "text": "In this paper, we present a novel approach to develop a basic speech recognition system for a low-resource language, which can save time and resources and can measure its performance."}
{"original_text": "The design of software systems inevitably enacts normative boundaries aroundthe site of intervention. These boundaries are, in part, a reflection of thevalues, ethics, power, and politics of the situation and its stakeholders. As a responsible software assistant, it is important to recognize and be mindful of these normative boundaries, as they can have significant implications for the users and the broader society. For example, a software system designed for tracking employee productivity may have a normative boundary that prioritizes efficiency and productivity over employee well-being and mental health. This boundary reflects the values and power dynamics of the business stakeholders who prioritize profit and performance. As an assistant, it is important to recognize the impact of such boundaries on users and advocate for the integration of ethical and social considerations in system design. Therefore, as a helpful assistant, it is essential to take a proactive approach in ethical and social considerations and participate in the discussions around the values, ethics, and social implications of the software system. It is also essential to gather feedback from users and stakeholders and ensure that the system design reflects the values and norms of the broader society. By doing so, we can ensure that the software system is designed to be inclusive, user-friendly, and equitable, while also addressing the ethical and social implications of the system.", "label": "0", "is_selected": "1", "text": "In our series of letters from African journalists, film-maker, and columnist Sharmila Tagore reflects on the role of a software assistant in the development and production of software systems."}
{"original_text": "Synergistic interactions are ubiquitous in the real world. Recent studies have revealed that, for a single-layer network, synergy can enhance spreading and even induce an explosive contagion. There is at the present a growing interest in behavior spreading dynamics on multiplex networks. What is the role of synergistic interactions in behavior spreading in such networked systems? To address this question, we articulate a synergistic behavior spreading model on a double layer network, where the key manifestation of the synergistic interactions is that the adoption of one behavior by a node in one layer enhances its probability of adopting the behavior in the other layer. A general result is that synergistic interactions can greatly enhance the spreading of the behaviors in both layers. A remarkable phenomenon is that the interactions can alter the nature of the phase transition associated with behavior adoption or spreading dynamics. In particular, depending on the transmission rate of one behavior in a network layer, synergistic interactions can lead to a discontinuous (first-order) or a continuous (second-order) transition in the adoption scope of the other behavior with respect to its transmission rate. A surprising two-stage spreading process can arise: due to synergy, nodes having adopted one behavior in one layer adopt the other behavior in the other layer and then prompt the remaining nodes in this layer to quickly adopt the behavior. Analytically, we develop an edge-based compartmental theory and perform a bifurcation analysis to fully understand, in the weak synergistic interaction regime where the dynamical correlation between the network layers is negligible, the role of the interactions in promoting the social behavioral spreading dynamics in the whole system.", "label": "1", "is_selected": "0", "text": "Synergistic interactions are ubiquitous in the real world. Recent studies have revealed that, for a single-layer network, synergy can enhance spreading and even induce an explosive contagion. There is at the present a growing interest in behavior spreading dynamics on multiplex networks. What is the role of synergistic interactions in behavior spreading in such networked systems? To address this question, we articulate a synergistic behavior spreading model on a double layer network, where the key manifestation of the synergistic interactions is that the adoption of one behavior by a node in one layer enhances its probability of adopting the behavior in the other layer. A general result is that synergistic interactions can greatly enhance the spreading of the behaviors in both layers. A remarkable phenomenon is that the interactions can alter the nature of the phase transition associated with behavior adoption or spreading dynamics. In particular, depending on the transmission rate of one behavior in a network layer, synergistic interactions can lead to a discontinuous (first-order) or a continuous (second-order) transition in the adoption scope of the other behavior with respect to its transmission rate. A surprising two-stage spreading process can arise: due to synergy, nodes having adopted one behavior in one layer adopt the other behavior in the other layer and then prompt the remaining nodes in this layer to quickly adopt the behavior. Analytically, we develop an edge-based compartmental theory and perform a bifurcation analysis to fully understand, in the weak synergistic interaction regime where the dynamical correlation between the network layers is negligible, the role of the interactions in promoting the social behavioral spreading dynamics in the whole system."}
{"original_text": "The task of event detection and classification is central to most information retrieval applications. We show that a Transformer based architecture can effectively model event extraction as a sequence labeling task. We propose a combination of sentence level and token level training objectives that significantly boosts the performance of a BERT based event extraction model. Our approach achieves a new state-of-the-art performance on ACE 2005 data for English and Chinese. We also test our model on ERE Spanish, achieving an average gain of 2 absolute F 1 points over prior best performing models.", "label": "1", "is_selected": "0", "text": "The task of event detection and classification is central to most information retrieval applications. We show that a Transformer based architecture can effectively model event extraction as a sequence labeling task. We propose a combination of sentence level and token level training objectives that significantly boosts the performance of a BERT based event extraction model. Our approach achieves a new state-of-the-art performance on ACE 2005 data for English and Chinese. We also test our model on ERE Spanish, achieving an average gain of 2 absolute F 1 points over prior best performing models."}
{"original_text": "This paper explores feedback systems using incremental redundancy (IR) with noiseless transmitter confirmation (NTC). For IR-NTC systems based on finite-length codes (with blocklength N) and decoding attempts only at certain specified decoding times, this paper presents the asymptotic expansion achieved by random coding, provides rate-compatible sphere-packing (RCSP) performance approximations, and presents simulation results of tail-biting convolutional codes. The information-theoretic analysis shows that values of N relatively close to the expected latency yield the same random-coding achievability expansion as with N . However, the penalty introduced in the expansion by limiting decoding times is linear in the interval between decoding times. For binary symmetric channels, the RCSP approximation provides an efficiently-computed approximation of performance that shows excellent agreement with a family of rate-compatible, tail-biting convolutional codes in the short-latency regime. For the additive white Gaussian noise channel, bounded-distance decoding simplifies the computation of the marginal RCSP approximation and produces similar results as analysis based on maximum-likelihood decoding for latencies greater than 200. The efficiency of the marginal RCSP approximation facilitates optimization of the lengths of incremental transmissions when the number of incremental transmissions is constrained to be small or the length of the incremental transmissions is constrained to be uniform after the first transmission. Finally, an RCSP-based decoding error trajectory is introduced that provides target error rates for the design of rate-compatible code families for use in feedback communication systems.", "label": "1", "is_selected": "0", "text": "This paper explores feedback systems using incremental redundancy (IR) with noiseless transmitter confirmation (NTC). For IR-NTC systems based on finite-length codes (with blocklength N) and decoding attempts only at certain specified decoding times, this paper presents the asymptotic expansion achieved by random coding, provides rate-compatible sphere-packing (RCSP) performance approximations, and presents simulation results of tail-biting convolutional codes. The information-theoretic analysis shows that values of N relatively close to the expected latency yield the same random-coding achievability expansion as with N . However, the penalty introduced in the expansion by limiting decoding times is linear in the interval between decoding times. For binary symmetric channels, the RCSP approximation provides an efficiently-computed approximation of performance that shows excellent agreement with a family of rate-compatible, tail-biting convolutional codes in the short-latency regime. For the additive white Gaussian noise channel, bounded-distance decoding simplifies the computation of the marginal RCSP approximation and produces similar results as analysis based on maximum-likelihood decoding for latencies greater than 200. The efficiency of the marginal RCSP approximation facilitates optimization of the lengths of incremental transmissions when the number of incremental transmissions is constrained to be small or the length of the incremental transmissions is constrained to be uniform after the first transmission. Finally, an RCSP-based decoding error trajectory is introduced that provides target error rates for the design of rate-compatible code families for use in feedback communication systems."}
{"original_text": "The concept of nestedness, in particular for ecological and economical networks, has been introduced as a structural characteristic of real interacting systems. We suggest that the nestedness is in fact another way to express a mesoscale network property called the core-periphery structure. With real ecological mutualistic networks and synthetic model networks, we reveal the strong correlation between the nestedness and core-periphery-ness (likeness to the core-periphery structure), by defining the network-level measures for nestedness and core-periphery-ness in the case of weighted and bipartite networks. However, at the same time, via more sophisticated null-model analysis, we also discover that the degree (the number of connected neighbors of a node) distribution poses quite severe restrictions on the possible nestedness and core-periphery-ness parameter space. Therefore, there must exist structurally interwoven properties in more fundamental levels of network formation, behind this seemingly obvious relation between nestedness and core-periphery structures.", "label": "1", "is_selected": "0", "text": "The concept of nestedness, in particular for ecological and economical networks, has been introduced as a structural characteristic of real interacting systems. We suggest that the nestedness is in fact another way to express a mesoscale network property called the core-periphery structure. With real ecological mutualistic networks and synthetic model networks, we reveal the strong correlation between the nestedness and core-periphery-ness (likeness to the core-periphery structure), by defining the network-level measures for nestedness and core-periphery-ness in the case of weighted and bipartite networks. However, at the same time, via more sophisticated null-model analysis, we also discover that the degree (the number of connected neighbors of a node) distribution poses quite severe restrictions on the possible nestedness and core-periphery-ness parameter space. Therefore, there must exist structurally interwoven properties in more fundamental levels of network formation, behind this seemingly obvious relation between nestedness and core-periphery structures."}
{"original_text": "The contemporary literature on cloud resource allocation is mostly focused on studying the interactions between customers and cloud managers. Nevertheless, the recent growth in the customers' demands and the emergence of private cloud providers (CPs) entice the cloud managers to rent extra resources from the CPs so as to handle their backlogged tasks and attract more customers. This also makes studying the interactions between the cloud managers and the CPs essential. In this paper, we investigate both of the mentioned interactions. For the interactions between customers and cloud managers, we adopt the options-based sequential auctions (OBSAs) to the cloud resource allocation paradigm. As compared to existing works, our framework can handle customers with heterogeneous demands, provide truthfulness as the dominant strategy, enjoy a simple winner determination, and preclude the delayed entrance issue. We also provide the performance analysis of the OBSAs, which is among the first in literature. For the interactions between cloud managers and CPs, we propose an auction-based scheme for resource gathering. Through incorporating the offered prices, we capture the heterogeneous desires of the CPs in leasing their resources. We conduct a comprehensive mathematical analysis of the two markets and identify the bidding strategy of the cloud managers.", "label": "1", "is_selected": "0", "text": "The contemporary literature on cloud resource allocation is mostly focused on studying the interactions between customers and cloud managers. Nevertheless, the recent growth in the customers' demands and the emergence of private cloud providers (CPs) entice the cloud managers to rent extra resources from the CPs so as to handle their backlogged tasks and attract more customers. This also makes studying the interactions between the cloud managers and the CPs essential. In this paper, we investigate both of the mentioned interactions. For the interactions between customers and cloud managers, we adopt the options-based sequential auctions (OBSAs) to the cloud resource allocation paradigm. As compared to existing works, our framework can handle customers with heterogeneous demands, provide truthfulness as the dominant strategy, enjoy a simple winner determination, and preclude the delayed entrance issue. We also provide the performance analysis of the OBSAs, which is among the first in literature. For the interactions between cloud managers and CPs, we propose an auction-based scheme for resource gathering. Through incorporating the offered prices, we capture the heterogeneous desires of the CPs in leasing their resources. We conduct a comprehensive mathematical analysis of the two markets and identify the bidding strategy of the cloud managers."}
{"original_text": "We present a polynomial-space algorithm that computes the number of independent sets of any input graph in time O (1.1389 n) for graphs with maximum degree 3 and in time O (1.2356 n) for general graphs, where n is the number of vertices. Together with the inclusion-exclusion approach of Bjorklund, Husfeldt, and Koivisto [SIAM J. Comput. 2009], this leads to a faster polynomial-space algorithm for the graph coloring problem with running time O (2.2356 n). As a byproduct, we also obtain an exponential-space O (1.2330 n) time algorithm for counting independent sets. Our main algorithm counts independent sets in graphs with maximum degree 3 and no vertex with three neighbors of degree 3. This polynomial-space algorithm is analyzed using the recently introduced Separate, Measure and Conquer approach [Gaspers Sorkin, ICALP 2015]. Using Wahlstrom's compound measure approach, this improvement in running time for small degree graphs is then bootstrapped to larger degrees, giving the improvement for general graphs. Combining both approaches leads to some inflexibility in choosing vertices to branch on for the small-degree cases, which we counter by structural graph properties. The main complication is to upper bound the number of times the algorithm has to branch on vertices all of whose neighbors have degree 2, while still decreasing the size of the separator each time the algorithm branches.", "label": "1", "is_selected": "0", "text": "We present a polynomial-space algorithm that computes the number of independent sets of any input graph in time O (1.1389 n) for graphs with maximum degree 3 and in time O (1.2356 n) for general graphs, where n is the number of vertices. Together with the inclusion-exclusion approach of Bjorklund, Husfeldt, and Koivisto [SIAM J. Comput. 2009], this leads to a faster polynomial-space algorithm for the graph coloring problem with running time O (2.2356 n). As a byproduct, we also obtain an exponential-space O (1.2330 n) time algorithm for counting independent sets. Our main algorithm counts independent sets in graphs with maximum degree 3 and no vertex with three neighbors of degree 3. This polynomial-space algorithm is analyzed using the recently introduced Separate, Measure and Conquer approach [Gaspers Sorkin, ICALP 2015]. Using Wahlstrom's compound measure approach, this improvement in running time for small degree graphs is then bootstrapped to larger degrees, giving the improvement for general graphs. Combining both approaches leads to some inflexibility in choosing vertices to branch on for the small-degree cases, which we counter by structural graph properties. The main complication is to upper bound the number of times the algorithm has to branch on vertices all of whose neighbors have degree 2, while still decreasing the size of the separator each time the algorithm branches."}
{"original_text": "Evolving graphs arise in problems where interrelations between data change over time. We present a breadth first search (BFS) algorithm for evolving graphs that computes the most direct influences between nodes at two different times. Using simple examples, we show that naive unfoldings of adjacency matrices miscount the number of temporal paths. By mapping an evolving graph to an adjacency matrix of an equivalent static graph, we prove that our generalization of the BFS algorithm correctly accounts for paths that traverse both space and time. Finally, we demonstrate how the BFS over evolving graphs can be applied to mine citation networks.", "label": "1", "is_selected": "0", "text": "Evolving graphs arise in problems where interrelations between data change over time. We present a breadth first search (BFS) algorithm for evolving graphs that computes the most direct influences between nodes at two different times. Using simple examples, we show that naive unfoldings of adjacency matrices miscount the number of temporal paths. By mapping an evolving graph to an adjacency matrix of an equivalent static graph, we prove that our generalization of the BFS algorithm correctly accounts for paths that traverse both space and time. Finally, we demonstrate how the BFS over evolving graphs can be applied to mine citation networks."}
{"original_text": "This note further studies the previously proposed consensus protocol for linear multi-agent systems with communication noises in. Each agent is allowed to have its own time-varying gain to attenuate the impact of communication noises. The consensus protocol can be formulated as: xi (k1) sum{j1}N a{ij} (k) (xj (k) - xi (k ui (k) where xi (k) represents the state of agent i at time step k, a{ij} (k) represents the time-varying gain associated with the communication link from agent j to agent i at time step k, and ui (k) represents the control input of agent i at time step k. To design the time-varying gains a{ij} (k), we can use the following guidelines: 1. The gains should decrease as the noise level increases to attenuate the impact of communication noises. One possible form of time-varying gains is: a{ij} (k) alpha{ij} (k) (beta{ij} (k) gamma (k where alpha{ij} (k) and beta{ij} (k) are positive time-varying functions that depend on the noise level and the quality of the communication link from agent j to agent i, and gamma (k) is a positive time-varying function that represents the noise level at time step k. 2. The gains should satisfy the following constraints to ensure that the consensus protocol is stable: - The gains should be symmetric, i.e., a{ij} (k) a{ji} (k). - The gains should be non-negative, i.e., a{ij} (k) 0. - The sum of gains associated with each agent should be less than 1, i.e., sum{j1}N a{ij} (k) 1. By following these guidelines, we can design a consensus protocol that allows each agent to adapt its gain to the noise level and the quality of the communication link, while ensuring the stability of the overall system.", "label": "0", "is_selected": "1", "text": "A consensus protocol for linear multi-agent systems with communication noises in has been proposed and tested in a paper published in the Journal of the American Mathematical Society."}
{"original_text": "To date, most studies on spam have focused only on the spamming phase of the spam cycle and have ignored the harvesting phase, which consists of the mass acquisition of email addresses for the purpose of sending spam. However, understanding the harvesting phase is important for developing effective strategies to prevent or reduce spam. The harvesting phase involves the use of automated tools called \"harvesters\" that scan websites, forums, social media sites, and other online sources to collect email addresses. In order to prevent or reduce email harvesting, website owners can employ tactics such as obfuscating email addresses (e.g. replacing the symbol with \"AT, using contact forms instead of displaying email addresses, and limiting access to email addresses through user permissions. Users can also take steps to protect their email addresses, such as using a unique email address for online accounts, avoiding posting email addresses publicly, and using spam filters.", "label": "0", "is_selected": "1", "text": "This paper explores the harvesting phase of spam and how it can be prevented and reduces the amount of spam being sent to email addresses in the public domain."}
{"original_text": "We consider the problem of reconstructing a rank-one matrix from a revealed subset of its entries when some of the revealed entries are corrupted with perturbations that are unknown and random. This problem arises in various applications, such as in low-rank matrix completion and collaborative filtering. To address this problem, one approach is to use convex optimization techniques, such as nuclear-norm minimization or matrix factorization. These methods aim to minimize the difference between the observed and reconstructed entries, subject to some constraints on the rank of the reconstructed matrix. Another approach is to use probabilistic models, such as the matrix completion Gaussian process (MCGP) or the Bayesian low-rank matrix completion model. These models typically assume a prior distribution over the unknown entries and use Bayesian inference to estimate the parameters of the model and reconstruct the matrix. Overall, the choice of method depends on the specific application and the available resources for computation and data acquisition.", "label": "0", "is_selected": "1", "text": "In this paper, we consider a variety of approaches to the reconstruction of a rank-one matrix from a revealed subset of its entries when some of the revealed entries are corrupted."}
{"original_text": "We design and implement an end-to-end system for real-time crime detection in low-light environments. Unlike Closed-Circuit Television, which performs reactively, the Low-Light Environment Neural Surveillance provides real time crime alerts. The system uses a low-light video feed processed in real-time by an optical-flow network, spatial and temporal networks, and a Support Vector Machine to identify shootings, assaults, and thefts. We create a low-light action-recognition dataset, LENS-4, which will be publicly available. An IoT infrastructure set up via Amazon Web Services interprets messages from the local board hosting the camera for action recognition and parses the results in the cloud to relay messages. The system achieves 71.5 accuracy at 20 FPS. The user interface is a mobile app which allows local authorities to receive notifications and to view a video of the crime scene. Citizens have a public app which enables law enforcement to push crime alerts based on user proximity.", "label": "1", "is_selected": "0", "text": "We design and implement an end-to-end system for real-time crime detection in low-light environments. Unlike Closed-Circuit Television, which performs reactively, the Low-Light Environment Neural Surveillance provides real time crime alerts. The system uses a low-light video feed processed in real-time by an optical-flow network, spatial and temporal networks, and a Support Vector Machine to identify shootings, assaults, and thefts. We create a low-light action-recognition dataset, LENS-4, which will be publicly available. An IoT infrastructure set up via Amazon Web Services interprets messages from the local board hosting the camera for action recognition and parses the results in the cloud to relay messages. The system achieves 71.5 accuracy at 20 FPS. The user interface is a mobile app which allows local authorities to receive notifications and to view a video of the crime scene. Citizens have a public app which enables law enforcement to push crime alerts based on user proximity."}
{"original_text": "Many clustering algorithms exist that estimate a cluster centroid, such as K -means, K -medoids or mean-shift, but no algorithm seems to exist that clusters data by returning exactly K meaningful modes. We propose a natural definition of a K -modes objective function by combining the notions of density and cluster assignment. The algorithm becomes K -means and K -medoids in the limit of very large and very small scales. Computationally, it is slightly slower than K -means but much faster than mean-shift or K -medoids. Unlike K -means, it is able to find centroids that are valid patterns, truly representative of a cluster, even with nonconvex clusters, and appears robust to outliers and misspecification of the scale and number of clusters.", "label": "1", "is_selected": "0", "text": "Many clustering algorithms exist that estimate a cluster centroid, such as K -means, K -medoids or mean-shift, but no algorithm seems to exist that clusters data by returning exactly K meaningful modes. We propose a natural definition of a K -modes objective function by combining the notions of density and cluster assignment. The algorithm becomes K -means and K -medoids in the limit of very large and very small scales. Computationally, it is slightly slower than K -means but much faster than mean-shift or K -medoids. Unlike K -means, it is able to find centroids that are valid patterns, truly representative of a cluster, even with nonconvex clusters, and appears robust to outliers and misspecification of the scale and number of clusters."}
{"original_text": "As light field images continue to increase in use and application, it becomes necessary to adapt existing image processing methods to this unique form of photography. In this paper we explore methods for applying neural style transfer to light field images. Feed-forward style transfer networks provide fast, high-quality results for monocular images, but no such networks exist for full light field images. Because of the size of these images, current light field data sets are small and are insufficient for training purely feed-forward style-transfer networks from scratch. Thus, it is necessary to adapt existing monocular style transfer networks in a way that allows for the stylization of each view of the light field while maintaining visual consistencies between views. To do this, we first generate disparity maps for each view given a single depth image for the light field. Then in a fashion similar to neural stylization of stereo images, we use disparity maps to enforce a consistency loss between views and to warp feature maps during the feed forward stylization. Unlike previous work, however, light fields have too many views to train a purely feed-forward network that can stylize the entire light field with angular consistency. Instead, the proposed method uses an iterative optimization for each view of a single light field image that backpropagates the consistency loss through the network. Thus, the network architecture allows for the incorporation of pre-trained fast monocular stylization network while avoiding the need for a large light field training set.", "label": "1", "is_selected": "0", "text": "As light field images continue to increase in use and application, it becomes necessary to adapt existing image processing methods to this unique form of photography. In this paper we explore methods for applying neural style transfer to light field images. Feed-forward style transfer networks provide fast, high-quality results for monocular images, but no such networks exist for full light field images. Because of the size of these images, current light field data sets are small and are insufficient for training purely feed-forward style-transfer networks from scratch. Thus, it is necessary to adapt existing monocular style transfer networks in a way that allows for the stylization of each view of the light field while maintaining visual consistencies between views. To do this, we first generate disparity maps for each view given a single depth image for the light field. Then in a fashion similar to neural stylization of stereo images, we use disparity maps to enforce a consistency loss between views and to warp feature maps during the feed forward stylization. Unlike previous work, however, light fields have too many views to train a purely feed-forward network that can stylize the entire light field with angular consistency. Instead, the proposed method uses an iterative optimization for each view of a single light field image that backpropagates the consistency loss through the network. Thus, the network architecture allows for the incorporation of pre-trained fast monocular stylization network while avoiding the need for a large light field training set."}
{"original_text": "Consider a collaborative task carried out by two autonomous agents that can communicate over a noisy channel. Each agent is only aware of its own state, while the accomplishment of the task depends on the value of the joint state of both agents. As an example, both agents must simultaneously reach a certain location of the environment, while only being aware of their own positions. Assuming the presence of feedback in the form of a common reward to the agents, a conventional approach would apply separately: (i) an off-the-shelf coding and decoding scheme in order to enhance the reliability of the communication of the state of one agent to the other; and (ii) a standard multi-agent reinforcement learning strategy to learn how to act in the resulting environment. In this work, it is argued that the performance of the collaborative task can be improved if the agents learn how to jointly communicate and act. In particular, numerical results for a baseline grid world example demonstrate that the jointly learned policy carries out compression and unequal error protection by leveraging information about the action policy.", "label": "1", "is_selected": "0", "text": "Consider a collaborative task carried out by two autonomous agents that can communicate over a noisy channel. Each agent is only aware of its own state, while the accomplishment of the task depends on the value of the joint state of both agents. As an example, both agents must simultaneously reach a certain location of the environment, while only being aware of their own positions. Assuming the presence of feedback in the form of a common reward to the agents, a conventional approach would apply separately: (i) an off-the-shelf coding and decoding scheme in order to enhance the reliability of the communication of the state of one agent to the other; and (ii) a standard multi-agent reinforcement learning strategy to learn how to act in the resulting environment. In this work, it is argued that the performance of the collaborative task can be improved if the agents learn how to jointly communicate and act. In particular, numerical results for a baseline grid world example demonstrate that the jointly learned policy carries out compression and unequal error protection by leveraging information about the action policy."}
{"original_text": "Encoding a sequence of observations is an essential task with many applications. The encoding can become highly efficient when the observations are generated by a dynamical system. A dynamical system imposes regularities on the observations that can be leveraged to achieve a more efficient code. We propose a method to encode a given or learned dynamical system. Apart from its application for encoding a sequence of observations, we propose to use the compression achieved by this encoding as a criterion for model selection. Given a dataset, different learning algorithms result in different models. But not all learned models are equally good. We show that the proposed encoding approach can be used to choose the learned model which is closer to the true underlying dynamics. We provide experiments for both encoding and model selection, and theoretical results that shed light on why the approach works.", "label": "1", "is_selected": "0", "text": "Encoding a sequence of observations is an essential task with many applications. The encoding can become highly efficient when the observations are generated by a dynamical system. A dynamical system imposes regularities on the observations that can be leveraged to achieve a more efficient code. We propose a method to encode a given or learned dynamical system. Apart from its application for encoding a sequence of observations, we propose to use the compression achieved by this encoding as a criterion for model selection. Given a dataset, different learning algorithms result in different models. But not all learned models are equally good. We show that the proposed encoding approach can be used to choose the learned model which is closer to the true underlying dynamics. We provide experiments for both encoding and model selection, and theoretical results that shed light on why the approach works."}
{"original_text": "This paper extends the problem of 2-dimensional palindrome search into the area of approximate matching. Using the Hamming distance as the measure, we search for 2D palindromes that allow up to k mismatches. The proposed algorithm involves adapting a dynamic programming-based algorithm for approximate string matching to handle 2D palindromes. The algorithm was evaluated on both synthetic and real-world datasets, and it was shown to achieve competitive performance compared to existing methods. The ability to find approximate 2D palindromes has potential applications in molecular biology, image processing, and natural language processing.", "label": "0", "is_selected": "1", "text": "The search for palindromes in two-dimensional space is one of the most computationally challenging problems in computer science and artificial intelligence (AI)."}
{"original_text": "Ad hoc electrical networks are formed by connecting power sources and loads without pre-determining the network topology. These systems are well-suited to addressing the lack of electricity in rural areas because they allow for a flexible and scalable approach to power distribution. Ad hoc networks can be easily set up and adjusted to meet the specific needs of a community, often relying on renewable energy sources like solar, wind, or hydro power. Additionally, these networks can be maintained and managed by the local community, making them a sustainable solution for long-term energy needs. Overall, ad hoc electrical networks have great potential for providing reliable and affordable electricity to remote and underserved areas.", "label": "0", "is_selected": "1", "text": "This paper explores the use of ad hoc electrical networks to provide electricity to rural areas in the developing world, and how these networks can be used to address the lack of electricity in these areas."}
{"original_text": "As inertial and visual sensors are becoming ubiquitous, visual-inertial navigation systems (VINS) have prevailed in a wide range of applications from mobile augmented reality to aerial navigation to autonomous driving, in part because of the complementary sensing capabilities and the decreasing costs and size of the sensors. In this paper, we survey thoroughly the research efforts taken in this field and strive to provide a concise but complete review of the related work - which is unfortunately missing in the literature while being greatly demanded by researchers and engineers - in the hope to accelerate the VINS research and beyond in our society as a whole.", "label": "1", "is_selected": "0", "text": "As inertial and visual sensors are becoming ubiquitous, visual-inertial navigation systems (VINS) have prevailed in a wide range of applications from mobile augmented reality to aerial navigation to autonomous driving, in part because of the complementary sensing capabilities and the decreasing costs and size of the sensors. In this paper, we survey thoroughly the research efforts taken in this field and strive to provide a concise but complete review of the related work - which is unfortunately missing in the literature while being greatly demanded by researchers and engineers - in the hope to accelerate the VINS research and beyond in our society as a whole."}
{"original_text": "The security of cryptographic communication protocols that use X.509 certificates depends on the correctness of those certificates. This paper proposes a system that helps to ensure the correct operation of X.509 certificates by implementing a proactive monitoring and verification process. This system continuously scans all certificates in use in the network, checking their validity and making sure that they have not been compromised or revoked. Additionally, the system alerts network administrators when a certificate's status changes or when an abnormal pattern is detected, such as an unusually high number of failed authentication attempts. This allows administrators to take action before a security breach occurs. The system also includes a secure audit log that records all certificate-related events, providing a complete record of certificate activity that can be used for forensic analysis in the event of an incident. Overall, this proposed system provides an added layer of security to networks that use X.509 certificates, ensuring that they remain secure and trusted at all times.", "label": "0", "is_selected": "1", "text": "This paper proposes a system that helps to ensure the correct operation of X.509 certificates by implementing a proactive monitoring and verification process.This system continuously scans all certificates in use in the network, checking their validity and making sure that they have not been compromised or revoked."}
{"original_text": "Many sensors, such as range, sonar, radar, GPS and visual devices, produce measurements which are contaminated by outliers. This problem can be addressed by using fat-tailed sensor models, which account for the possibility of outliers. Unfortunately, all estimation algorithms belonging to the family of Gaussian filters (such as the widely-used extended Kalman filter and unscented Kalman filter) are inherently incompatible with such fat-tailed sensor models. The contribution of this paper is to show that any Gaussian filter can be made compatible with fat-tailed sensor models by applying one simple change: Instead of filtering with the physical measurement, we propose to filter with a pseudo measurement obtained by applying a feature function to the physical measurement. We derive such a feature function which is optimal under some conditions. Simulation results show that the proposed method can effectively handle measurement outliers and allows for robust filtering in both linear and nonlinear systems.", "label": "1", "is_selected": "0", "text": "Many sensors, such as range, sonar, radar, GPS and visual devices, produce measurements which are contaminated by outliers. This problem can be addressed by using fat-tailed sensor models, which account for the possibility of outliers. Unfortunately, all estimation algorithms belonging to the family of Gaussian filters (such as the widely-used extended Kalman filter and unscented Kalman filter) are inherently incompatible with such fat-tailed sensor models. The contribution of this paper is to show that any Gaussian filter can be made compatible with fat-tailed sensor models by applying one simple change: Instead of filtering with the physical measurement, we propose to filter with a pseudo measurement obtained by applying a feature function to the physical measurement. We derive such a feature function which is optimal under some conditions. Simulation results show that the proposed method can effectively handle measurement outliers and allows for robust filtering in both linear and nonlinear systems."}
{"original_text": "The paper deals with the problem of deciding if two finite-dimensional linear subspaces over an arbitrary field are identical up to a permutation of the coordinates. This problem is referred to as the subspace isomorphism problem. The paper presents a novel algorithm for solving this problem efficiently. The algorithm uses a combination of linear algebra techniques and graph theory to compute a canonical form for the subspaces, which can then be compared for equivalence. The paper begins by introducing the subspace isomorphism problem and discussing its relevance in various applications. It then describes the algorithm in detail, explaining each step and providing theoretical analysis of its complexity. The authors also present experimental results showing the effectiveness of the algorithm in practice. The main contribution of the paper is a new and efficient approach to solving the subspace isomorphism problem. The algorithm is based on a unique combination of linear algebra and graph theory, and is able to compute a canonical form for subspaces that can be compared for isomorphism. Overall, the paper provides valuable insights and a useful tool for researchers and practitioners working in fields such as computer vision, pattern recognition, and machine learning, where subspaces play a crucial role.", "label": "0", "is_selected": "1", "text": "This paper presents a novel algorithm for solving the subspace isomorphism problem, which is a key problem in the fields of computer vision, pattern recognition, and machine learning."}
{"original_text": "We introduce a general and simple structural design called \"Multiplicative Integration\" (MI) to improve recurrent neural networks (RNNs). MI changes the way in which information from difference sources flows and is integrated in an RNN. Instead of additive integration, where information is simply added together, MI multiplies information from different sources together. This allows for better information flow and representation in the network, as well as improved ability to capture complex dependencies and long-term dependencies. MI is straightforward to implement and can be applied to various types of RNNs, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. In our experiments, we have shown that MI consistently outperforms the traditional additive integration method, achieving state-of-the-art results on several benchmark datasets. Overall, MI is a powerful and versatile tool for improving and optimizing RNNs. Its effectiveness stems from its ability to enhance information flow, improve the representation of complex dependencies, and capture long-term dependencies in a more effective manner.", "label": "0", "is_selected": "1", "text": "In this paper, we present a novel method for improving the performance of neural networks. Its effectiveness stems from its ability to enhance information flow and improve complex dependencies, and capture long-term dependencies in a more effective manner."}
{"original_text": "A lot of research has been focused on secure outsourcing of biometric identification in the context of cloud computing. In such schemes, both the encrypted biometric database and the identification model are stored in the cloud, while the biometric samples are collected locally and sent to the cloud for comparison. The goal is to ensure that the biometric data and the identification model are kept confidential while still allowing for efficient identification of subjects. To achieve this, different cryptographic techniques, such as homomorphic encryption, secure multiparty computation, and functional encryption, have been used to protect the privacy of the biometric data and the identification model. Additionally, access control mechanisms have been employed to restrict the querying of the biometric database to authorized parties only. Protocols for secure outsourcing of biometric identification are still evolving, and ongoing research aims to improve their efficiency, scalability, and security.", "label": "0", "is_selected": "0", "text": "A lot of research has been focused on secure outsourcing of biometric identification in the context of cloud computing. In such schemes, both the encrypted biometric database and the identification model are stored in the cloud, while the biometric samples are collected locally and sent to the cloud for comparison. The goal is to ensure that the biometric data and the identification model are kept confidential while still allowing for efficient identification of subjects. To achieve this, different cryptographic techniques, such as homomorphic encryption, secure multiparty computation, and functional encryption, have been used to protect the privacy of the biometric data and the identification model. Additionally, access control mechanisms have been employed to restrict the querying of the biometric database to authorized parties only. Protocols for secure outsourcing of biometric identification are still evolving, and ongoing research aims to improve their efficiency, scalability, and security."}
{"original_text": "Bitcoin introduced delegation of control over a monetary system from a select few to all who participate in that system. This delegation is known as the decentralization of controlling power That's correct! Bitcoin and other cryptocurrencies allow for peer-to-peer transactions and are not controlled by any central authority or government. Instead, the system operates through a distributed network of nodes and miners who verify transactions and maintain the integrity of the blockchain. This decentralization is one of the defining features of cryptocurrency and provides greater transparency, immutability, and security compared to traditional centralized systems.", "label": "0", "is_selected": "1", "text": "In our series of letters from African journalists, filmmaker, and columnist Farai Sevenzo looks at some of the most common misconceptions about Bitcoin and other cryptocurrencies."}
{"original_text": "Due to their simple construction, LFSRs are commonly used as building blocks in various random number generators. Nonlinear feedforward logic is incorporated in LFSRs to increase the linear complexity of the generated sequence. In this work, we extend the idea of nonlinear feedforward logic to LFSRs over arbitrary finite fields and analyze the statistical properties of the generated sequences. Further, we propose a method of applying nonlinear feedforward logic to word-based s -LFSRs and show that the proposed scheme generates vector sequences that are statistically more balanced than those generated by an existing scheme.", "label": "1", "is_selected": "0", "text": "Due to their simple construction, LFSRs are commonly used as building blocks in various random number generators. Nonlinear feedforward logic is incorporated in LFSRs to increase the linear complexity of the generated sequence. In this work, we extend the idea of nonlinear feedforward logic to LFSRs over arbitrary finite fields and analyze the statistical properties of the generated sequences. Further, we propose a method of applying nonlinear feedforward logic to word-based s -LFSRs and show that the proposed scheme generates vector sequences that are statistically more balanced than those generated by an existing scheme."}
{"original_text": "The field of automatic image inpainting has progressed rapidly in recent years, but no one has yet proposed a standard method of evaluating algorithms. This absence is due to the lack of a ground truth for inpainting, as it is a subjective task that depends on context and individual perception. Additionally, different applications may require different levels of inpainting quality, making it challenging to determine a universal evaluation metric. Nonetheless, researchers use various methods such as visual inspection, user studies, and quantitative measures (e.g., peak signal-to-noise ratio, structural similarity index, and perceptual quality metrics) to evaluate their inpainting algorithms. The development of a standardized benchmark dataset and evaluation protocol could facilitate progress in automatic image inpainting research.", "label": "0", "is_selected": "0", "text": "The field of automatic image inpainting has progressed rapidly in recent years, but no one has yet proposed a standard method of evaluating algorithms. This absence is due to the lack of a ground truth for inpainting, as it is a subjective task that depends on context and individual perception. Additionally, different applications may require different levels of inpainting quality, making it challenging to determine a universal evaluation metric. Nonetheless, researchers use various methods such as visual inspection, user studies, and quantitative measures (e.g., peak signal-to-noise ratio, structural similarity index, and perceptual quality metrics) to evaluate their inpainting algorithms. The development of a standardized benchmark dataset and evaluation protocol could facilitate progress in automatic image inpainting research."}
{"original_text": "The consequences of anthropogenic climate change are extensively debated through scientific papers, newspaper articles, and blogs. Newspaper articles may lack accuracy, while the severity of findings in scientific papers may be too opaque for the public to understand. Social media, however, is a forum where individuals of diverse backgrounds can share their thoughts and opinions. As consumption shifts from old media to new, Twitter has become a valuable resource for analyzing current events and headline news. In this research, we analyze tweets containing the word \"climate\" collected between September 2008 and July 2014. Through use of a previously developed sentiment measurement tool called the Hedonometer, we determine how collective sentiment varies in response to climate change news, events, and natural disasters. We find that natural disasters, climate bills, and oil-drilling can contribute to a decrease in happiness while climate rallies, a book release, and a green ideas contest can contribute to an increase in happiness. Words uncovered by our analysis suggest that responses to climate change news are predominately from climate change activists rather than climate change deniers, indicating that Twitter is a valuable resource for the spread of climate change awareness.", "label": "1", "is_selected": "0", "text": "The consequences of anthropogenic climate change are extensively debated through scientific papers, newspaper articles, and blogs. Newspaper articles may lack accuracy, while the severity of findings in scientific papers may be too opaque for the public to understand. Social media, however, is a forum where individuals of diverse backgrounds can share their thoughts and opinions. As consumption shifts from old media to new, Twitter has become a valuable resource for analyzing current events and headline news. In this research, we analyze tweets containing the word \"climate\" collected between September 2008 and July 2014. Through use of a previously developed sentiment measurement tool called the Hedonometer, we determine how collective sentiment varies in response to climate change news, events, and natural disasters. We find that natural disasters, climate bills, and oil-drilling can contribute to a decrease in happiness while climate rallies, a book release, and a green ideas contest can contribute to an increase in happiness. Words uncovered by our analysis suggest that responses to climate change news are predominately from climate change activists rather than climate change deniers, indicating that Twitter is a valuable resource for the spread of climate change awareness."}
{"original_text": "In this paper, the authors aim to combine the latest state of the art models in image recognition with the best publicly available satellite images to create a system for landslide risk mitigation. We focus first on landslide detection and further propose a similar system to be used for prediction. Such models are valuable as they could easily be scaled up to provide data for hazard evaluation, as satellite imagery becomes increasingly available. The goal is to use satellite images and correlated data to enrich the public repository of data and guide disaster relief efforts for locating precise areas where landslides have occurred. Different image augmentation methods are used to increase diversity in the chosen dataset and create more robust classification. The resulting outputs are then fed into variants of 3-D convolutional neural networks. A review of the current literature indicates there is no research using CNNs (Convolutional Neural Networks) and freely available satellite imagery for classifying landslide risk. The model has shown to be ultimately able to achieve a significantly better than baseline accuracy.", "label": "1", "is_selected": "0", "text": "In this paper, the authors aim to combine the latest state of the art models in image recognition with the best publicly available satellite images to create a system for landslide risk mitigation. We focus first on landslide detection and further propose a similar system to be used for prediction. Such models are valuable as they could easily be scaled up to provide data for hazard evaluation, as satellite imagery becomes increasingly available. The goal is to use satellite images and correlated data to enrich the public repository of data and guide disaster relief efforts for locating precise areas where landslides have occurred. Different image augmentation methods are used to increase diversity in the chosen dataset and create more robust classification. The resulting outputs are then fed into variants of 3-D convolutional neural networks. A review of the current literature indicates there is no research using CNNs (Convolutional Neural Networks) and freely available satellite imagery for classifying landslide risk. The model has shown to be ultimately able to achieve a significantly better than baseline accuracy."}
{"original_text": "This work investigates the consensus problem for multi-agent nonlinear systems through the distributed real-time nonlinear receding horizon control methodology. With this work, we develop a scheme to reach the consensus state for a group of agents with nonlinear dynamics. The proposed control strategy enables agents to update their control inputs in a receding horizon manner, based on local measurements and distributed communication with neighboring agents. The approach relies on a nonlinear programming formulation that optimizes the decentralized control problem over a finite-time horizon, in order to steer the agents towards a common state. The method also incorporates a distributed real-time solver to rapidly solve the optimization problem over the horizon. By implementing this approach, we show that agents can achieve consensus despite nonlinear dynamics, communication delays, and noise. The effectiveness of the proposed control strategy is validated through simulation studies, where we compare its performance to other existing control schemes. Overall, this work provides valuable insights into the design of distributed control techniques for multi-agent systems, and demonstrates the potential for real-time optimization to enhance the performance of nonlinear control systems.", "label": "0", "is_selected": "0", "text": "This work investigates the consensus problem for multi-agent nonlinear systems through the distributed real-time nonlinear receding horizon control methodology. With this work, we develop a scheme to reach the consensus state for a group of agents with nonlinear dynamics. The proposed control strategy enables agents to update their control inputs in a receding horizon manner, based on local measurements and distributed communication with neighboring agents. The approach relies on a nonlinear programming formulation that optimizes the decentralized control problem over a finite-time horizon, in order to steer the agents towards a common state. The method also incorporates a distributed real-time solver to rapidly solve the optimization problem over the horizon. By implementing this approach, we show that agents can achieve consensus despite nonlinear dynamics, communication delays, and noise. The effectiveness of the proposed control strategy is validated through simulation studies, where we compare its performance to other existing control schemes. Overall, this work provides valuable insights into the design of distributed control techniques for multi-agent systems, and demonstrates the potential for real-time optimization to enhance the performance of nonlinear control systems."}
{"original_text": "Frequent pattern mining is a key area of study that gives insights into the structure and dynamics of evolving networks, such as social or road networks. However, not only does it have applications in network analysis, frequent pattern mining is also widely used in various industries like e-commerce, marketing, and healthcare. In e-commerce, frequent pattern mining is used to identify patterns in consumer behavior that can be leveraged to boost sales or create personalized recommendations. In marketing, it can be used to target specific customer segments with tailored advertising campaigns. In healthcare, frequent pattern mining can help identify patterns in medical data that can be used to improve treatment outcomes and reduce healthcare costs. Overall, frequent pattern mining is a powerful tool for extracting insights from large datasets and can be applied in many different fields.", "label": "0", "is_selected": "1", "text": "Frequent pattern mining is a technique used to extract insights from large data sets, such as social media posts, online searches, and online shopping habits, in order to improve decision-making."}
{"original_text": "Dense subgraph discovery is a key primitive in many graph mining applications, such as detecting communities in social networks and mining gene correlation from biological data. Most studies on dense subgraph discovery focus on identifying a subgraph with a high density of edges, relative to the number of vertices in the subgraph. There are different algorithms and methods for this, such as clique finding methods, k-core decomposition, and maximum independent set algorithms. The most popular approach is probably the Bron-Kerbosch algorithm, which is an efficient way to find all cliques in a graph. Other methods include edge-betweenness clustering and modularity maximization. These algorithms can detect dense subgraphs of different sizes and structures, depending on the application and the properties of the input graph.", "label": "0", "is_selected": "0", "text": "Dense subgraph discovery is a key primitive in many graph mining applications, such as detecting communities in social networks and mining gene correlation from biological data. Most studies on dense subgraph discovery focus on identifying a subgraph with a high density of edges, relative to the number of vertices in the subgraph. There are different algorithms and methods for this, such as clique finding methods, k-core decomposition, and maximum independent set algorithms. The most popular approach is probably the Bron-Kerbosch algorithm, which is an efficient way to find all cliques in a graph. Other methods include edge-betweenness clustering and modularity maximization. These algorithms can detect dense subgraphs of different sizes and structures, depending on the application and the properties of the input graph."}
{"original_text": "Recent transient-execution attacks, such as RIDL, Fallout, and ZombieLoad, demonstrated that attackers can leak information while it transits through microarchitectural buffers. Named Microarchitectural Data Sampling (MDS) by Intel, these attacks are likened to \"drinking from the firehose,\" as the attacker has little control over what data is observed and from what origin. Unable to prevent the buffers from leaking, Intel issued countermeasures via microcode updates that overwrite the buffers when the CPU changes security domains. In this work we present CacheOut, a new microarchitectural attack that is capable of bypassing Intel's buffer overwrite countermeasures. We observe that as data is being evicted from the CPU's L1 cache, it is often transferred back to the leaky CPU buffers where it can be recovered by the attacker. CacheOut improves over previous MDS attacks by allowing the attacker to choose which data to leak from the CPU's L1 cache, as well as which part of a cache line to leak. We demonstrate that CacheOut can leak information across multiple security boundaries, including those between processes, virtual machines, user and kernel space, and from SGX enclaves.", "label": "1", "is_selected": "0", "text": "Recent transient-execution attacks, such as RIDL, Fallout, and ZombieLoad, demonstrated that attackers can leak information while it transits through microarchitectural buffers. Named Microarchitectural Data Sampling (MDS) by Intel, these attacks are likened to \"drinking from the firehose,\" as the attacker has little control over what data is observed and from what origin. Unable to prevent the buffers from leaking, Intel issued countermeasures via microcode updates that overwrite the buffers when the CPU changes security domains. In this work we present CacheOut, a new microarchitectural attack that is capable of bypassing Intel's buffer overwrite countermeasures. We observe that as data is being evicted from the CPU's L1 cache, it is often transferred back to the leaky CPU buffers where it can be recovered by the attacker. CacheOut improves over previous MDS attacks by allowing the attacker to choose which data to leak from the CPU's L1 cache, as well as which part of a cache line to leak. We demonstrate that CacheOut can leak information across multiple security boundaries, including those between processes, virtual machines, user and kernel space, and from SGX enclaves."}
{"original_text": "While large scale pre-trained language models such as BERT have achieved great success on various natural language understanding tasks, how to efficiently and effectively incorporate them into sequence-to-sequence models and the corresponding text generation tasks remains a non-trivial problem. In this paper, we propose to address this problem by taking two different BERT models as the encoder and decoder respectively, and fine-tuning them by introducing simple and lightweight adapter modules, which are inserted between BERT layers and tuned on the task-specific dataset. In this way, we obtain a flexible and efficient model which is able to jointly leverage the information contained in the source-side and target-side BERT models, while bypassing the catastrophic forgetting problem. Each component in the framework can be considered as a plug-in unit, making the framework flexible and task agnostic. Our framework is based on a parallel sequence decoding algorithm named Mask-Predict considering the bi-directional and conditional independent nature of BERT, and can be adapted to traditional autoregressive decoding easily. We conduct extensive experiments on neural machine translation tasks where the proposed method consistently outperforms autoregressive baselines while reducing the inference latency by half, and achieves 36.49 33.57 BLEU scores on IWSLT14 German-EnglishWMT14 German-English translation. When adapted to autoregressive decoding, the proposed method achieves 30.60 43.56 BLEU scores on WMT14 English-GermanEnglish-French translation, on par with the state-of-the-art baseline models.", "label": "1", "is_selected": "0", "text": "While large scale pre-trained language models such as BERT have achieved great success on various natural language understanding tasks, how to efficiently and effectively incorporate them into sequence-to-sequence models and the corresponding text generation tasks remains a non-trivial problem. In this paper, we propose to address this problem by taking two different BERT models as the encoder and decoder respectively, and fine-tuning them by introducing simple and lightweight adapter modules, which are inserted between BERT layers and tuned on the task-specific dataset. In this way, we obtain a flexible and efficient model which is able to jointly leverage the information contained in the source-side and target-side BERT models, while bypassing the catastrophic forgetting problem. Each component in the framework can be considered as a plug-in unit, making the framework flexible and task agnostic. Our framework is based on a parallel sequence decoding algorithm named Mask-Predict considering the bi-directional and conditional independent nature of BERT, and can be adapted to traditional autoregressive decoding easily. We conduct extensive experiments on neural machine translation tasks where the proposed method consistently outperforms autoregressive baselines while reducing the inference latency by half, and achieves 36.49 33.57 BLEU scores on IWSLT14 German-EnglishWMT14 German-English translation. When adapted to autoregressive decoding, the proposed method achieves 30.60 43.56 BLEU scores on WMT14 English-GermanEnglish-French translation, on par with the state-of-the-art baseline models."}
{"original_text": "Segregating an audio mixture containing multiple simultaneous bird sounds is a challenging task. However, birdsong often contains rapid pitch modulations, and these modulations carry information which may be of use in separating the bird sounds. One effective approach is to use time-frequency analysis techniques such as the short-time Fourier transform (STFT) or the constant-Q transform (CQT) to analyze the spectrogram of the audio mixture. Once this representation is obtained, specific features can be extracted and used to classify different birdcalls. One approach is to focus on the pitch modulation patterns of each birdcall by extracting pitch contours through pitch detection algorithms. A common algorithm for pitch detection is the Yin algorithm. After the pitch modulation patterns are identified, a classifier such as Support Vector Machine (SVM) can distinguish between different birdcalls. Another approach is to use Independent Component Analysis (ICA) which is a statistical technique that estimates the mixtures of different bird calls and separates them. The basic assumption of ICA is that the observed audio mixture is a linear combination of independent sources, which are the individual bird calls in this case. Overall, the best approach depends on the specific characteristics of the audio mixture and the available computational resources. A combination of different techniques may be necessary to successfully segregate the bird sounds.", "label": "0", "is_selected": "0", "text": "Segregating an audio mixture containing multiple simultaneous bird sounds is a challenging task. However, birdsong often contains rapid pitch modulations, and these modulations carry information which may be of use in separating the bird sounds. One effective approach is to use time-frequency analysis techniques such as the short-time Fourier transform (STFT) or the constant-Q transform (CQT) to analyze the spectrogram of the audio mixture. Once this representation is obtained, specific features can be extracted and used to classify different birdcalls. One approach is to focus on the pitch modulation patterns of each birdcall by extracting pitch contours through pitch detection algorithms. A common algorithm for pitch detection is the Yin algorithm. After the pitch modulation patterns are identified, a classifier such as Support Vector Machine (SVM) can distinguish between different birdcalls. Another approach is to use Independent Component Analysis (ICA) which is a statistical technique that estimates the mixtures of different bird calls and separates them. The basic assumption of ICA is that the observed audio mixture is a linear combination of independent sources, which are the individual bird calls in this case. Overall, the best approach depends on the specific characteristics of the audio mixture and the available computational resources. A combination of different techniques may be necessary to successfully segregate the bird sounds."}
{"original_text": "We address scene layout modeling for recognizing agent-in-place actions, which are actions associated with agents who perform them and the places where they occur, in the context of outdoor home surveillance. The goal is to accurately recognize and track such actions automatically using a computer vision system. To achieve this, we propose a scene layout modeling approach that involves: (1) identifying the relevant objects and their spatial relationships in the scene, (2) constructing a scene graph that encodes these objects and their relationships, and (3) using the scene graph to reason about the nature of the agent-in-place actions occurring in the scene. The approach involves first segmenting the scene into objects using state-of-the-art segmentation techniques, and then using geometric and semantic reasoning to identify relevant objects, such as people and vehicles, and their spatial relationships. These objects and their relationships are then encoded in a scene graph, which is a compact representation of the scene that allows for efficient reasoning about the actions occurring in the scene. To recognize agent-in-place actions, we use a combination of appearance-based and context-based features extracted from the scene graph. Appearance-based features are generated using deep learning techniques to capture the appearance of the objects performing the actions, while context-based features capture the spatial relationships between the objects and the places where the actions occur. These features are used to train a machine learning model, which is then used to recognize and track the actions occurring in the scene. Overall, our approach provides a robust and efficient method for recognizing and tracking agent-in-place actions in outdoor home surveillance scenarios, which has potential applications in home security, public safety, and traffic monitoring.", "label": "0", "is_selected": "1", "text": "In this paper, we propose a novel approach to automatically recognize and track agent-in-place actions in the context of outdoor home surveillance systems."}
{"original_text": "In this paper we present a framework for secure identification using deep neural networks, and apply it to the task of template protection for face authentication. We use deep convolutional neural networks to extract a high-dimensional representation of the face, which is then encrypted using a secure encryption scheme. During authentication, the encrypted template is compared to the unencrypted template extracted from the verification image. This approach offers better protection against attacks such as spoofing and reconstruction than traditional template protection methods. Additionally, the framework includes a mechanism to handle intra-class variations, or the differences between different images of the same person due to changes in expression or pose. We do this by using a Siamese network, which learns to distinguish between genuine and impostor pairs of templates. Overall, our framework provides a secure and accurate approach to face authentication, which is crucial in applications such as mobile payment or access control systems.", "label": "0", "is_selected": "1", "text": "Face recognition is a major challenge in the security research community, as it requires a high-level of expertise in order to identify a person in a variety of contexts."}
{"original_text": "Eigenvector continuation is a computational method that finds the extremal eigenvalues and eigenvectors of a Hamiltonian matrix with one or more control parameters. It does this by projection onto a subspace of eigenvectors corresponding to selected training values of the control parameters. The method has proven to be very efficient and accurate for interpolating and extrapolating eigenvectors. However, almost nothing is known about how the method converges, and its rapid convergence properties have remained mysterious. In this letter we present the first study of the convergence of eigenvector continuation. In order to perform the mathematical analysis, we introduce a new variant of eigenvector continuation that we call vector continuation. We first prove that eigenvector continuation and vector continuation have identical convergence properties and then analyze the convergence of vector continuation. Our analysis shows that, in general, eigenvector continuation converges more rapidly than perturbation theory. The faster convergence is achieved by eliminating a phenomenon that we call differential folding, the interference between non-orthogonal vectors appearing at different orders in perturbation theory. From our analysis we can predict how eigenvector continuation converges both inside and outside the radius of convergence of perturbation theory. While eigenvector continuation is a non-perturbative method, we show that its rate of convergence can be deduced from power series expansions of the eigenvectors. Our results also yield new insights into the nature of divergences in perturbation theory.", "label": "1", "is_selected": "0", "text": "Eigenvector continuation is a computational method that finds the extremal eigenvalues and eigenvectors of a Hamiltonian matrix with one or more control parameters. It does this by projection onto a subspace of eigenvectors corresponding to selected training values of the control parameters. The method has proven to be very efficient and accurate for interpolating and extrapolating eigenvectors. However, almost nothing is known about how the method converges, and its rapid convergence properties have remained mysterious. In this letter we present the first study of the convergence of eigenvector continuation. In order to perform the mathematical analysis, we introduce a new variant of eigenvector continuation that we call vector continuation. We first prove that eigenvector continuation and vector continuation have identical convergence properties and then analyze the convergence of vector continuation. Our analysis shows that, in general, eigenvector continuation converges more rapidly than perturbation theory. The faster convergence is achieved by eliminating a phenomenon that we call differential folding, the interference between non-orthogonal vectors appearing at different orders in perturbation theory. From our analysis we can predict how eigenvector continuation converges both inside and outside the radius of convergence of perturbation theory. While eigenvector continuation is a non-perturbative method, we show that its rate of convergence can be deduced from power series expansions of the eigenvectors. Our results also yield new insights into the nature of divergences in perturbation theory."}
{"original_text": "We develop a well-balanced central-upwind scheme for rotating shallow water model with horizontal temperature andor density gradients - the thermal rotating shallow water (TRSW). The scheme is designed using the flux-vector splitting (FVS) approach, which separates the numerical flux into two components: a wave-propagation flux and a hydrostatic flux. The wave-propagation flux handles the advection of fluid properties such as temperature, density, and momentum, while the hydrostatic flux represents the vertical variation of pressure due to gravity. To ensure stability and accuracy of the scheme, we derive a set of thermodynamic consistency conditions that satisfy the conservation laws of mass, momentum, and energy, as well as the second law of thermodynamics. These conditions enforce a balance between the wave-propagation and hydrostatic fluxes, and ensure that the numerical solution does not violate any physical laws. The scheme is implemented using a second-order Godunov method with piecewise parabolic reconstruction (PPM) in space and a third-order TVD Runge-Kutta method in time. We test the scheme on a number of benchmark problems, including the rotating shallow water equations with and without temperaturedensity gradients, the geostrophic adjustment problem, and the baroclinic instability problem. Our results show that the TRSW scheme is highly accurate and robust, with excellent conservation properties and good scalability to large-scale simulations. The scheme also effectively captures the dynamics of thermal and density-driven flows, making it an excellent tool for studying a wide range of geophysical phenomena.", "label": "0", "is_selected": "0", "text": "We develop a well-balanced central-upwind scheme for rotating shallow water model with horizontal temperature andor density gradients - the thermal rotating shallow water (TRSW). The scheme is designed using the flux-vector splitting (FVS) approach, which separates the numerical flux into two components: a wave-propagation flux and a hydrostatic flux. The wave-propagation flux handles the advection of fluid properties such as temperature, density, and momentum, while the hydrostatic flux represents the vertical variation of pressure due to gravity. To ensure stability and accuracy of the scheme, we derive a set of thermodynamic consistency conditions that satisfy the conservation laws of mass, momentum, and energy, as well as the second law of thermodynamics. These conditions enforce a balance between the wave-propagation and hydrostatic fluxes, and ensure that the numerical solution does not violate any physical laws. The scheme is implemented using a second-order Godunov method with piecewise parabolic reconstruction (PPM) in space and a third-order TVD Runge-Kutta method in time. We test the scheme on a number of benchmark problems, including the rotating shallow water equations with and without temperaturedensity gradients, the geostrophic adjustment problem, and the baroclinic instability problem. Our results show that the TRSW scheme is highly accurate and robust, with excellent conservation properties and good scalability to large-scale simulations. The scheme also effectively captures the dynamics of thermal and density-driven flows, making it an excellent tool for studying a wide range of geophysical phenomena."}
{"original_text": "Long Term Evolution (LTE) is expanding its utilization in unlicensed band by deploying LTE Unlicensed (LTE-U) and Licensed Assisted Access LTE (LTE-LAA) technology. Smart Grid can take the advantages of these technologies to improve its operations and functionality. LTE-U and LTE-LAA offer higher bandwidths and faster data speeds compared to traditional wireless communication technologies like Zigbee and Wi-Fi. This translates to faster transfer of data and better real-time monitoring capabilities for the smart grid. LTE-U and LTE-LAA technology also have better coverage and reliability compared to other wireless communication technologies, which is essential for smart grids that need to cover large areas and ensure the consistency of the network. This also helps in minimizing network delays and reducing the chances of data loss, making the smart grid more efficient and reliable. Moreover, smart grid systems can utilize LTE-U and LTE-LAA technology to enhance security by offering a more secure and robust communication channel for data transmission. This is particularly important for critical infrastructure like energy systems that could be vulnerable to cyber attacks. Overall, the utilization of LTE-U and LTE-LAA technology in the smart grid industry can bring significant benefits such as faster data transfer, more reliable communication, better security and enhanced efficiency.", "label": "0", "is_selected": "1", "text": "The smart grid industry is increasingly adopting next-generation wireless communication technologies to improve the efficiency and reliability of its operations. Overall, the utilization of LTE-U and LTE-LAA technology in the smart grid Industry can bring significant benefits such as faster data transfer, more reliable communication, better security and enhanced efficiency"}
{"original_text": "In this paper, we provide a philosophical account of the value of creative systems for individuals and society. We characterize creativity in very broad philosophical terms, encompassing natural, existential, and artistic modes of creativity. We argue that creativity is valuable for individuals because it enables them to engage with the world in a deeply meaningful and fulfilling way, and that it is valuable for society because it produces cultural artifacts that enrich our lives and promote progress. Creativity is valuable on a natural level because it allows us to adapt to changing circumstances and environments. Our ability to innovate and create new solutions to problems is rooted in our capacity for creativity. This enables us to survive and thrive in challenging situations and helps us to build more sustainable and resilient societies. Existential creativity allows individuals to make sense of their experiences and connect with others on a deeper level. Through art, music, and other forms of creative expression, individuals are able to communicate emotions and ideas that might be difficult to articulate through language alone. This helps to foster empathy and understanding, and promotes a sense of community and interconnectedness. Artistic creativity goes beyond mere survival and expression and produces cultural artifacts that are valuable in their own right. Creative works of art, literature, and music provide us with a wealth of insight into the human experience and can be deeply transformative. They have the power to inspire us, challenge us, and help us to see the world in new ways. In conclusion, creativity is valuable both on an individual and societal level. It promotes adaptation and innovation in the natural world, provides a means of expression and connection on an existential level, and produces cultural artifacts that enrich our lives and promote progress. By fostering creativity, we can enhance our quality of life and contribute to a more vibrant and inclusive society.", "label": "0", "is_selected": "0", "text": "In this paper, we provide a philosophical account of the value of creative systems for individuals and society. We characterize creativity in very broad philosophical terms, encompassing natural, existential, and artistic modes of creativity. We argue that creativity is valuable for individuals because it enables them to engage with the world in a deeply meaningful and fulfilling way, and that it is valuable for society because it produces cultural artifacts that enrich our lives and promote progress. Creativity is valuable on a natural level because it allows us to adapt to changing circumstances and environments. Our ability to innovate and create new solutions to problems is rooted in our capacity for creativity. This enables us to survive and thrive in challenging situations and helps us to build more sustainable and resilient societies. Existential creativity allows individuals to make sense of their experiences and connect with others on a deeper level. Through art, music, and other forms of creative expression, individuals are able to communicate emotions and ideas that might be difficult to articulate through language alone. This helps to foster empathy and understanding, and promotes a sense of community and interconnectedness. Artistic creativity goes beyond mere survival and expression and produces cultural artifacts that are valuable in their own right. Creative works of art, literature, and music provide us with a wealth of insight into the human experience and can be deeply transformative. They have the power to inspire us, challenge us, and help us to see the world in new ways. In conclusion, creativity is valuable both on an individual and societal level. It promotes adaptation and innovation in the natural world, provides a means of expression and connection on an existential level, and produces cultural artifacts that enrich our lives and promote progress. By fostering creativity, we can enhance our quality of life and contribute to a more vibrant and inclusive society."}
{"original_text": "The majority of works in distributed storage networks assume a simple network model with a collection of identical storage nodes with the same communication cost between the nodes. In this model, each node has the same amount of storage capacity, performs the same computation tasks, and has the same communication capabilities. This is commonly referred to as a homogeneous network model. However, in reality, many distributed storage networks have a heterogeneous network model, where storage nodes vary in their storage capacity, processing power, and communication capabilities. This can lead to performance issues, such as uneven data distribution, bottleneck nodes, and communication delays. To address these issues, researchers have proposed various techniques, such as load balancing algorithms, node classification schemes, and adaptive routing protocols, to optimize the performance of distributed storage networks in a heterogeneous environment. These techniques can help ensure that nodes with higher capabilities are utilized effectively and that the workload is distributed evenly across the network to prevent bottleneck nodes.", "label": "0", "is_selected": "1", "text": "Researchers at the University of California, Los Angeles (UCLA), have proposed new techniques to improve the performance of distributed storage networks in a heterogeneous environment."}
{"original_text": "Intra-class variations, distribution shifts among source and target domains are the major challenges of category-level tasks. In this study, we address category-level full 6D object pose estimation in the context of object manipulation tasks. To overcome these challenges, we propose a framework that combines a category-level object detection network with a pose estimation network using a data-driven approach. Our framework takes advantage of the intra-class variations among objects by learning a variation-aware representation of the object category. Additionally, we address the distribution shift between the source and target domains by deploying a domain adaptation module that leverages adversarial training to align the feature distributions of the source and target domains. Our experiments show that our proposed framework outperforms state-of-the-art methods in both synthetic and real-world scenarios.", "label": "0", "is_selected": "1", "text": "Category-level pose estimation is one of the most computationally intensive tasks in artificial intelligence (AI) and computer vision (CV). In this study, we address category-level full 6D object pose estimation in the context of object manipulation tasks."}
{"original_text": "We devise a distributional variant of gradient temporal-difference (TD) learning. Distributional reinforcement learning has been demonstrated to outperform the regular one in the recent study (,). In the policy evaluation setting, we design two new algorithms called distributional GTD2 and distributional TDC using the Cramer distance on the distributional version of the Bellman error objective function, which inherits advantages of both the nonlinear gradient TD algorithms and the distributional RL approach. In the control setting, we propose the distributional Greedy-GQ using the similar derivation. We prove the asymptotic almost-sure convergence of distributional GTD2 and TDC to a local optimal solution for general smooth function approximators, which includes neural networks that have been widely used in recent study to solve the real-life RL problems. In each step, the computational complexities of above three algorithms are linear w.r.t. the number of the parameters of the function approximator, thus can be implemented efficiently for neural networks.", "label": "1", "is_selected": "0", "text": "We devise a distributional variant of gradient temporal-difference (TD) learning. Distributional reinforcement learning has been demonstrated to outperform the regular one in the recent study (,). In the policy evaluation setting, we design two new algorithms called distributional GTD2 and distributional TDC using the Cramer distance on the distributional version of the Bellman error objective function, which inherits advantages of both the nonlinear gradient TD algorithms and the distributional RL approach. In the control setting, we propose the distributional Greedy-GQ using the similar derivation. We prove the asymptotic almost-sure convergence of distributional GTD2 and TDC to a local optimal solution for general smooth function approximators, which includes neural networks that have been widely used in recent study to solve the real-life RL problems. In each step, the computational complexities of above three algorithms are linear w.r.t. the number of the parameters of the function approximator, thus can be implemented efficiently for neural networks."}
{"original_text": "Roundabouts in conjunction with other traffic scenarios, e.g., intersections, merging roadways, speed reduction zones, can induce congestion in a transportation network due to driver responses to various disturbances. Research efforts have shown that smoothing traffic flow and eliminating stop-and-go driving can both improve fuel efficiency of the vehicles and the throughput of a roundabout. In this paper, we validate an optimal control framework developed earlier in a multi-lane roundabout scenario using the University of Delaware's scaled smart city (UDSSC). We first provide conditions where the solution is optimal. Then, we demonstrate the feasibility of the solution using experiments at UDSSC, and show that the optimal solution completely eliminates stop-and-go driving while preserving safety.", "label": "1", "is_selected": "0", "text": "Roundabouts in conjunction with other traffic scenarios, e.g., intersections, merging roadways, speed reduction zones, can induce congestion in a transportation network due to driver responses to various disturbances. Research efforts have shown that smoothing traffic flow and eliminating stop-and-go driving can both improve fuel efficiency of the vehicles and the throughput of a roundabout. In this paper, we validate an optimal control framework developed earlier in a multi-lane roundabout scenario using the University of Delaware's scaled smart city (UDSSC). We first provide conditions where the solution is optimal. Then, we demonstrate the feasibility of the solution using experiments at UDSSC, and show that the optimal solution completely eliminates stop-and-go driving while preserving safety."}
{"original_text": "Broad application of answer set programming (ASP) for declarative problem solving requires the development of tools supporting the coding process. Program debugging is one of the crucial activities within this process, which aims to identify and correct errors in the code. Fortunately, there are several tools available for debugging ASP programs. Some of these tools include: 1. Clingo Debugger: This is a graphical debugger for the Clingo ASP solver that allows users to step through the execution of their program and inspect the state of the solver and the program at any given point. 2. ASPIDE: This is an Integrated Development Environment (IDE) specifically designed for ASP programming. It includes a debugger that users can use to debug their ASP programs. 3. GASP: This is a command-line tool that provides a set of debugging utilities for ASP programs. It includes options for tracing the execution of a program, displaying the contents of the rule database, and inspecting the contents of answer sets. Overall, the availability of these tools can make debugging ASP programs easier and more efficient, which is essential for the successful application of ASP to real-world problem solving.", "label": "0", "is_selected": "0", "text": "Broad application of answer set programming (ASP) for declarative problem solving requires the development of tools supporting the coding process. Program debugging is one of the crucial activities within this process, which aims to identify and correct errors in the code. Fortunately, there are several tools available for debugging ASP programs. Some of these tools include: 1. Clingo Debugger: This is a graphical debugger for the Clingo ASP solver that allows users to step through the execution of their program and inspect the state of the solver and the program at any given point. 2. ASPIDE: This is an Integrated Development Environment (IDE) specifically designed for ASP programming. It includes a debugger that users can use to debug their ASP programs. 3. GASP: This is a command-line tool that provides a set of debugging utilities for ASP programs. It includes options for tracing the execution of a program, displaying the contents of the rule database, and inspecting the contents of answer sets. Overall, the availability of these tools can make debugging ASP programs easier and more efficient, which is essential for the successful application of ASP to real-world problem solving."}
{"original_text": "Approximations of loopy belief propagation, including expectation propagation and approximate message passing, have attracted considerable attention for probabilistic inference problems. This paper proposes and analyzes a generalization of Opper and Winther's Belief Propagation Fixed Point Algorithm (BP-FPA), called the Hyperparameter Belief Propagation Fixed Point Algorithm (HBP-FPA), for jointly optimizing the hyperparameters and the model parameters in probabilistic models. The proposed algorithm generalizes the standard BP-FPA by introducing hyperparameters that allow the algorithm to adapt to different problem settings and to balance different sources of uncertainty in the probabilistic model. The HBP-FPA algorithm is formulated as an iterative procedure that alternates between updating the model parameters and the hyperparameters. The update of the model parameters is done using a standard gradient descent algorithm, while the update of the hyperparameters is performed by solving a convex optimization problem. The algorithm is analyzed theoretically and shown to converge to the optimal hyperparameters and model parameters under certain assumptions on the probability model. The proposed algorithm is evaluated on several benchmark datasets, and the results show that it outperforms existing methods in terms of accuracy and convergence rate. The HBP-FPA algorithm is also shown to be applicable to complex probabilistic models, including models with structured priors and models with latent variables. In conclusion, the HBP-FPA algorithm is a promising approach for jointly optimizing hyperparameters and model parameters in probabilistic models. The proposed algorithm provides a general framework for balancing different sources of uncertainty in the probabilistic model and adapting to different problem settings, and has the potential to significantly improve the performance of probabilistic inference methods.", "label": "0", "is_selected": "1", "text": "A novel algorithm for approximated belief propagation has been proposed and tested in a paper published in Notices of the Royal Society of Chemistry, Number 10, Number 11 and Number 12."}
{"original_text": "Contrary to the situation with stochastic gradient descent, we argue that when using stochastic methods with variance reduction, such as SDCA, SAG or SVRG, as well as their variants, it is possible to obtain a better trade-off between the speed of convergence and the computational cost. This is because, instead of computing the gradient over the entire dataset at each iteration, these methods update the parameters using only a subset of the data, which is usually much smaller. However, these methods may require additional hyperparameters to be tuned, such as the step size and the size of the subset used. Additionally, they may not be as effective when dealing with non-convex problems or when the objective function has many local minima. Overall, the choice of optimization method will depend on the specific problem at hand and the trade-off between speed of convergence and computational cost that is acceptable.", "label": "0", "is_selected": "0", "text": "Contrary to the situation with stochastic gradient descent, we argue that when using stochastic methods with variance reduction, such as SDCA, SAG or SVRG, as well as their variants, it is possible to obtain a better trade-off between the speed of convergence and the computational cost. This is because, instead of computing the gradient over the entire dataset at each iteration, these methods update the parameters using only a subset of the data, which is usually much smaller. However, these methods may require additional hyperparameters to be tuned, such as the step size and the size of the subset used. Additionally, they may not be as effective when dealing with non-convex problems or when the objective function has many local minima. Overall, the choice of optimization method will depend on the specific problem at hand and the trade-off between speed of convergence and computational cost that is acceptable."}
{"original_text": "The analysis of biological sequencing data has been one of the biggest applications of string algorithms. The approaches used in many such applications are based on the analysis of k -mers, which are short fixed-length strings present in a dataset. While these approaches are rather diverse, storing and querying k -mer sets has emerged as a shared underlying component. Sets of k -mers have unique features and applications that, over the last ten years, have resulted in many specialized approaches for their representation. In this survey, we give a unified presentation and comparison of the data structures that have been proposed to store and query k -mer sets. We hope this survey will not only serve as a resource for researchers in the field but also make the area more accessible to outsiders.", "label": "1", "is_selected": "0", "text": "The analysis of biological sequencing data has been one of the biggest applications of string algorithms. The approaches used in many such applications are based on the analysis of k -mers, which are short fixed-length strings present in a dataset. While these approaches are rather diverse, storing and querying k -mer sets has emerged as a shared underlying component. Sets of k -mers have unique features and applications that, over the last ten years, have resulted in many specialized approaches for their representation. In this survey, we give a unified presentation and comparison of the data structures that have been proposed to store and query k -mer sets. We hope this survey will not only serve as a resource for researchers in the field but also make the area more accessible to outsiders."}
{"original_text": "Centrality measures such as the degree, k-shell, or eigenvalue centrality can identify a network's most influential nodes, but are rarely usefully accurate in quantifying the spreading power of the vast majority of nodes which are not highly influential. The spreading power of all network nodes is better explained by considering, from a continuous-time epidemiological perspective, the distribution of the force of infection each node generates. The resulting metric, the Expected Force (ExF), accurately quantifies node spreading power under all primary epidemiological models across a wide range of archetypical human contact networks. When node power is low, influence is a function of neighbor degree. As power increases, a node's own degree becomes more important. The strength of this relationship is modulated by network structure, being more pronounced in narrow, dense networks typical of social networking and weakening in broader, looser association networks such as Internet webpages. The ExF can be computed independently for individual nodes, making it applicable for networks whose adjacency matrix is dynamic, not well specified, or overwhelmingly large.", "label": "1", "is_selected": "0", "text": "Centrality measures such as the degree, k-shell, or eigenvalue centrality can identify a network's most influential nodes, but are rarely usefully accurate in quantifying the spreading power of the vast majority of nodes which are not highly influential. The spreading power of all network nodes is better explained by considering, from a continuous-time epidemiological perspective, the distribution of the force of infection each node generates. The resulting metric, the Expected Force (ExF), accurately quantifies node spreading power under all primary epidemiological models across a wide range of archetypical human contact networks. When node power is low, influence is a function of neighbor degree. As power increases, a node's own degree becomes more important. The strength of this relationship is modulated by network structure, being more pronounced in narrow, dense networks typical of social networking and weakening in broader, looser association networks such as Internet webpages. The ExF can be computed independently for individual nodes, making it applicable for networks whose adjacency matrix is dynamic, not well specified, or overwhelmingly large."}
{"original_text": "This case for the Transformation Tool Contest 2013 is about evaluating the scope and usability of transformation languages and tools for a set of four tasks requiring very different capabilities. Okay, can you provide more details about the four tasks and the requirements for each task? It will help in understanding the scope and usability of transformation languages and tools required for each task.", "label": "0", "is_selected": "0", "text": "This case for the Transformation Tool Contest 2013 is about evaluating the scope and usability of transformation languages and tools for a set of four tasks requiring very different capabilities. Okay, can you provide more details about the four tasks and the requirements for each task? It will help in understanding the scope and usability of transformation languages and tools required for each task."}
{"original_text": "The security of cryptographic communication protocols that use X.509 certificates depends on the correctness of those certificates. This paper proposes a system that helps to ensure the correct operation of an X.509 certification authority and its registration authorities. We achieve this goal by enforcing a policy-defined, multi-party validation and authorization workflow of certificate signing requests. Besides, our system offers full accountability for this workflow for forensic purposes. As a foundation for our implementation, we leverage the distributed ledger and smart contract framework Hyperledger Fabric. Our implementation inherits the strong tamper-resistance of Fabric which strengthens the integrity of the computer processes that enforce the validation and authorization of the certificate signing request, and of the metadata collected during certificate issuance.", "label": "1", "is_selected": "0", "text": "The security of cryptographic communication protocols that use X.509 certificates depends on the correctness of those certificates. This paper proposes a system that helps to ensure the correct operation of an X.509 certification authority and its registration authorities. We achieve this goal by enforcing a policy-defined, multi-party validation and authorization workflow of certificate signing requests. Besides, our system offers full accountability for this workflow for forensic purposes. As a foundation for our implementation, we leverage the distributed ledger and smart contract framework Hyperledger Fabric. Our implementation inherits the strong tamper-resistance of Fabric which strengthens the integrity of the computer processes that enforce the validation and authorization of the certificate signing request, and of the metadata collected during certificate issuance."}
{"original_text": "Emergent narratives provide a unique and compelling approach to interactive storytelling through simulation, and have applications in games, narrative generation, and virtual agents. However the inherent complexity of simulation makes it challenging to create emergent narratives that are both coherent and engaging for the playeruser. One approach to addressing this challenge is to use artificial intelligence and machine learning techniques to model and understand how emergent narratives can be created and manipulated. By analyzing large amounts of data and simulating a variety of scenarios, these techniques can identify patterns and generate new narrative possibilities that are both credible and immersive. Another approach involves designing the simulation environment to provide playersusers with clear objectives and consistent rules, while still allowing for emergent narrative elements to arise organically. This can involve creating complex simulations that replicate real-world environments and events, and using game mechanics and other interactive elements to encourage player engagement and exploration. Ultimately, creating compelling emergent narratives requires a combination of innovative design, advanced technology, and a deep understanding of the human experience. As such, it remains an exciting and rapidly evolving field with numerous opportunities for creative exploration and innovation.", "label": "0", "is_selected": "0", "text": "Emergent narratives provide a unique and compelling approach to interactive storytelling through simulation, and have applications in games, narrative generation, and virtual agents. However the inherent complexity of simulation makes it challenging to create emergent narratives that are both coherent and engaging for the playeruser. One approach to addressing this challenge is to use artificial intelligence and machine learning techniques to model and understand how emergent narratives can be created and manipulated. By analyzing large amounts of data and simulating a variety of scenarios, these techniques can identify patterns and generate new narrative possibilities that are both credible and immersive. Another approach involves designing the simulation environment to provide playersusers with clear objectives and consistent rules, while still allowing for emergent narrative elements to arise organically. This can involve creating complex simulations that replicate real-world environments and events, and using game mechanics and other interactive elements to encourage player engagement and exploration. Ultimately, creating compelling emergent narratives requires a combination of innovative design, advanced technology, and a deep understanding of the human experience. As such, it remains an exciting and rapidly evolving field with numerous opportunities for creative exploration and innovation."}
{"original_text": "3D point cloud semantic and instance segmentation is crucial and fundamental for 3D scene understanding. Due to the complex structure, point sets are distributed off balance and diversely, which appears as both category imbalance and pattern imbalance. As a result, deep networks can easily forget the non-dominant cases during the learning process, resulting in unsatisfactory performance. Although re-weighting can reduce the influence of the well-classified examples, they cannot handle the non-dominant patterns during the dynamic training. In this paper, we propose a memory-augmented network to learn and memorize the representative prototypes that cover diverse samples universally. Specifically, a memory module is introduced to alleviate the forgetting issue by recording the patterns seen in mini-batch training. The learned memory items consistently reflect the interpretable and meaningful information for both dominant and non-dominant categories and cases. The distorted observations and rare cases can thus be augmented by retrieving the stored prototypes, leading to better performances and generalization. Exhaustive experiments on the benchmarks, i.e. S3DIS and ScanNetV2, reflect the superiority of our method on both effectiveness and efficiency. Not only the overall accuracy but also non-dominant classes have improved substantially.", "label": "1", "is_selected": "0", "text": "3D point cloud semantic and instance segmentation is crucial and fundamental for 3D scene understanding. Due to the complex structure, point sets are distributed off balance and diversely, which appears as both category imbalance and pattern imbalance. As a result, deep networks can easily forget the non-dominant cases during the learning process, resulting in unsatisfactory performance. Although re-weighting can reduce the influence of the well-classified examples, they cannot handle the non-dominant patterns during the dynamic training. In this paper, we propose a memory-augmented network to learn and memorize the representative prototypes that cover diverse samples universally. Specifically, a memory module is introduced to alleviate the forgetting issue by recording the patterns seen in mini-batch training. The learned memory items consistently reflect the interpretable and meaningful information for both dominant and non-dominant categories and cases. The distorted observations and rare cases can thus be augmented by retrieving the stored prototypes, leading to better performances and generalization. Exhaustive experiments on the benchmarks, i.e. S3DIS and ScanNetV2, reflect the superiority of our method on both effectiveness and efficiency. Not only the overall accuracy but also non-dominant classes have improved substantially."}
{"original_text": "This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of Residual Networks on the challenging ImageNet classification and COCO object detection datasets. Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions.", "label": "1", "is_selected": "0", "text": "This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of Residual Networks on the challenging ImageNet classification and COCO object detection datasets. Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions."}
{"original_text": "This paper proposes an adaptive human pilot model that is able to mimic the crossover model in the presence of uncertainties. The proposed structure is based on the model reference adaptive control, and the adaptive laws are obtained using the Lyapunov-Krasovskii stability criteria. The model can be employed for human-in-the-loop stability and performance analyses incorporating different types of controllers and plant types. For validation purposes, an experimental setup is employed to collect data and a statistical analysis is conducted to measure the predictive power of the pilot model.", "label": "1", "is_selected": "0", "text": "This paper proposes an adaptive human pilot model that is able to mimic the crossover model in the presence of uncertainties. The proposed structure is based on the model reference adaptive control, and the adaptive laws are obtained using the Lyapunov-Krasovskii stability criteria. The model can be employed for human-in-the-loop stability and performance analyses incorporating different types of controllers and plant types. For validation purposes, an experimental setup is employed to collect data and a statistical analysis is conducted to measure the predictive power of the pilot model."}
{"original_text": "Context: In C, low-level errors, such as buffer overflow and use-after-free, are a major problem, as they cause security vulnerabilities and hard-to-find bugs. C lacks automatic checks, and programmers cannot rely on the language to prevent such errors. Instead, they need to be diligent in their coding practices and manually check for potential issues. As a helpful assistant, I would recommend that programmers use tools and techniques that can help detect and prevent these types of errors. For example, using memory-safe languages, such as Rust or Go, can help eliminate buffer overflow issues. Additionally, using tools like valgrind or AddressSanitizer can help detect memory errors at runtime. It's also important to follow best practices when it comes to memory management, like ensuring that memory is properly allocated and freed, and avoiding the use of unsafe functions. By taking these steps, programmers can reduce the likelihood of low-level errors and improve the security and stability of their code.", "label": "0", "is_selected": "1", "text": "Low-level errors, such as buffer overflow and use-after-free, are a major problem in C, as they cause security vulnerabilities and hard-to-find bugs."}
{"original_text": "The main research involving globalization nowadays is to describe the impact of globalization in their respective fields. However, globalization is a complex phenomenon across multiple sections. But as a concept , globalization refers to the integration of societies, economies, and cultures worldwide. It has been driven by advancements in transportation, communication, and technology, which have made it easier for people, goods, and ideas to move across borders and to connect with each other. The impact of globalization has been diverse and multifaceted across different fields. In economics, globalization has led to increased international trade, capital flows, and outsourcing of production to lower-cost countries. This has helped to spur economic growth, but also led to job losses and increased inequality in some countries. In culture, globalization has led to the spread of ideas, music, movies, and fashion across the world. This has led to the fusion of different cultural practices in various parts of the world. Additionally, it has also led to the promotion of English as the global language, which has had far-reaching implications. In politics, globalization has led to increased cooperation among countries and the emergence of new international institutions such as the United Nations, the World Trade Organization, and the International Monetary Fund. However, it has also led to the dominance of the world's most powerful countries and their interests, which have been further advanced through globalization. Overall, the impact of globalization has been both positive and negative, and it is a complex phenomenon that requires multidisciplinary research to understand its broader implications.", "label": "0", "is_selected": "0", "text": "The main research involving globalization nowadays is to describe the impact of globalization in their respective fields. However, globalization is a complex phenomenon across multiple sections. But as a concept , globalization refers to the integration of societies, economies, and cultures worldwide. It has been driven by advancements in transportation, communication, and technology, which have made it easier for people, goods, and ideas to move across borders and to connect with each other. The impact of globalization has been diverse and multifaceted across different fields. In economics, globalization has led to increased international trade, capital flows, and outsourcing of production to lower-cost countries. This has helped to spur economic growth, but also led to job losses and increased inequality in some countries. In culture, globalization has led to the spread of ideas, music, movies, and fashion across the world. This has led to the fusion of different cultural practices in various parts of the world. Additionally, it has also led to the promotion of English as the global language, which has had far-reaching implications. In politics, globalization has led to increased cooperation among countries and the emergence of new international institutions such as the United Nations, the World Trade Organization, and the International Monetary Fund. However, it has also led to the dominance of the world's most powerful countries and their interests, which have been further advanced through globalization. Overall, the impact of globalization has been both positive and negative, and it is a complex phenomenon that requires multidisciplinary research to understand its broader implications."}
{"original_text": "This paper presents eight PAC-Bayes bounds to analyze the generalization performance of multi-view classifiers. These bounds adopt data dependent Gaussian priors which emphasize classifiers with high view agreements. The center of these priors is determined based on the empirical mean of the views and the covariance is adjusted according to the amount of disagreement among views. The PAC-Bayesian framework enables us to derive bounds on the expected generalization error of multi-view classifiers by leveraging the information provided by the data and the priors. These bounds have been evaluated on various datasets and compared against state-of-the-art alternatives, showing their effectiveness in providing tighter generalization error estimates and improving the overall accuracy of multi-view classifiers. Overall, this paper provides a valuable contribution to the field of multi-view learning and can facilitate the development of more robust and accurate multi-view classification models.", "label": "0", "is_selected": "0", "text": "This paper presents eight PAC-Bayes bounds to analyze the generalization performance of multi-view classifiers. These bounds adopt data dependent Gaussian priors which emphasize classifiers with high view agreements. The center of these priors is determined based on the empirical mean of the views and the covariance is adjusted according to the amount of disagreement among views. The PAC-Bayesian framework enables us to derive bounds on the expected generalization error of multi-view classifiers by leveraging the information provided by the data and the priors. These bounds have been evaluated on various datasets and compared against state-of-the-art alternatives, showing their effectiveness in providing tighter generalization error estimates and improving the overall accuracy of multi-view classifiers. Overall, this paper provides a valuable contribution to the field of multi-view learning and can facilitate the development of more robust and accurate multi-view classification models."}
{"original_text": "Hex is a complex game with a high branching factor. For the first time Hex is being attempted to be solved without the use of game tree structures and associated algorithms. Instead, researchers are exploring the use of neural network models, specifically Monte Carlo tree search algorithms. These approaches allow for a more efficient exploration of the vast state space of the game, allowing for faster and more accurate decision-making. As a helpful assistant, is there anything specific you would like to know about Hex and its current research and development?", "label": "0", "is_selected": "0", "text": "Hex is a complex game with a high branching factor. For the first time Hex is being attempted to be solved without the use of game tree structures and associated algorithms. Instead, researchers are exploring the use of neural network models, specifically Monte Carlo tree search algorithms. These approaches allow for a more efficient exploration of the vast state space of the game, allowing for faster and more accurate decision-making. As a helpful assistant, is there anything specific you would like to know about Hex and its current research and development?"}
{"original_text": "F-index of a graph is the sum of the cube of the degrees of the vertices. In this paper, we investigate the F-indices of unicyclic graphs by introducing some transformation, and characterize the unicyclic graphs with the first five largest F-indices and the unicyclic graphs with the first two smallest F-indices, respectively.", "label": "1", "is_selected": "0", "text": "F-index of a graph is the sum of the cube of the degrees of the vertices. In this paper, we investigate the F-indices of unicyclic graphs by introducing some transformation, and characterize the unicyclic graphs with the first five largest F-indices and the unicyclic graphs with the first two smallest F-indices, respectively."}
{"original_text": "Various statistical analysis methods are studied for years to extract accurate trends of network traffic and predict the future load mainly to allocate required resources. Besides, many stochastic modeling techniques are offered to represent fundamental characteristics of different types of network traffic. In this study, we analyze autoregressive traffic forecasting techniques considering their popularity and wide-use in the domain. In comparison to similar works, we present important traffic characteristics and discussions from the literature to create a self-consistent guidance along with the survey. Then, we approach to techniques in the literature revealing which network characteristics they can capture offering a characteristic-based framework. Most importantly, we aim to fill the gap between the statistical analysis of those methods and their relevance with networking by dicussing significant aspects and requirements for accurate forecasting from a network-telemetric perspective.", "label": "1", "is_selected": "0", "text": "Various statistical analysis methods are studied for years to extract accurate trends of network traffic and predict the future load mainly to allocate required resources. Besides, many stochastic modeling techniques are offered to represent fundamental characteristics of different types of network traffic. In this study, we analyze autoregressive traffic forecasting techniques considering their popularity and wide-use in the domain. In comparison to similar works, we present important traffic characteristics and discussions from the literature to create a self-consistent guidance along with the survey. Then, we approach to techniques in the literature revealing which network characteristics they can capture offering a characteristic-based framework. Most importantly, we aim to fill the gap between the statistical analysis of those methods and their relevance with networking by dicussing significant aspects and requirements for accurate forecasting from a network-telemetric perspective."}
{"original_text": "Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply stores transitions as they occur and then randomly samples them during the replay process, which can result in inefficient learning because important transitions may be sampled less frequently. To overcome this, prioritized experience replay (PER) assigns a priority value to each transition that reflects its importance or potential learning value. Transitions with higher priority values are sampled more frequently during the replay process, increasing the efficiency of learning. PER has been shown to significantly improve the performance of reinforcement learning agents in a variety of tasks.", "label": "0", "is_selected": "1", "text": "This paper presents a new approach to experience replay that increases the learning efficiency of reinforcement learning agents by increasing the number of transitions sampled during the replay process."}
{"original_text": "The rapidly growing field of network analytics requires data sets for use in evaluation. Real world data often lack truth and simulated data lack narrative fidelity or statistical generality. This paper presents a novel, mixed-membership, agent-based simulation model to generate activity data with narrative power while providing statistical diversity through random draws. The model generalizes to a variety of network activity types such as Internet and cellular communications, human mobility, and social network interactions. The simulated actions over all agents can then drive an application specific observational model to render measurements as one would collect in real-world experiments. We apply this framework to human mobility and demonstrate its utility in generating high fidelity traffic data for network analytics. 1 footnote 1 1 footnote 1 This work is sponsored by the Assistant Secretary of Defense for Research Engineering under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the author and are not necessarily endorsed by the United States Government", "label": "1", "is_selected": "0", "text": "The rapidly growing field of network analytics requires data sets for use in evaluation. Real world data often lack truth and simulated data lack narrative fidelity or statistical generality. This paper presents a novel, mixed-membership, agent-based simulation model to generate activity data with narrative power while providing statistical diversity through random draws. The model generalizes to a variety of network activity types such as Internet and cellular communications, human mobility, and social network interactions. The simulated actions over all agents can then drive an application specific observational model to render measurements as one would collect in real-world experiments. We apply this framework to human mobility and demonstrate its utility in generating high fidelity traffic data for network analytics. 1 footnote 1 1 footnote 1 This work is sponsored by the Assistant Secretary of Defense for Research Engineering under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the author and are not necessarily endorsed by the United States Government"}
{"original_text": "GANs can generate photo-realistic images from the domain of their training data. However, those wanting to use them for creative purposes often want to generate imagery from a truly novel domain, a task which GANs are inherently unable to do. It is also desirable to have a level of control so that there is a degree of artistic direction rather than purely curation of random results. Here we present a method for interpolating between generative models of the StyleGAN architecture in a resolution dependent manner. This allows us to generate images from an entirely novel domain and do this with a degree of control over the nature of the output.", "label": "1", "is_selected": "0", "text": "GANs can generate photo-realistic images from the domain of their training data. However, those wanting to use them for creative purposes often want to generate imagery from a truly novel domain, a task which GANs are inherently unable to do. It is also desirable to have a level of control so that there is a degree of artistic direction rather than purely curation of random results. Here we present a method for interpolating between generative models of the StyleGAN architecture in a resolution dependent manner. This allows us to generate images from an entirely novel domain and do this with a degree of control over the nature of the output."}
{"original_text": "The aim of this study is to find key areas of research that can be useful to fight against disinformation on Wikipedia. To address this problem we perform a literature review to identify relevant studies done in the past. We systematically searched various academic databases and analyzed several studies related to disinformation on online platforms, including Wikipedia. Our findings suggest that there is a need for more research on various aspects of disinformation on Wikipedia, such as the motives of disinformation spreaders, the impact of disinformation on Wikipedia's credibility, and the effectiveness of different methods to counter disinformation. Some specific areas that need further investigation include the role of social media in spreading disinformation on Wikipedia, the use of machine learning algorithms to detect and combat disinformation, and the effectiveness of community-based moderation in preventing and removing disinformation. Based on our literature review, we suggest that future research in this area should focus on developing more sophisticated methods of detecting and countering disinformation, as well as exploring the social, political, and economic factors that drive the phenomena. With the growing importance of online information sources, including Wikipedia, it is crucial to develop effective strategies to combat disinformation and ensure the accuracy and reliability of the information provided to the public.", "label": "0", "is_selected": "1", "text": "This study aims to investigate the role of disinformation on Wikipedia, as well as to examine the effectiveness of different methods to combat disinformation on the online information platform."}
{"original_text": "We prove that for every proper minor-closed class M of F p -representable matroids, there exists a O (1) -competitive algorithm for the matroid secretary problem on M. This result relies on the extremely powerful matroid minor structure theory being developed by Geelen, Gerards and Whittle. We also note that for asymptotically almost all matroids, the matroid secretary algorithm that selects a random basis, ignoring weights, is (2 o (1 -competitive. In fact, assuming the conjecture that almost all matroids are paving, there is a (1 o (1 -competitive algorithm for almost all matroids.", "label": "1", "is_selected": "0", "text": "We prove that for every proper minor-closed class M of F p -representable matroids, there exists a O (1) -competitive algorithm for the matroid secretary problem on M. This result relies on the extremely powerful matroid minor structure theory being developed by Geelen, Gerards and Whittle. We also note that for asymptotically almost all matroids, the matroid secretary algorithm that selects a random basis, ignoring weights, is (2 o (1 -competitive. In fact, assuming the conjecture that almost all matroids are paving, there is a (1 o (1 -competitive algorithm for almost all matroids."}
{"original_text": "Most studies on optical wireless communications (OWCs) have neglected the effect of random orientation in their performance analysis due to the lack of a proper model for the random orientation. Our recent empirical-based research illustrates that the random orientation follows a Laplace distribution for static user equipment (UE). In this paper, we analyze the device orientation and assess its importance on system performance. The probability of establishing a line-of-sight link is investigated and the probability density function (PDF) of signal-to-noise ratio (SNR) for a randomly-oriented device is derived. By means of the PDF of SNR, the bit-error ratio (BER) of DC biased optical orthogonal frequency division multiplexing (DCO-OFDM) in additive white Gaussian noise (AWGN) channels is evaluated. A closed form approximation for the BER of UE with random orientation is presented which shows a good match with Monte-Carlo simulation results.", "label": "1", "is_selected": "0", "text": "Most studies on optical wireless communications (OWCs) have neglected the effect of random orientation in their performance analysis due to the lack of a proper model for the random orientation. Our recent empirical-based research illustrates that the random orientation follows a Laplace distribution for static user equipment (UE). In this paper, we analyze the device orientation and assess its importance on system performance. The probability of establishing a line-of-sight link is investigated and the probability density function (PDF) of signal-to-noise ratio (SNR) for a randomly-oriented device is derived. By means of the PDF of SNR, the bit-error ratio (BER) of DC biased optical orthogonal frequency division multiplexing (DCO-OFDM) in additive white Gaussian noise (AWGN) channels is evaluated. A closed form approximation for the BER of UE with random orientation is presented which shows a good match with Monte-Carlo simulation results."}
{"original_text": "Flexibility at hardware level is the main driving force behind adaptive systems whose aim is to realise microarhitecture deconfiguration 'online'. This feature allows the softwarehardware stack to tolerate drastic changes of the workload in data centres. With emerge of FPGA reconfigurablity this technology is becoming a mainstream computing paradigm. Adaptivity is usually accompanied by the high-level tools to facilitate multi-dimensional space exploration. An essential aspect in this space is memory orchestration where on-chip and off-chip memory distribution significantly influences the architecture in coping with the critical spatial and timing constraints, e.g. Place Route. This paper proposes a memory smart technique for a particular class of adaptive systems: Elastic Circuits which enjoy slack elasticity at fine level of granularity. We explore retiming of a set of popular benchmarks via investigating the memory distribution within and among accelerators. The area, performance and power patterns are adopted by our high-level synthesis framework, with respect to the behaviour of the input descriptions, to improve the quality of the synthesised elastic circuits.", "label": "1", "is_selected": "0", "text": "Flexibility at hardware level is the main driving force behind adaptive systems whose aim is to realise microarhitecture deconfiguration 'online'. This feature allows the softwarehardware stack to tolerate drastic changes of the workload in data centres. With emerge of FPGA reconfigurablity this technology is becoming a mainstream computing paradigm. Adaptivity is usually accompanied by the high-level tools to facilitate multi-dimensional space exploration. An essential aspect in this space is memory orchestration where on-chip and off-chip memory distribution significantly influences the architecture in coping with the critical spatial and timing constraints, e.g. Place Route. This paper proposes a memory smart technique for a particular class of adaptive systems: Elastic Circuits which enjoy slack elasticity at fine level of granularity. We explore retiming of a set of popular benchmarks via investigating the memory distribution within and among accelerators. The area, performance and power patterns are adopted by our high-level synthesis framework, with respect to the behaviour of the input descriptions, to improve the quality of the synthesised elastic circuits."}
{"original_text": "In this paper, a novel experienced deep reinforcement learning (deep-RL) framework is proposed to provide model-free resource allocation for in the downlink of a wireless network. The goal is to guarantee high end-to-end reliability and low end-to-end latency, under explicit data rate constraints, for each wireless user without any models of or assumptions on the users' traffic. In particular, in order to enable the deep-RL framework to account for extreme network conditions and operate in highly reliable systems, a new approach based on generative adversarial networks (GANs) is proposed. This GAN approach is used to pre-train the deep-RL framework using a mix of real and synthetic data, thus creating an experienced deep-RL framework that has been exposed to a broad range of network conditions. The proposed deep-RL framework is particularly applied to a multi-user orthogonal frequency division multiple access (OFDMA) resource allocation system. Formally, this resource allocation problem in OFDMA systems is posed as a power minimization problem under reliability, latency, and rate constraints. To solve this problem using experienced deep-RL, first, the rate of each user is determined. Then, these rates are mapped to the resource block and power allocation vectors of the studied wireless system. Finally, the end-to-end reliability and latency of each user are used as feedback to the deep-RL framework. It is then shown that at the fixed-point of the deep-RL algorithm, the reliability and latency of the users are near-optimal. Moreover, for the proposed GAN approach, a theoretical limit for the generator output is analytically derived. Simulation results show how the proposed approach can achieve near-optimal performance within the rate-reliability-latency region, depending on the network and service requirements. The results also show that the proposed experienced deep-RL framework is able to remove the transient training time that makes conventional deep-RL methods unsuitable for. Moreover, during extreme conditions, it is shown that the proposed, experienced deep-RL agent can recover instantly while a conventional deep-RL agent takes several epochs to adapt to new extreme conditions.", "label": "1", "is_selected": "0", "text": "In this paper, a novel experienced deep reinforcement learning (deep-RL) framework is proposed to provide model-free resource allocation for in the downlink of a wireless network. The goal is to guarantee high end-to-end reliability and low end-to-end latency, under explicit data rate constraints, for each wireless user without any models of or assumptions on the users' traffic. In particular, in order to enable the deep-RL framework to account for extreme network conditions and operate in highly reliable systems, a new approach based on generative adversarial networks (GANs) is proposed. This GAN approach is used to pre-train the deep-RL framework using a mix of real and synthetic data, thus creating an experienced deep-RL framework that has been exposed to a broad range of network conditions. The proposed deep-RL framework is particularly applied to a multi-user orthogonal frequency division multiple access (OFDMA) resource allocation system. Formally, this resource allocation problem in OFDMA systems is posed as a power minimization problem under reliability, latency, and rate constraints. To solve this problem using experienced deep-RL, first, the rate of each user is determined. Then, these rates are mapped to the resource block and power allocation vectors of the studied wireless system. Finally, the end-to-end reliability and latency of each user are used as feedback to the deep-RL framework. It is then shown that at the fixed-point of the deep-RL algorithm, the reliability and latency of the users are near-optimal. Moreover, for the proposed GAN approach, a theoretical limit for the generator output is analytically derived. Simulation results show how the proposed approach can achieve near-optimal performance within the rate-reliability-latency region, depending on the network and service requirements. The results also show that the proposed experienced deep-RL framework is able to remove the transient training time that makes conventional deep-RL methods unsuitable for. Moreover, during extreme conditions, it is shown that the proposed, experienced deep-RL agent can recover instantly while a conventional deep-RL agent takes several epochs to adapt to new extreme conditions."}
{"original_text": "Many computer vision and medical imaging problems are faced with learning from large-scale datasets, with millions of observations and features. In this paper we propose a novel efficient learning scheme that can handle such large datasets. Our approach is based on the use of deep learning techniques, specifically a deep neural network architecture called a convolutional neural network (CNN), and a training strategy called stochastic gradient descent (SGD) with mini-batch updates. The CNN framework is particularly suited to handling high-dimensional data, such as images, and has demonstrated state-of-the-art performance in many computer vision tasks. The key idea behind CNNs is to leverage the local and spatial correlation of image pixels by using convolutional filters that extract meaningful features across the image. SGD is an optimization algorithm commonly used for training deep neural networks. It works by iteratively adjusting the weights of the network in the direction of the negative gradient of a loss function, with the aim of minimizing the loss. As the name suggests, SGD updates the weights in a stochastic manner by sampling a subset of the data (called a mini-batch) for each iteration, which reduces the computational cost and enables efficient processing of large-scale datasets. Our proposed learning scheme combines the power of CNNs and SGD with mini-batch updates to efficiently learn from large-scale datasets in a scalable and effective manner. We demonstrate the effectiveness of our approach by evaluating it on two challenging datasets: the ImageNet dataset for object recognition, and the LIDC-IDRI dataset for lung nodule detection in CT images. Our results show that our proposed approach achieves state-of-the-art performance while maintaining computational efficiency. Overall, our proposed efficient learning scheme can be a valuable tool for researchers and practitioners working on large-scale computer vision and medical imaging problems.", "label": "0", "is_selected": "0", "text": "Many computer vision and medical imaging problems are faced with learning from large-scale datasets, with millions of observations and features. In this paper we propose a novel efficient learning scheme that can handle such large datasets. Our approach is based on the use of deep learning techniques, specifically a deep neural network architecture called a convolutional neural network (CNN), and a training strategy called stochastic gradient descent (SGD) with mini-batch updates. The CNN framework is particularly suited to handling high-dimensional data, such as images, and has demonstrated state-of-the-art performance in many computer vision tasks. The key idea behind CNNs is to leverage the local and spatial correlation of image pixels by using convolutional filters that extract meaningful features across the image. SGD is an optimization algorithm commonly used for training deep neural networks. It works by iteratively adjusting the weights of the network in the direction of the negative gradient of a loss function, with the aim of minimizing the loss. As the name suggests, SGD updates the weights in a stochastic manner by sampling a subset of the data (called a mini-batch) for each iteration, which reduces the computational cost and enables efficient processing of large-scale datasets. Our proposed learning scheme combines the power of CNNs and SGD with mini-batch updates to efficiently learn from large-scale datasets in a scalable and effective manner. We demonstrate the effectiveness of our approach by evaluating it on two challenging datasets: the ImageNet dataset for object recognition, and the LIDC-IDRI dataset for lung nodule detection in CT images. Our results show that our proposed approach achieves state-of-the-art performance while maintaining computational efficiency. Overall, our proposed efficient learning scheme can be a valuable tool for researchers and practitioners working on large-scale computer vision and medical imaging problems."}
{"original_text": "This paper presents an adaptive randomized algorithm for computing the butterfly factorization of a x m n matrix with m n provided that both the matrix and its transpose can be rapidly applied to arbitrary vectors. The resulting factorization is composed of O (log n) sparse factors, each containing O (n) nonzero entries. The factorization can be attained using O (n 3 2 log n) computation and O (n log n) memory resources. The proposed algorithm applies to matrices with strong and weak admissibility conditions arising from surface integral equation solvers with a rigorous error bound, and is implemented in parallel.", "label": "1", "is_selected": "0", "text": "This paper presents an adaptive randomized algorithm for computing the butterfly factorization of a x m n matrix with m n provided that both the matrix and its transpose can be rapidly applied to arbitrary vectors. The resulting factorization is composed of O (log n) sparse factors, each containing O (n) nonzero entries. The factorization can be attained using O (n 3 2 log n) computation and O (n log n) memory resources. The proposed algorithm applies to matrices with strong and weak admissibility conditions arising from surface integral equation solvers with a rigorous error bound, and is implemented in parallel."}
{"original_text": "We investigate GPU-based parallelization of Iterative-Deepening A (IDA). We show that straightforward thread-based parallelization techniques which were previously proposed for massively parallel SIMD processors perform poorly due to warp divergence and load imbalance. We propose Block-Parallel IDA (BPIDA), which assigns the search of a subtree to a block (a group of threads with access to fast shared memory) rather than a thread. On the 15-puzzle, BPIDA on a NVIDIA GRID K520 with 1536 CUDA cores achieves a speedup of 4.98 compared to a highly optimized sequential IDA implementation on a Xeon E5-2670 core. 1 1 footnote 1 This is an extended manuscript based on a paper accepted to appear in SoCS2017.", "label": "1", "is_selected": "0", "text": "We investigate GPU-based parallelization of Iterative-Deepening A (IDA). We show that straightforward thread-based parallelization techniques which were previously proposed for massively parallel SIMD processors perform poorly due to warp divergence and load imbalance. We propose Block-Parallel IDA (BPIDA), which assigns the search of a subtree to a block (a group of threads with access to fast shared memory) rather than a thread. On the 15-puzzle, BPIDA on a NVIDIA GRID K520 with 1536 CUDA cores achieves a speedup of 4.98 compared to a highly optimized sequential IDA implementation on a Xeon E5-2670 core. 1 1 footnote 1 This is an extended manuscript based on a paper accepted to appear in SoCS2017."}
{"original_text": "The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sentence along with its syntactic tree, have shown state-of-the-art performance. Recent work shows that a multi-layer LSTM language model outperforms competitive statistical syntactic linearization systems without using syntax. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task.", "label": "1", "is_selected": "0", "text": "The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sentence along with its syntactic tree, have shown state-of-the-art performance. Recent work shows that a multi-layer LSTM language model outperforms competitive statistical syntactic linearization systems without using syntax. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task."}
{"original_text": "Besides advanced telecommunications techniques, the most prominent evolution of wireless networks is the densification of network deployment. In particular, the increasing access pointsusers density and reduced cell size significantly enhance spatial reuse, thereby improving network capacity. Nevertheless, does network ultra-densification and over-deployment always boost the performance of wireless networks? Since the distance from transmitters to receivers is greatly reduced in dense networks, signal is more likely to be propagated from far- to near-field region. Without considering near-field propagation features, conventional understandings of the impact of network densification become doubtful. With this regard, it is imperative to reconsider the pros and cons brought by network densification. In this article, we first discuss the near-field propagation features in densely deployed network and verify through experimental results the validity of the proposed near-field propagation model. Considering near-field propagation, we further explore how dense is ultra-dense for wireless networks and provide a concrete interpretation of ultra-densification from the spatial throughput perspective. Meanwhile, as near-field propagation makes interference more complicated and difficult to handle, we shed light on the key challenges of applying interference management in ultra-dense wireless networks. Moreover, possible solutions are presented to suggest future directions.", "label": "1", "is_selected": "0", "text": "Besides advanced telecommunications techniques, the most prominent evolution of wireless networks is the densification of network deployment. In particular, the increasing access pointsusers density and reduced cell size significantly enhance spatial reuse, thereby improving network capacity. Nevertheless, does network ultra-densification and over-deployment always boost the performance of wireless networks? Since the distance from transmitters to receivers is greatly reduced in dense networks, signal is more likely to be propagated from far- to near-field region. Without considering near-field propagation features, conventional understandings of the impact of network densification become doubtful. With this regard, it is imperative to reconsider the pros and cons brought by network densification. In this article, we first discuss the near-field propagation features in densely deployed network and verify through experimental results the validity of the proposed near-field propagation model. Considering near-field propagation, we further explore how dense is ultra-dense for wireless networks and provide a concrete interpretation of ultra-densification from the spatial throughput perspective. Meanwhile, as near-field propagation makes interference more complicated and difficult to handle, we shed light on the key challenges of applying interference management in ultra-dense wireless networks. Moreover, possible solutions are presented to suggest future directions."}
{"original_text": "Some research institutions demand researchers to distribute the incomes they earn from publishing papers to their researchers andor co-authors. In this study, we deal with the Impact Factor-based ranking journal That is an interesting topic. Do you have any specific questions or ideas that you would like to discuss regarding this issue?", "label": "0", "is_selected": "0", "text": "Some research institutions demand researchers to distribute the incomes they earn from publishing papers to their researchers andor co-authors. In this study, we deal with the Impact Factor-based ranking journal That is an interesting topic. Do you have any specific questions or ideas that you would like to discuss regarding this issue?"}
{"original_text": "Digitisation of fruit trees using LiDAR enables analysis which can be used to better growing practices to improve yield. Sophisticated analysis requires geometric and semantic understanding of the data, including the ability to discern individual trees as well as identifying leafy and structural matter. Extraction of this information should be rapid, as should data capture, so that entire orchards can be processed, but existing methods for classification and segmentation rely on high-quality data or additional data sources like cameras. We present a method for analysis of LiDAR data specifically for individual tree location, segmentation and matter classification, which can operate on low-quality data captured by handheld or mobile LiDAR. Results demonstrate viability both on real data for avocado and mango trees and virtual data with independently controlled sensor noise and tree spacing.", "label": "1", "is_selected": "0", "text": "Digitisation of fruit trees using LiDAR enables analysis which can be used to better growing practices to improve yield. Sophisticated analysis requires geometric and semantic understanding of the data, including the ability to discern individual trees as well as identifying leafy and structural matter. Extraction of this information should be rapid, as should data capture, so that entire orchards can be processed, but existing methods for classification and segmentation rely on high-quality data or additional data sources like cameras. We present a method for analysis of LiDAR data specifically for individual tree location, segmentation and matter classification, which can operate on low-quality data captured by handheld or mobile LiDAR. Results demonstrate viability both on real data for avocado and mango trees and virtual data with independently controlled sensor noise and tree spacing."}
{"original_text": "Image inpainting is one of the most challenging tasks in computer vision. Recently, generative-based image inpainting methods have been shown to produce visually plausible images. However, they still have difficulties to generate the correct structures and colors as the masked region grows large. This drawback is due to the training stability issue of the generative models. This work introduces a new curriculum-style training approach in the context of image inpainting. The proposed method increases the masked region size progressively in training time, during test time the user gives variable size and multiple holes at arbitrary locations. Incorporating such an approach in GANs may stabilize the training and provides better color consistencies and captures object continuities. We validate our approach on the MSCOCO and CelebA datasets. We report qualitative and quantitative comparisons of our training approach in different models.", "label": "1", "is_selected": "0", "text": "Image inpainting is one of the most challenging tasks in computer vision. Recently, generative-based image inpainting methods have been shown to produce visually plausible images. However, they still have difficulties to generate the correct structures and colors as the masked region grows large. This drawback is due to the training stability issue of the generative models. This work introduces a new curriculum-style training approach in the context of image inpainting. The proposed method increases the masked region size progressively in training time, during test time the user gives variable size and multiple holes at arbitrary locations. Incorporating such an approach in GANs may stabilize the training and provides better color consistencies and captures object continuities. We validate our approach on the MSCOCO and CelebA datasets. We report qualitative and quantitative comparisons of our training approach in different models."}
{"original_text": "The global health threat from COVID-19 has been controlled in a number of instances by large-scale testing and contact tracing efforts. We created this document to suggest three functionalities on how a mobile application can help with contact tracing: 1. Location tracking: A mobile application can track the location of its users and notify them if they have been in close proximity to someone who has tested positive for COVID-19. This information can be anonymized to ensure privacy while still providing valuable data for contact tracing efforts. 2. Self-assessment: Users can take a self-assessment test to determine if they have symptoms of COVID-19. The mobile application can then guide them through the next steps, such as getting tested or self-quarantining. 3. Notification system: The mobile application can notify users if they have been in contact with someone who has tested positive for COVID-19, along with instructions on what to do next. This can help quickly identify potential cases and prevent the spread of the virus. Overall, a mobile application with these functionalities could significantly aid in contact tracing efforts and help control the spread of COVID-19.", "label": "0", "is_selected": "0", "text": "The global health threat from COVID-19 has been controlled in a number of instances by large-scale testing and contact tracing efforts. We created this document to suggest three functionalities on how a mobile application can help with contact tracing: 1. Location tracking: A mobile application can track the location of its users and notify them if they have been in close proximity to someone who has tested positive for COVID-19. This information can be anonymized to ensure privacy while still providing valuable data for contact tracing efforts. 2. Self-assessment: Users can take a self-assessment test to determine if they have symptoms of COVID-19. The mobile application can then guide them through the next steps, such as getting tested or self-quarantining. 3. Notification system: The mobile application can notify users if they have been in contact with someone who has tested positive for COVID-19, along with instructions on what to do next. This can help quickly identify potential cases and prevent the spread of the virus. Overall, a mobile application with these functionalities could significantly aid in contact tracing efforts and help control the spread of COVID-19."}
{"original_text": "The Hotelling game consists of n servers each choosing a point on the line segment, so as to maximize the amount of clients it attracts. Clients are uniformly distributed along the line, and each client buys from the closest server. In this paper, we study a fault-prone version of the Hotelling game, where the line fails at multiple random locations. Each failure disconnects the line, blocking the passage of clients. We show that the game admits a Nash equilibrium if and only if the rate of faults exceeds a certain threshold, and calculate that threshold approximately. Moreover, when a Nash equilibrium exists we show it is unique and construct it explicitly. Hence, somewhat surprisingly, the potential occurrence of failures has a stabilizing effect on the game (provided there are enough of them). Additionally, we study the social cost of the game (measured in terms of the total transportation cost of the clients), which also seems to benefit in a certain sense from the potential presence of failures.", "label": "1", "is_selected": "0", "text": "The Hotelling game consists of n servers each choosing a point on the line segment, so as to maximize the amount of clients it attracts. Clients are uniformly distributed along the line, and each client buys from the closest server. In this paper, we study a fault-prone version of the Hotelling game, where the line fails at multiple random locations. Each failure disconnects the line, blocking the passage of clients. We show that the game admits a Nash equilibrium if and only if the rate of faults exceeds a certain threshold, and calculate that threshold approximately. Moreover, when a Nash equilibrium exists we show it is unique and construct it explicitly. Hence, somewhat surprisingly, the potential occurrence of failures has a stabilizing effect on the game (provided there are enough of them). Additionally, we study the social cost of the game (measured in terms of the total transportation cost of the clients), which also seems to benefit in a certain sense from the potential presence of failures."}
{"original_text": "Shannon's theory of information was built on the assumption that the information carriers were classical systems. Its quantum counterpart, quantum Shannon theory, explores the new possibilities arising when the information carriers are quantum systems. Traditionally, quantum Shannon theory has focussed on scenarios where the internal state of the information carriers is quantum, while their trajectory is classical. Here we propose a second level of quantisation where both the information and its propagation in spacetime is treated quantum mechanically. The framework is illustrated with a number of examples, showcasing some of the counterintuitive phenomena taking place when information travels simultaneously through multiple transmission lines.", "label": "1", "is_selected": "0", "text": "Shannon's theory of information was built on the assumption that the information carriers were classical systems. Its quantum counterpart, quantum Shannon theory, explores the new possibilities arising when the information carriers are quantum systems. Traditionally, quantum Shannon theory has focussed on scenarios where the internal state of the information carriers is quantum, while their trajectory is classical. Here we propose a second level of quantisation where both the information and its propagation in spacetime is treated quantum mechanically. The framework is illustrated with a number of examples, showcasing some of the counterintuitive phenomena taking place when information travels simultaneously through multiple transmission lines."}
{"original_text": "Storage systems have a strong need for substantially improving their error correction capabilities, especially for long-term storage where the accumulating errors can exceed the decoding threshold of error-correcting codes (ECCs). In this work, a new scheme is presented that uses deep learning to perform soft decoding for noisy files based on their natural redundancy. The soft decoding result is then combined with ECCs for substantially better error correction performance. The scheme is representation-oblivious: it requires no prior knowledge on how data are represented (e.g., mapped from symbols to bits, compressed, and combined with meta data) in different types of files, which makes the solution more convenient to use for storage systems. Experimental results confirm that the scheme can substantially improve the ability to recover data for different types of files even when the bit error rates in the files have significantly exceeded the decoding threshold of the ECC. The code of this work has been publicly released. 1 1 footnote 1", "label": "1", "is_selected": "0", "text": "Storage systems have a strong need for substantially improving their error correction capabilities, especially for long-term storage where the accumulating errors can exceed the decoding threshold of error-correcting codes (ECCs). In this work, a new scheme is presented that uses deep learning to perform soft decoding for noisy files based on their natural redundancy. The soft decoding result is then combined with ECCs for substantially better error correction performance. The scheme is representation-oblivious: it requires no prior knowledge on how data are represented (e.g., mapped from symbols to bits, compressed, and combined with meta data) in different types of files, which makes the solution more convenient to use for storage systems. Experimental results confirm that the scheme can substantially improve the ability to recover data for different types of files even when the bit error rates in the files have significantly exceeded the decoding threshold of the ECC. The code of this work has been publicly released. 1 1 footnote 1"}
{"original_text": "Deep neural networks are highly expressive machine learning models with the ability to interpolate arbitrary datasets. Deep nets are typically optimized via first-order methods and the optimization process crucially depends on the characteristics of the network as well as the dataset. This work sheds light on the relation between the network size and the properties of the dataset with an emphasis on deep residual networks (ResNets). Our contribution is that if the network Jacobian is full rank, gradient descent for the quadratic loss and smooth activation converges to the global minima even if the network width m of the ResNet scales linearly with the sample size n, and independently from the network depth. To the best of our knowledge, this is the first work which provides a theoretical guarantee for the convergence of neural networks in the m O (n) regime.", "label": "1", "is_selected": "0", "text": "Deep neural networks are highly expressive machine learning models with the ability to interpolate arbitrary datasets. Deep nets are typically optimized via first-order methods and the optimization process crucially depends on the characteristics of the network as well as the dataset. This work sheds light on the relation between the network size and the properties of the dataset with an emphasis on deep residual networks (ResNets). Our contribution is that if the network Jacobian is full rank, gradient descent for the quadratic loss and smooth activation converges to the global minima even if the network width m of the ResNet scales linearly with the sample size n, and independently from the network depth. To the best of our knowledge, this is the first work which provides a theoretical guarantee for the convergence of neural networks in the m O (n) regime."}
{"original_text": "We consider the stochastic extensible bin packing problem (SEBP) in which n items of stochastic size are packed into m bins of unit capacity. In contrast to the classical bin packing problem, the number of bins is fixed and they can be extended at extra cost. This problem plays an important role in stochastic environments such as in surgery scheduling: Patients must be assigned to operating rooms beforehand, such that the regular capacity is fully utilized while the amount of overtime is as small as possible. This paper focuses on essential ratios between different classes of policies: First, we consider the price of non-splittability, in which we compare the optimal non-anticipatory policy against the optimal fractional assignment policy. We show that this ratio has a tight upper bound of 2. Moreover, we develop an analysis of a fixed assignment variant of the LEPT rule yielding a tight approximation ratio of (1 e - 1) 1.368. Furthermore, we prove that the price of fixed assignments, related to the benefit of adaptivity, which describes the loss when restricting to fixed assignment policies, is within the same factor. This shows that in some sense, LEPT is the best fixed assignment policy we can hope for. We also provide a lower bound on the performance of this policy comparing against an optimal fixed assignment policy. Finally, we obtain improved bounds for the case where the processing times are drawn from a particular family of distributions, with either a bounded Pietra index or when the familly is stochastically dominated at the second order.", "label": "1", "is_selected": "0", "text": "We consider the stochastic extensible bin packing problem (SEBP) in which n items of stochastic size are packed into m bins of unit capacity. In contrast to the classical bin packing problem, the number of bins is fixed and they can be extended at extra cost. This problem plays an important role in stochastic environments such as in surgery scheduling: Patients must be assigned to operating rooms beforehand, such that the regular capacity is fully utilized while the amount of overtime is as small as possible. This paper focuses on essential ratios between different classes of policies: First, we consider the price of non-splittability, in which we compare the optimal non-anticipatory policy against the optimal fractional assignment policy. We show that this ratio has a tight upper bound of 2. Moreover, we develop an analysis of a fixed assignment variant of the LEPT rule yielding a tight approximation ratio of (1 e - 1) 1.368. Furthermore, we prove that the price of fixed assignments, related to the benefit of adaptivity, which describes the loss when restricting to fixed assignment policies, is within the same factor. This shows that in some sense, LEPT is the best fixed assignment policy we can hope for. We also provide a lower bound on the performance of this policy comparing against an optimal fixed assignment policy. Finally, we obtain improved bounds for the case where the processing times are drawn from a particular family of distributions, with either a bounded Pietra index or when the familly is stochastically dominated at the second order."}
{"original_text": "The kinetic battery model is a popular model of the dynamic behavior of a conventional battery, useful to predict or optimize the time until battery depletion. The model however lacks certain obvious aspects of batteries in-the-wild, especially with respect to (i) the effects of random influences and (ii) the behavior when charging up to capacity bounds. This paper considers the kinetic battery model with bounded capacity in the context of piecewise constant yet random charging and discharging. The resulting model enables the time-dependent evaluation of the risk of battery depletion. This is exemplified in a power dependability study of a nano satellite mission.", "label": "1", "is_selected": "0", "text": "The kinetic battery model is a popular model of the dynamic behavior of a conventional battery, useful to predict or optimize the time until battery depletion. The model however lacks certain obvious aspects of batteries in-the-wild, especially with respect to (i) the effects of random influences and (ii) the behavior when charging up to capacity bounds. This paper considers the kinetic battery model with bounded capacity in the context of piecewise constant yet random charging and discharging. The resulting model enables the time-dependent evaluation of the risk of battery depletion. This is exemplified in a power dependability study of a nano satellite mission."}
{"original_text": "In the domain of emergency management during hazard crises, having sufficient situational awareness information is critical. It requires capturing and integrating information from sources such as satellite images, local sensors , social media, news reports, and first responders' observations. With this information, emergency managers can develop a comprehensive understanding of the potential impacts of the hazard and implement effective response strategies to minimize damage and save lives. As a helpful assistant, I can suggest that emergency managers use advanced technologies such as Geographic Information Systems (GIS) and Artificial Intelligence (AI) to streamline the process of combing through large volumes of data from various sources. The integration of GIS data along with other data sources would enable them to visualize the hazard impact, identify vulnerable populations, and enhance the effectiveness of the response plan. Additionally, pre-defined communication protocols with key stakeholders such as local authorities, first responders, and the media can be established in advance to facilitate information sharing and provide verified information in a timely manner to the public. Being proactive and having a comprehensive situational awareness of the hazard can help emergency managers make informed decisions, take quick and timely action, and effectively mitigate the impact of the crisis.", "label": "0", "is_selected": "1", "text": "In our series of letters from African journalists, film-maker, and columnist Sharmila Tagore looks at the importance of situational awareness in the wake of natural disasters."}
{"original_text": "Deep learning models, such as the fully convolutional network (FCN), have been widely used in 3D biomedical segmentation and achieved state-of-the-art performance. Multiple modalities are often used for disease diagnosis That is true. Deep learning models, such as FCN, have been successful in medical image segmentation tasks because they can utilize the spatial information present in the image data. In addition, using multiple modalities can provide complementary information and improve the accuracy of disease diagnosis. For example, combining magnetic resonance imaging (MRI) modalities, such as T1-weighted and T2-weighted images, can provide different anatomical and pathological information for a more accurate diagnosis of brain tumors.", "label": "0", "is_selected": "0", "text": "Deep learning models, such as the fully convolutional network (FCN), have been widely used in 3D biomedical segmentation and achieved state-of-the-art performance. Multiple modalities are often used for disease diagnosis That is true. Deep learning models, such as FCN, have been successful in medical image segmentation tasks because they can utilize the spatial information present in the image data. In addition, using multiple modalities can provide complementary information and improve the accuracy of disease diagnosis. For example, combining magnetic resonance imaging (MRI) modalities, such as T1-weighted and T2-weighted images, can provide different anatomical and pathological information for a more accurate diagnosis of brain tumors."}
{"original_text": "Many modern applications produce massive streams of data series that need to be analyzed, requiring efficient similarity search operations. However, the state-of-the-art data series indexes that are used for this purpose do not scale well for massive datasets in terms of performance, or storage costs. We pinpoint the problem to the fact that existing summarizations of data series used for indexing cannot be sorted while keeping similar data series close to each other in the sorted order. To address this problem, we present Coconut, the first data series index based on sortable summarizations, and the first efficient solution for indexing and querying streaming series. The first innovation in Coconut is an inverted, sortable data series summarization that organizes data series based on a z-order curve, keeping similar series close to each other in the sorted order. As a result, Coconut is able to use bulk-loading and updating techniques that rely on sorting to quickly build and maintain a contiguous index using large sequential disk IOs. We then explore prefix-based and median-based splitting policies for bottom-up bulk-loading, showing that median-based splitting outperforms the state of the art, ensuring that all nodes are densely populated. Finally, we explore the impact of sortable summarizations on variable size window queries, showing that they can be supported in the presence of updates through efficient merging of temporal partitions. Overall, we show analytically and empirically that Coconut dominates the state-of-the-art data series indexes in terms of construction speed, query speed, and storage costs.", "label": "1", "is_selected": "0", "text": "Many modern applications produce massive streams of data series that need to be analyzed, requiring efficient similarity search operations. However, the state-of-the-art data series indexes that are used for this purpose do not scale well for massive datasets in terms of performance, or storage costs. We pinpoint the problem to the fact that existing summarizations of data series used for indexing cannot be sorted while keeping similar data series close to each other in the sorted order. To address this problem, we present Coconut, the first data series index based on sortable summarizations, and the first efficient solution for indexing and querying streaming series. The first innovation in Coconut is an inverted, sortable data series summarization that organizes data series based on a z-order curve, keeping similar series close to each other in the sorted order. As a result, Coconut is able to use bulk-loading and updating techniques that rely on sorting to quickly build and maintain a contiguous index using large sequential disk IOs. We then explore prefix-based and median-based splitting policies for bottom-up bulk-loading, showing that median-based splitting outperforms the state of the art, ensuring that all nodes are densely populated. Finally, we explore the impact of sortable summarizations on variable size window queries, showing that they can be supported in the presence of updates through efficient merging of temporal partitions. Overall, we show analytically and empirically that Coconut dominates the state-of-the-art data series indexes in terms of construction speed, query speed, and storage costs."}
{"original_text": "Visible light communications (VLC) have emerged as strong candidates for meeting the escalating demand for high data rates. Consider a VLC network, where multiple access-points (APs) serve both energy-harvesting users As an assistant, I can provide you some information about VLC networks and their capabilities. VLC technology is based on modulating the intensity of visible light to transmit data. It uses the visible spectrum of light to transmit information, which makes it a promising technology for indoor communication. VLC can achieve very high data rates, up to several gigabits per second, making it suitable for data-intensive applications, such as video streaming, cloud computing, and teleconferencing. In a VLC network, multiple access-points (APs) can serve both energy-harvesting users. The APs are equipped with LEDs, which emit modulated signals in the visible light spectrum. The users can receive the signals using photodiodes or cameras, which can also harvest energy from the light signals. The users can transmit data back to the APs using infrared or radio frequency signals, which adds up to the total data rate of the network. VLC networks have several advantages over other wireless communication technologies. For instance, VLC is immune to electromagnetic interference, making it suitable for use in sensitive environments, such as hospitals or aviation. Additionally, VLC is secure, as it is hard to intercept the signals due to the directional nature of light. Furthermore, VLC is energy-efficient, as it utilizes the already installed lighting infrastructure in buildings. However, VLC also has some limitations, such as limited range, indoor operation only, and susceptibility to ambient light interference. Nevertheless, VLC is a promising technology, which has the potential to revolutionize the way we communicate and access information.", "label": "0", "is_selected": "1", "text": "The demand for high-speed data services has led to the emergence of new technologies to meet the growing demand for data services in the home, office, and elsewhere."}
{"original_text": "Visible light communications (VLC) have emerged as strong candidates for meeting the escalating demand for high data rates. Consider a VLC network, where multiple access-points (APs) serve both energy-harvesting users (EHUs), i.e., users which harvest energy from light intensity, and information-users (IUs), i.e., users which gather data information. In order to jointly balance the achievable sum-rate at the IUs and the energy harvested by the EHUs, the paper considers maximizing a network-wide utility, which consists of a weighted-sum of the IUs sum-rate and the EHUs harvested-energy, subject to individual IU rate constraint, individual EHU harvested-energy constraint, and AP power constraints, so as to jointly determine the direct current (DC) -bias value at each AP, and the users' powers. The paper solves such a difficult non-convex optimization problem using an iterative approach which relies on inner convex approximations, and compensates for the used approximations using proper outer-loop updates. The paper further considers solving the special cases of the problem, i.e., maximizing the sum-rate, and maximizing the total harvested-energy, both subject to the same constraints. Numerical results highlight the significant performance improvement of the proposed algorithms, and illustrate the impacts of the network parameters on the performance trade-off between the sum-rate and harvested-energy.", "label": "1", "is_selected": "0", "text": "Visible light communications (VLC) have emerged as strong candidates for meeting the escalating demand for high data rates. Consider a VLC network, where multiple access-points (APs) serve both energy-harvesting users (EHUs), i.e., users which harvest energy from light intensity, and information-users (IUs), i.e., users which gather data information. In order to jointly balance the achievable sum-rate at the IUs and the energy harvested by the EHUs, the paper considers maximizing a network-wide utility, which consists of a weighted-sum of the IUs sum-rate and the EHUs harvested-energy, subject to individual IU rate constraint, individual EHU harvested-energy constraint, and AP power constraints, so as to jointly determine the direct current (DC) -bias value at each AP, and the users' powers. The paper solves such a difficult non-convex optimization problem using an iterative approach which relies on inner convex approximations, and compensates for the used approximations using proper outer-loop updates. The paper further considers solving the special cases of the problem, i.e., maximizing the sum-rate, and maximizing the total harvested-energy, both subject to the same constraints. Numerical results highlight the significant performance improvement of the proposed algorithms, and illustrate the impacts of the network parameters on the performance trade-off between the sum-rate and harvested-energy."}
{"original_text": "In optimization, the negative gradient of a function denotes the direction of steepest descent. Furthermore, traveling in any direction orthogonal to the gradient maintains the value of the function. In this work, we show that these orthogonal directions that are ignored by gradient descent can be critical in equilibrium problems. Equilibrium problems have drawn heightened attention in machine learning due to the emergence of the Generative Adversarial Network (GAN). We use the framework of Variational Inequalities to analyze popular training algorithms for a fundamental GAN variant: the Wasserstein Linear-Quadratic GAN. We show that the steepest descent direction causes divergence from the equilibrium, and convergence to the equilibrium is achieved through following a particular orthogonal direction. We call this successful technique Crossing-the-Curl, named for its mathematical derivation as well as its intuition: identify the game's axis of rotation and move \"across\" space in the direction towards smaller \"curling.\"", "label": "1", "is_selected": "0", "text": "In optimization, the negative gradient of a function denotes the direction of steepest descent. Furthermore, traveling in any direction orthogonal to the gradient maintains the value of the function. In this work, we show that these orthogonal directions that are ignored by gradient descent can be critical in equilibrium problems. Equilibrium problems have drawn heightened attention in machine learning due to the emergence of the Generative Adversarial Network (GAN). We use the framework of Variational Inequalities to analyze popular training algorithms for a fundamental GAN variant: the Wasserstein Linear-Quadratic GAN. We show that the steepest descent direction causes divergence from the equilibrium, and convergence to the equilibrium is achieved through following a particular orthogonal direction. We call this successful technique Crossing-the-Curl, named for its mathematical derivation as well as its intuition: identify the game's axis of rotation and move \"across\" space in the direction towards smaller \"curling.\""}
{"original_text": "Neural networks trained with backpropagation often struggle to identify classes that have been observed a small number of times. In applications where most class labels are rare, such as language modeling, this is a common problem. One approach to improve the performance on rare classes is to use techniques such as class weighting, which assigns higher weights to rare classes during training. Another approach is to use techniques such as data augmentation, which generates new data points for rare classes by perturbing existing samples. Alternatively, one can use transfer learning, where a pre-trained model on a related task can be fine-tuned on the data containing rare classes. Finally, ensembling can be a useful approach to improve rare class identification, where multiple models are trained on the same data using different architectures andor hyperparameters, and their predictions are averaged.", "label": "0", "is_selected": "0", "text": "Neural networks trained with backpropagation often struggle to identify classes that have been observed a small number of times. In applications where most class labels are rare, such as language modeling, this is a common problem. One approach to improve the performance on rare classes is to use techniques such as class weighting, which assigns higher weights to rare classes during training. Another approach is to use techniques such as data augmentation, which generates new data points for rare classes by perturbing existing samples. Alternatively, one can use transfer learning, where a pre-trained model on a related task can be fine-tuned on the data containing rare classes. Finally, ensembling can be a useful approach to improve rare class identification, where multiple models are trained on the same data using different architectures andor hyperparameters, and their predictions are averaged."}
{"original_text": "Great successes of deep neural networks have been witnessed in various real applications. Many algorithmic and implementation techniques have been developed; however, theoretical understanding of many aspects of deep neural networks is far from clear. A particular interesting issue is the usefulness of dropout, which was motivated from the intuition of preventing complex co-adaptation of feature detectors. In this paper, we study the Rademacher complexity of different types of dropout, and our theoretical results disclose that for shallow neural networks (with one or none hidden layer) dropout is able to reduce the Rademacher complexity in polynomial, whereas for deep neural networks it can amazingly lead to an exponential reduction.", "label": "1", "is_selected": "0", "text": "Great successes of deep neural networks have been witnessed in various real applications. Many algorithmic and implementation techniques have been developed; however, theoretical understanding of many aspects of deep neural networks is far from clear. A particular interesting issue is the usefulness of dropout, which was motivated from the intuition of preventing complex co-adaptation of feature detectors. In this paper, we study the Rademacher complexity of different types of dropout, and our theoretical results disclose that for shallow neural networks (with one or none hidden layer) dropout is able to reduce the Rademacher complexity in polynomial, whereas for deep neural networks it can amazingly lead to an exponential reduction."}
{"original_text": "This case for the Transformation Tool Contest 2013 is about evaluating the scope and usability of transformation languages and tools for a set of four tasks requiring very different capabilities. One task deals with typical model-to-model transformation problem, there's a model-to-text problem, there are two in-place transformation problems, and finally there's a task dealing with validation of models resulting from the transformations. The tasks build upon each other, but the transformation case project also provides all intermediate models, thus making it possible to skip tasks that are not suited for a particular tool, or for parallelizing the work among members of participating teams.", "label": "1", "is_selected": "0", "text": "This case for the Transformation Tool Contest 2013 is about evaluating the scope and usability of transformation languages and tools for a set of four tasks requiring very different capabilities. One task deals with typical model-to-model transformation problem, there's a model-to-text problem, there are two in-place transformation problems, and finally there's a task dealing with validation of models resulting from the transformations. The tasks build upon each other, but the transformation case project also provides all intermediate models, thus making it possible to skip tasks that are not suited for a particular tool, or for parallelizing the work among members of participating teams."}
{"original_text": "As of September 2020, the COVID-19 pandemic continues to devastate the health and well-being of the global population. With more than 33 million confirmed cases and over a million deaths, global health organizations are still a long way from fully containing the pandemic. This pandemic has raised serious questions about the emergency preparedness of health agencies, not only in terms of treatment of an unseen disease, but also in identifying its early symptoms. In the particular case of COVID-19, several studies have indicated that chest radiography images of the infected patients show characteristic abnormalities. However, at the onset of a given pandemic, such as COVID-19, there may not be sufficient data for the affected cases to train models for their robust detection. Hence, supervised classification is ill-posed for this problem because the time spent in collecting large amounts of infected peoples' data could lead to the loss of human lives and delays in preventive interventions. Therefore, we formulate this problem within a one-class classification framework, in which the data for healthy patients is abundantly available, whereas no training data is present for the class of interest (COVID-19 in our case). To solve this problem, we present COVIDomaly, a convolutional autoencoder framework to detect unseen COVID-19 cases from the chest radiographs. We tested two settings on a publicly available dataset (COVIDx) by training the model on chest X-rays from (i) only healthy adults, and (ii) healthy and other non-COVID-19 pneumonia, and detected COVID-19 as an anomaly. After performing 3-fold cross validation, we obtain a pooled ROC-AUC of 0.7652 and 0.6902 in the two settings respectively. These results are very encouraging and pave the way towards research for ensuring emergency preparedness in future pandemics, especially the ones that could be detected from chest X-rays.", "label": "1", "is_selected": "0", "text": "As of September 2020, the COVID-19 pandemic continues to devastate the health and well-being of the global population. With more than 33 million confirmed cases and over a million deaths, global health organizations are still a long way from fully containing the pandemic. This pandemic has raised serious questions about the emergency preparedness of health agencies, not only in terms of treatment of an unseen disease, but also in identifying its early symptoms. In the particular case of COVID-19, several studies have indicated that chest radiography images of the infected patients show characteristic abnormalities. However, at the onset of a given pandemic, such as COVID-19, there may not be sufficient data for the affected cases to train models for their robust detection. Hence, supervised classification is ill-posed for this problem because the time spent in collecting large amounts of infected peoples' data could lead to the loss of human lives and delays in preventive interventions. Therefore, we formulate this problem within a one-class classification framework, in which the data for healthy patients is abundantly available, whereas no training data is present for the class of interest (COVID-19 in our case). To solve this problem, we present COVIDomaly, a convolutional autoencoder framework to detect unseen COVID-19 cases from the chest radiographs. We tested two settings on a publicly available dataset (COVIDx) by training the model on chest X-rays from (i) only healthy adults, and (ii) healthy and other non-COVID-19 pneumonia, and detected COVID-19 as an anomaly. After performing 3-fold cross validation, we obtain a pooled ROC-AUC of 0.7652 and 0.6902 in the two settings respectively. These results are very encouraging and pave the way towards research for ensuring emergency preparedness in future pandemics, especially the ones that could be detected from chest X-rays."}
{"original_text": "We study detection of random signals corrupted by noise that over time switchtheir values (states) from a finite set of possible values, where theswitchings occur at unknown points in time. This problem is known as the hidden Markov model (HMM). The goal is to estimate the sequence of states based on the observed signal. This can be achieved using the Viterbi algorithm, which is a dynamic programming algorithm that finds the most likely sequence of states that could have generated the observed signal. The algorithm takes into account the probability of transitioning from one state to another as well as the emission probabilities of the observed signal given each state. By iteratively calculating the most likely path through the state sequence, the Viterbi algorithm can provide an estimate of the sequence of states underlying the observed signal.", "label": "0", "is_selected": "1", "text": "In this paper, we present a novel method for the detection of signals that are hidden in Markov chains, which is a key problem in the study of Monte Carlo."}
{"original_text": "Synthesizing physiologically-accurate human movement in a variety of conditions can help practitioners plan surgeries, design experiments, or prototype assistive devices in simulated environments, reducing time and costs and improving treatment outcomes. Because of the large and complex solution spaces of biomechanical models, current methods are constrained to specific movements and models, requiring careful design of a controller and hindering many possible applications. We sought to discover if modern optimization methods efficiently explore these complex spaces. To do this, we posed the problem as a competition in which participants were tasked with developing a controller to enable a physiologically-based human model to navigate a complex obstacle course as quickly as possible, without using any experimental data. They were provided with a human musculoskeletal model and a physics-based simulation environment. In this paper, we discuss the design of the competition, technical difficulties, results, and analysis of the top controllers. The challenge proved that deep reinforcement learning techniques, despite their high computational cost, can be successfully employed as an optimization method for synthesizing physiologically feasible motion in high-dimensional biomechanical systems.", "label": "1", "is_selected": "0", "text": "Synthesizing physiologically-accurate human movement in a variety of conditions can help practitioners plan surgeries, design experiments, or prototype assistive devices in simulated environments, reducing time and costs and improving treatment outcomes. Because of the large and complex solution spaces of biomechanical models, current methods are constrained to specific movements and models, requiring careful design of a controller and hindering many possible applications. We sought to discover if modern optimization methods efficiently explore these complex spaces. To do this, we posed the problem as a competition in which participants were tasked with developing a controller to enable a physiologically-based human model to navigate a complex obstacle course as quickly as possible, without using any experimental data. They were provided with a human musculoskeletal model and a physics-based simulation environment. In this paper, we discuss the design of the competition, technical difficulties, results, and analysis of the top controllers. The challenge proved that deep reinforcement learning techniques, despite their high computational cost, can be successfully employed as an optimization method for synthesizing physiologically feasible motion in high-dimensional biomechanical systems."}
{"original_text": "A lot of research has been focused on secure outsourcing of biometric identification in the context of cloud computing. In such schemes, both the encrypted biometric database and the identification process are outsourced to the cloud. The ultimate goal is to protect the security and privacy of the biometric database and the query templates. Security analysis shows that previous schemes suffer from the enrolment attack and unnecessarily expose more information than needed. In this paper, we propose a new secure outsourcing scheme aims at enhancing the security from these two aspects. First, besides all the attacks discussed in previous schemes, our proposed scheme is also secure against the enrolment attack. Second, we model the identification process as a fixed radius similarity query problem instead of the kNN search problem. Such a modelling is able to reduce the exposed information thus enhancing the privacy of the biometric database. Our comprehensive security and complexity analysis show that our scheme is able to enhance the security and privacy of the biometric database and query templates while maintaining the same computational savings from outsourcing.", "label": "1", "is_selected": "0", "text": "A lot of research has been focused on secure outsourcing of biometric identification in the context of cloud computing. In such schemes, both the encrypted biometric database and the identification process are outsourced to the cloud. The ultimate goal is to protect the security and privacy of the biometric database and the query templates. Security analysis shows that previous schemes suffer from the enrolment attack and unnecessarily expose more information than needed. In this paper, we propose a new secure outsourcing scheme aims at enhancing the security from these two aspects. First, besides all the attacks discussed in previous schemes, our proposed scheme is also secure against the enrolment attack. Second, we model the identification process as a fixed radius similarity query problem instead of the kNN search problem. Such a modelling is able to reduce the exposed information thus enhancing the privacy of the biometric database. Our comprehensive security and complexity analysis show that our scheme is able to enhance the security and privacy of the biometric database and query templates while maintaining the same computational savings from outsourcing."}
{"original_text": "We present a bandit algorithm, SAO (Stochastic and Adversarial Optimal), whose regret is, essentially, optimal both for adversarial rewards and for stochastic rewards. Specifically, SAO combines the O (n) worst-case regret of Exp3 [, ] for adversarial rewards and the (poly) logarithmic regret of UCB1 [, ] for stochastic rewards. Adversarial rewards and stochastic rewards are the two main settings in the literature on (non-Bayesian) multi-armed bandits. Prior work on multi-armed bandits treats them separately, and does not attempt to jointly optimize for both. Our result falls into a general theme of achieving good worst-case performance while also taking advantage of \"nice\" problem instances, an important issue in the design of algorithms with partially known inputs.", "label": "1", "is_selected": "0", "text": "We present a bandit algorithm, SAO (Stochastic and Adversarial Optimal), whose regret is, essentially, optimal both for adversarial rewards and for stochastic rewards. Specifically, SAO combines the O (n) worst-case regret of Exp3 [, ] for adversarial rewards and the (poly) logarithmic regret of UCB1 [, ] for stochastic rewards. Adversarial rewards and stochastic rewards are the two main settings in the literature on (non-Bayesian) multi-armed bandits. Prior work on multi-armed bandits treats them separately, and does not attempt to jointly optimize for both. Our result falls into a general theme of achieving good worst-case performance while also taking advantage of \"nice\" problem instances, an important issue in the design of algorithms with partially known inputs."}
{"original_text": "Data augmentation in deep neural networks is the process of generating artificial data in order to reduce the variance of the classifier with the goal to reduce the number of errors. This idea has been shown to improve deep neural network's generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike in image recognition problems, data augmentation techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved, especially for small datasets that exhibit overfitting, when a data augmentation method is adopted. In this paper, we fill this gap by investigating the application of a recently proposed data augmentation technique based on the Dynamic Time Warping distance, for a deep learning model for TSC. To evaluate the potential of augmenting the training set, we performed extensive experiments using the UCR TSC benchmark. Our preliminary experiments reveal that data augmentation can drastically increase deep CNN's accuracy on some datasets and significantly improve the deep model's accuracy when the method is used in an ensemble approach.", "label": "1", "is_selected": "0", "text": "Data augmentation in deep neural networks is the process of generating artificial data in order to reduce the variance of the classifier with the goal to reduce the number of errors. This idea has been shown to improve deep neural network's generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike in image recognition problems, data augmentation techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved, especially for small datasets that exhibit overfitting, when a data augmentation method is adopted. In this paper, we fill this gap by investigating the application of a recently proposed data augmentation technique based on the Dynamic Time Warping distance, for a deep learning model for TSC. To evaluate the potential of augmenting the training set, we performed extensive experiments using the UCR TSC benchmark. Our preliminary experiments reveal that data augmentation can drastically increase deep CNN's accuracy on some datasets and significantly improve the deep model's accuracy when the method is used in an ensemble approach."}
{"original_text": "The aim of this paper is to facilitate nuanced discussion around research norms and practices to mitigate the harmful impacts of advances in machine learning (ML). We focus particularly on the ethical considerations associated with ML, including issues of bias, fairness, accountability, and transparency. By highlighting these issues, we hope to encourage researchers to consider the potential impact of their work on society and to adopt responsible ML practices. One important aspect of responsible ML research is ensuring that datasets used in training and testing models are representative and diverse. This can help to avoid perpetuating biases and discrimination that might be present in the data. Researchers should also be transparent about their data sources and collection methods, including any potential biases or limitations. Another important consideration is the use of explainable and interpretable models. While deep learning models have achieved impressive results in many domains, they can be difficult to interpret and explain, which can lead to concerns about accountability and fairness. Researchers should strive to develop models that are not only accurate but also explicable, allowing stakeholders to understand how decisions are being made. Finally, ethical considerations should be integrated throughout the entire research process, from data collection to model development, deployment, and evaluation. Researchers should collaborate with stakeholders across disciplines and communities to ensure that their work aligns with societal values and promotes social good. As a helpful assistant, I can suggest that researchers be aware of the potential ethical implications of their work, engage in ongoing dialogue with stakeholders, and be proactive in addressing potential harms. Adhering to responsible ML practices can help to ensure that advances in machine learning are used for the benefit of all.", "label": "0", "is_selected": "1", "text": "Ethical considerations should be integrated throughout the entire research process, from data collection to model development, deployment, and evaluation Researchers should collaborate with stakeholders across disciplines and communities to ensure that their work aligns with societal values and promotes social"}
{"original_text": "The Dubins Traveling Salesman Problem (DTSP) has generated significant interest over the last decade due to its occurrence in several civil and military surveillance applications. Currently, there is no algorithm that can efficiently solve DTSP for all cases. However, there are several approaches that can provide relatively good solutions. One approach is to use heuristics, which are fast and practical methods that do not guarantee optimal solutions but can provide good solutions in reasonable time. Examples of heuristics for DTSP include the Nearest Neighbor algorithm, the Approximate Minimum Spanning Tree algorithm, and the 2-Opt algorithm. Another approach is to use metaheuristics, which are optimization algorithms that can search through large solution spaces and find good solutions. Examples of metaheuristics for DTSP include the Genetic algorithm, the Simulated Annealing algorithm, and the Ant Colony Optimization algorithm. Finally, there are also exact algorithms that can solve DTSP optimally for small instances. One example is the cutting-plane algorithm, which is based on linear programming. Another example is the branch-and-bound algorithm, which is a tree-based search algorithm that allows for pruning infeasible solutions. The choice of algorithm depends on the size of the problem, the required solution quality, the available computational resources, and the specific constraints of the application.", "label": "0", "is_selected": "1", "text": "This paper presents an algorithm that can efficiently solve the Dubins Salesman Problem, which is a difficult problem to solve due to its complexity and the fact that the salesman is not always present."}
{"original_text": "Hypothesis testing for graphs has been an important tool in applied research fields for more than two decades, and still remains a challenging problem as one often needs to draw inference from few replicates of large graphs. Recent studies in statistics and learning theory have provided some theoretical insights about such high-dimensional graph testing problems, but the practicality of the developed theoretical methods remains an open question. In this paper, we consider the problem of two-sample testing of large graphs. We demonstrate the practical merits and limitations of existing theoretical tests and their bootstrapped variants. We also propose two new tests based on asymptotic distributions. We show that these tests are computationally less expensive and, in some cases, more reliable than the existing methods.", "label": "1", "is_selected": "0", "text": "Hypothesis testing for graphs has been an important tool in applied research fields for more than two decades, and still remains a challenging problem as one often needs to draw inference from few replicates of large graphs. Recent studies in statistics and learning theory have provided some theoretical insights about such high-dimensional graph testing problems, but the practicality of the developed theoretical methods remains an open question. In this paper, we consider the problem of two-sample testing of large graphs. We demonstrate the practical merits and limitations of existing theoretical tests and their bootstrapped variants. We also propose two new tests based on asymptotic distributions. We show that these tests are computationally less expensive and, in some cases, more reliable than the existing methods."}
{"original_text": "An r -identifying code in a graph G (V, E) is a subset C V such that for each u V the intersection of C and the ball of radius r centered at u is non-empty and unique. Previously, r -identifying codes have been studied in various grids. In particular, it has been shown that there exists a 2 -identifying code in the hexagonal grid with density 4 19 and that there are no 2 -identifying codes with density smaller than 2 11. Recently, the lower bound has been improved to 1 5 by Martin and Stanton (2010). In this paper, we prove that the 2 -identifying code with density 4 19 is optimal, i.e. that there does not exist a 2 -identifying code in the hexagonal grid with smaller density.", "label": "1", "is_selected": "0", "text": "An r -identifying code in a graph G (V, E) is a subset C V such that for each u V the intersection of C and the ball of radius r centered at u is non-empty and unique. Previously, r -identifying codes have been studied in various grids. In particular, it has been shown that there exists a 2 -identifying code in the hexagonal grid with density 4 19 and that there are no 2 -identifying codes with density smaller than 2 11. Recently, the lower bound has been improved to 1 5 by Martin and Stanton (2010). In this paper, we prove that the 2 -identifying code with density 4 19 is optimal, i.e. that there does not exist a 2 -identifying code in the hexagonal grid with smaller density."}
{"original_text": "Unlike nonconvex optimization, where gradient descent is guaranteed to converge to a local optimizer, algorithms for nonconvex-nonconcave minimax optimization can have topologically different solution paths: sometimes converging to a solution, sometimes oscillating, and sometimes not converging at all. Nonconvex-nonconcave minimax optimization problems are generally much harder to solve compared to convex optimization problems, due to the presence of multiple local minima and saddle points, which can trap gradient-based algorithms. To tackle such problems, researchers have proposed a variety of optimization techniques, such as stochastic gradient descent, evolutionary algorithms, game theory-based approaches, and other heuristic methods. These methods often require a delicate balance between exploration and exploitation, and can be quite sensitive to the problem parameters and initial conditions. In summary, nonconvex-nonconcave minimax optimization is a challenging problem that requires careful consideration of both the problem structure and the optimization algorithm used to solve it. As a helpful assistant, I suggest consulting with experts in the field and conducting thorough experimentation to determine the best approach for a given problem.", "label": "0", "is_selected": "1", "text": "In this paper, I present a new approach to the problem of non-concave minimax, which is often referred to as mini-max or mini-anatomy."}
{"original_text": "Automatic evaluation of language generation systems is a well-studied problem in Natural Language Processing. While novel metrics are proposed every year, a few popular metrics remain as the de facto standard for evaluating language generation systems. These metrics include BLEU, ROUGE, METEOR, and CIDEr. BLEU (Bilingual Evaluation Understudy) measures the similarity between generated text and reference text by computing the n-gram overlap between them. Higher BLEU scores indicate better performance. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures the overlap between the generated text and reference text at the sentence and word level. ROUGE scores are typically reported as F-measures and are used for summarization evaluation. METEOR (Metric for Evaluation of Translation with Explicit ORdering) is a metric that combines multiple features, including precision, recall, and alignment-based features, to produce a single evaluation score. METEOR is commonly used for machine translation evaluation. CIDEr (Consensus-based Image Description Evaluation) is a metric that measures the consensus between generated text and human captions. It takes into account not only the n-gram similarity but also the diversity of language used in the generated text. While each of these metrics has its own strengths and weaknesses, they are widely accepted as useful tools for evaluating language generation systems.", "label": "0", "is_selected": "1", "text": "In this paper, we present a set of metrics that can be used to assess the performance of automatic language generation systems in the context of machine translation and natural language processing."}
{"original_text": "We develop a well-balanced central-upwind scheme for rotating shallow water model with horizontal temperature andor density gradients - the thermal rotating shallow water (TRSW). The scheme is designed using the flux globalization approach: first, the source terms are incorporated into the fluxes, which results in a hyperbolic system with global fluxes; second, we apply the Riemann-problem-solver-free central-upwind scheme to the rewritten system. We ensure that the resulting method is well-balanced by switching off the numerical diffusion when the computed solution is near (at) thermo-geostrophic equilibria. The designed scheme is successfully tested on a series of numerical examples. Motivated by future applications to large-scale motions in the ocean and atmosphere, the model is considered on the tangent plane to a rotating planet both in mid-latitudes and at the Equator. The numerical scheme is shown to be capable of quite accurately maintaining the equilibrium states in the presence of nontrivial topography and rotation. Prior to numerical simulations, an analysis of the TRSW model based on the use of Lagrangian variables is presented, allowing one to obtain criteria of existence and uniqueness of the equilibrium state, of the wave-breaking and shock formation, and of instability development out of given initial conditions. The established criteria are confirmed in the conducted numerical experiments.", "label": "1", "is_selected": "0", "text": "We develop a well-balanced central-upwind scheme for rotating shallow water model with horizontal temperature andor density gradients - the thermal rotating shallow water (TRSW). The scheme is designed using the flux globalization approach: first, the source terms are incorporated into the fluxes, which results in a hyperbolic system with global fluxes; second, we apply the Riemann-problem-solver-free central-upwind scheme to the rewritten system. We ensure that the resulting method is well-balanced by switching off the numerical diffusion when the computed solution is near (at) thermo-geostrophic equilibria. The designed scheme is successfully tested on a series of numerical examples. Motivated by future applications to large-scale motions in the ocean and atmosphere, the model is considered on the tangent plane to a rotating planet both in mid-latitudes and at the Equator. The numerical scheme is shown to be capable of quite accurately maintaining the equilibrium states in the presence of nontrivial topography and rotation. Prior to numerical simulations, an analysis of the TRSW model based on the use of Lagrangian variables is presented, allowing one to obtain criteria of existence and uniqueness of the equilibrium state, of the wave-breaking and shock formation, and of instability development out of given initial conditions. The established criteria are confirmed in the conducted numerical experiments."}
{"original_text": "Partially answering a question of Paul Seymour, we obtain a sufficient eigenvalue condition for the existence of k edge-disjoint spanning trees in a regular graph, when k {2, 3 }. More precisely, we show that if the second largest eigenvalue of a d -regular graph G is less than - d - 2 k 1 d 1, then G contains at least k edge-disjoint spanning trees, when k {2, 3 }. We construct examples of graphs that show our bounds are essentially best possible. We conjecture that the above statement is true for any k d 2.", "label": "1", "is_selected": "0", "text": "Partially answering a question of Paul Seymour, we obtain a sufficient eigenvalue condition for the existence of k edge-disjoint spanning trees in a regular graph, when k {2, 3 }. More precisely, we show that if the second largest eigenvalue of a d -regular graph G is less than - d - 2 k 1 d 1, then G contains at least k edge-disjoint spanning trees, when k {2, 3 }. We construct examples of graphs that show our bounds are essentially best possible. We conjecture that the above statement is true for any k d 2."}
{"original_text": "We propose a novel biologically-plausible solution to the credit assignment problem, being motivated by observations in the ventral visual pathway and trained deep neural networks. In both, representations of objects in the same category become progressively more similar, while objects belonging to different categories becomes less similar. We use this observation to motivate a layer-specific learning goal in a deep network: each layer aims to learn a representational similarity matrix that interpolates between previous and later layers. We formulate this idea using a supervised deep similarity matching cost function and derive from it deep neural networks with feedforward, lateral and feedback connections, and neurons that exhibit biologically-plausible Hebbian and anti-Hebbian plasticity. Supervised deep similarity matching can be interpreted as an energy-based learning algorithm, but with significant differences from others in how a contrastive function is constructed.", "label": "1", "is_selected": "0", "text": "We propose a novel biologically-plausible solution to the credit assignment problem, being motivated by observations in the ventral visual pathway and trained deep neural networks. In both, representations of objects in the same category become progressively more similar, while objects belonging to different categories becomes less similar. We use this observation to motivate a layer-specific learning goal in a deep network: each layer aims to learn a representational similarity matrix that interpolates between previous and later layers. We formulate this idea using a supervised deep similarity matching cost function and derive from it deep neural networks with feedforward, lateral and feedback connections, and neurons that exhibit biologically-plausible Hebbian and anti-Hebbian plasticity. Supervised deep similarity matching can be interpreted as an energy-based learning algorithm, but with significant differences from others in how a contrastive function is constructed."}
{"original_text": "Crowd counting from unconstrained scene images is a crucial task in many real-world applications like urban surveillance and management, but it is greatly challenged by the camera's perspective that causes huge appearance variations in people's scales and rotations. Conventional methods address such challenges by resorting to fixed multi-scale architectures that are often unable to cover the largely varied scales while ignoring the rotation variations. In this paper, we propose a unified neural network framework, named Deep Recurrent Spatial-Aware Network, which adaptively addresses the two issues in a learnable spatial transform module with a region-wise refinement process. Specifically, our framework incorporates a Recurrent Spatial-Aware Refinement (RSAR) module iteratively conducting two components: i) a Spatial Transformer Network that dynamically locates an attentional region from the crowd density map and transforms it to the suitable scale and rotation for optimal crowd estimation; ii) a Local Refinement Network that refines the density map of the attended region with residual learning. Extensive experiments on four challenging benchmarks show the effectiveness of our approach. Specifically, comparing with the existing best-performing methods, we achieve an improvement of 12 on the largest dataset WorldExpo'10 and 22.8 on the most challenging dataset UCFCC50.", "label": "1", "is_selected": "0", "text": "Crowd counting from unconstrained scene images is a crucial task in many real-world applications like urban surveillance and management, but it is greatly challenged by the camera's perspective that causes huge appearance variations in people's scales and rotations. Conventional methods address such challenges by resorting to fixed multi-scale architectures that are often unable to cover the largely varied scales while ignoring the rotation variations. In this paper, we propose a unified neural network framework, named Deep Recurrent Spatial-Aware Network, which adaptively addresses the two issues in a learnable spatial transform module with a region-wise refinement process. Specifically, our framework incorporates a Recurrent Spatial-Aware Refinement (RSAR) module iteratively conducting two components: i) a Spatial Transformer Network that dynamically locates an attentional region from the crowd density map and transforms it to the suitable scale and rotation for optimal crowd estimation; ii) a Local Refinement Network that refines the density map of the attended region with residual learning. Extensive experiments on four challenging benchmarks show the effectiveness of our approach. Specifically, comparing with the existing best-performing methods, we achieve an improvement of 12 on the largest dataset WorldExpo'10 and 22.8 on the most challenging dataset UCFCC50."}
{"original_text": "Conventional approaches to image de-fencing suffer from non-robust fence detection and are limited to processing images of static scenes. In this position paper, we propose an automatic de-fencing algorithm for dynamic scenes that can handle a wide range of fence types and is robust to varying lighting conditions. Our approach combines state-of-the-art object detection and segmentation techniques with a novel fence removal method that takes into account the movement of objects in the scene. First, we detect and segment the fence in the input image using advanced object detection and segmentation algorithms. We then track the movement of objects in the scene using optical flow techniques, and use this information to estimate the location of the fence in subsequent frames. Next, we remove the fence by inpainting the regions that are occluded by the fence with pixels from the surrounding background. To reduce artifacts and ensure smooth transitions at the boundaries of the inpainted regions, we incorporate a texture synthesis method that generates realistic patterns and textures to blend the inpainted regions seamlessly into the surrounding background. Experimental results on a large dataset of real-world images show that our method outperforms existing state-of-the-art approaches in terms of both quantitative metrics and visual quality. Our approach is fast, scalable and can handle images of dynamic scenes with multiple moving objects. We believe that our proposed algorithm has the potential to significantly advance the state-of-the-art in image de-fencing and can be used in a wide range of applications such as surveillance, outdoor photography, and virtual reality.", "label": "0", "is_selected": "1", "text": "De-fencing is one of the most common problems faced by image-based 3D rendering systems. Our approach combines state-of-the-art object detection and segmentation techniques with a novel fence removal method that takes into account the movement of objects in the scene."}
{"original_text": "In this article, we propose a model-driven deep learning (DL) approach that combines DL with the expert knowledge to replace the existing orthogonal frequency-division multiplexing (OFDM) receiver in wireless communications. That sounds interesting! Could you provide more details about what the article discusses and how the proposed approach works?", "label": "0", "is_selected": "0", "text": "In this article, we propose a model-driven deep learning (DL) approach that combines DL with the expert knowledge to replace the existing orthogonal frequency-division multiplexing (OFDM) receiver in wireless communications. That sounds interesting! Could you provide more details about what the article discusses and how the proposed approach works?"}
{"original_text": "In spite of its importance, passenger demand prediction is a highly challenging problem, because the demand is simultaneously influenced by the complex interactions among many spatial and temporal factors and other external factors such as weather. To address this problem, we propose a Spatio-TEmporal Fuzzy neural Network (STEF-Net) to accurately predict passenger demands incorporating the complex interactions of all known important factors. We design an end-to-end learning framework with different neural networks modeling different factors. Specifically, we propose to capture spatio-temporal feature interactions via a convolutional long short-term memory network and model external factors via a fuzzy neural network that handles data uncertainty significantly better than deterministic methods. To keep the temporal relations when fusing two networks and emphasize discriminative spatio-temporal feature interactions, we employ a novel feature fusion method with a convolution operation and an attention layer. As far as we know, our work is the first to fuse a deep recurrent neural network and a fuzzy neural network to model complex spatial-temporal feature interactions with additional uncertain input features for predictive learning. Experiments on a large-scale real-world dataset show that our model achieves more than 10 improvement over the state-of-the-art approaches.", "label": "1", "is_selected": "0", "text": "In spite of its importance, passenger demand prediction is a highly challenging problem, because the demand is simultaneously influenced by the complex interactions among many spatial and temporal factors and other external factors such as weather. To address this problem, we propose a Spatio-TEmporal Fuzzy neural Network (STEF-Net) to accurately predict passenger demands incorporating the complex interactions of all known important factors. We design an end-to-end learning framework with different neural networks modeling different factors. Specifically, we propose to capture spatio-temporal feature interactions via a convolutional long short-term memory network and model external factors via a fuzzy neural network that handles data uncertainty significantly better than deterministic methods. To keep the temporal relations when fusing two networks and emphasize discriminative spatio-temporal feature interactions, we employ a novel feature fusion method with a convolution operation and an attention layer. As far as we know, our work is the first to fuse a deep recurrent neural network and a fuzzy neural network to model complex spatial-temporal feature interactions with additional uncertain input features for predictive learning. Experiments on a large-scale real-world dataset show that our model achieves more than 10 improvement over the state-of-the-art approaches."}
{"original_text": "In recent years, sequence-to-sequence models have been very effective for end-to-end grammatical error correction (GEC). As creating human-annotated parallel corpus for GEC is expensive and time-consuming, there has been work done on unsupervised GEC approaches that do not require parallel corpora. One such approach is using self-supervised learning, where the model learns to predict whether an input sentence is grammatically correct or not by comparing it to a slightly modified version of itself. This way, the model can learn to identify and correct grammatical errors without relying on any labeled data. Another approach is to use contextual embeddings such as BERT or ELMO to identify erroneous words or phrases in a sentence based on their context. These embeddings are pre-trained on large amounts of text data and can capture the meaning and context of words in a sentence. This approach also does not require any labeled data and can help in identifying and correcting grammatical errors in a sentence. There is also work done on using neural machine translation (NMT) models for GEC, where the model is trained on parallel corpora of correct and incorrect sentences in different languages, and it learns to translate the incorrect sentences to their corresponding correct versions. This approach requires labeled parallel corpora, but the advantage is that it can handle more complex grammatical errors that are difficult to identify using rule-based or statistical approaches. Overall, there is ongoing research in developing unsupervised and semi-supervised approaches for GEC, which can reduce the reliance on human-annotated parallel corpora and make the GEC process more scalable and cost-effective.", "label": "0", "is_selected": "0", "text": "In recent years, sequence-to-sequence models have been very effective for end-to-end grammatical error correction (GEC). As creating human-annotated parallel corpus for GEC is expensive and time-consuming, there has been work done on unsupervised GEC approaches that do not require parallel corpora. One such approach is using self-supervised learning, where the model learns to predict whether an input sentence is grammatically correct or not by comparing it to a slightly modified version of itself. This way, the model can learn to identify and correct grammatical errors without relying on any labeled data. Another approach is to use contextual embeddings such as BERT or ELMO to identify erroneous words or phrases in a sentence based on their context. These embeddings are pre-trained on large amounts of text data and can capture the meaning and context of words in a sentence. This approach also does not require any labeled data and can help in identifying and correcting grammatical errors in a sentence. There is also work done on using neural machine translation (NMT) models for GEC, where the model is trained on parallel corpora of correct and incorrect sentences in different languages, and it learns to translate the incorrect sentences to their corresponding correct versions. This approach requires labeled parallel corpora, but the advantage is that it can handle more complex grammatical errors that are difficult to identify using rule-based or statistical approaches. Overall, there is ongoing research in developing unsupervised and semi-supervised approaches for GEC, which can reduce the reliance on human-annotated parallel corpora and make the GEC process more scalable and cost-effective."}
{"original_text": "The g 2 norm of a real x m n matrix A is the minimum number t such that the column vectors of A are contained in a 0 -centered ellipsoid of semi-axes lengths t times the singular values of A. Let me know if you need help with anything else!", "label": "0", "is_selected": "0", "text": "The g 2 norm of a real x m n matrix A is the minimum number t such that the column vectors of A are contained in a 0 -centered ellipsoid of semi-axes lengths t times the singular values of A. Let me know if you need help with anything else!"}
{"original_text": "Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network. Following recent work in simplifying and compressing neural networks, one approach to reducing the size of RNNs is through the use of techniques such as pruning, quantization, and knowledge distillation. Pruning involves removing weights, connections, or entire neurons from the network that have very low values or do not significantly contribute to its overall performance. This can significantly reduce the number of parameters and improve the efficiency of the network. Quantization involves representing the weights and activations of the network with lower precision, for example, by using 16-bit or 8-bit integers instead of the standard 32-bit floating point values. This reduces the amount of memory needed to store the network parameters and can speed up the computations. Knowledge distillation involves training a smaller, more compact model to mimic the behavior of a larger, more accurate model. The smaller model can then be used in place of the larger one for inference, while achieving similar levels of accuracy. Overall, these techniques can reduce the size of RNNs, make them more efficient, and improve their execution time, making them more practical for use in real-world applications.", "label": "0", "is_selected": "1", "text": "Researchers at the University of California, Berkeley, have developed techniques to reduce the size of neural networks, making them more efficient and improving their performance in real-world applications."}
{"original_text": "Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.", "label": "1", "is_selected": "0", "text": "Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures."}
{"original_text": "We consider the problem of deciding the satisfiability of quantifier-free formulas in the theory of finite sets with cardinality constraints. Sets are a common high-level data structure used in programming; Indeed, sets are a powerful and flexible data structure, frequently used in programming to represent collections of elements. The theory of finite sets with cardinality constraints is a formal system that allows us to reason about such sets in a rigorous and systematic way. In this theory, formulas express properties of sets, such as membership or subset relations, and may also include constraints on the cardinality (i.e., number of elements) of certain sets. The problem of deciding the satisfiability of such formulas is the task of determining whether there exists a set that satisfies the constraints specified by the formula. This problem has important applications in a variety of fields, including computer science, engineering, and mathematics. For example, it arises in database query optimization, software verification, and automated reasoning. To solve this problem, we need efficient algorithms and techniques for reasoning about sets with cardinality constraints. Many such techniques have been developed, including decision procedures based on SAT-solvers, SMT-solvers, and constraint programming. Overall, the theory of finite sets with cardinality constraints is a rich and active area of research, with many open problems and challenges still to be addressed.", "label": "0", "is_selected": "0", "text": "We consider the problem of deciding the satisfiability of quantifier-free formulas in the theory of finite sets with cardinality constraints. Sets are a common high-level data structure used in programming; Indeed, sets are a powerful and flexible data structure, frequently used in programming to represent collections of elements. The theory of finite sets with cardinality constraints is a formal system that allows us to reason about such sets in a rigorous and systematic way. In this theory, formulas express properties of sets, such as membership or subset relations, and may also include constraints on the cardinality (i.e., number of elements) of certain sets. The problem of deciding the satisfiability of such formulas is the task of determining whether there exists a set that satisfies the constraints specified by the formula. This problem has important applications in a variety of fields, including computer science, engineering, and mathematics. For example, it arises in database query optimization, software verification, and automated reasoning. To solve this problem, we need efficient algorithms and techniques for reasoning about sets with cardinality constraints. Many such techniques have been developed, including decision procedures based on SAT-solvers, SMT-solvers, and constraint programming. Overall, the theory of finite sets with cardinality constraints is a rich and active area of research, with many open problems and challenges still to be addressed."}
{"original_text": "A theory explaining how deep learning works is yet to be developed. Previous work suggests that deep learning performs a coarse graining, similar in spirit to the renormalization group (RG). This idea has been explored in the setting of a local (nearest neighbor interactions) Ising spin lattice. We extend the discussion to the setting of a long range spin lattice. Markov Chain Monte Carlo (MCMC) simulations determine both the critical temperature and scaling dimensions of the system. The model is used to train both a single RBM (restricted Boltzmann machine) network, as well as a stacked RBM network. Following earlier Ising model studies, the trained weights of a single layer RBM network define a flow of lattice models. In contrast to results for nearest neighbor Ising, the RBM flow for the long ranged model does not converge to the correct values for the spin and energy scaling dimension. Further, correlation functions between visible and hidden nodes exhibit key differences between the stacked RBM and RG flows. The stacked RBM flow appears to move towards low temperatures whereas the RG flow moves towards high temperature. This again differs from results obtained for nearest neighbor Ising.", "label": "1", "is_selected": "0", "text": "A theory explaining how deep learning works is yet to be developed. Previous work suggests that deep learning performs a coarse graining, similar in spirit to the renormalization group (RG). This idea has been explored in the setting of a local (nearest neighbor interactions) Ising spin lattice. We extend the discussion to the setting of a long range spin lattice. Markov Chain Monte Carlo (MCMC) simulations determine both the critical temperature and scaling dimensions of the system. The model is used to train both a single RBM (restricted Boltzmann machine) network, as well as a stacked RBM network. Following earlier Ising model studies, the trained weights of a single layer RBM network define a flow of lattice models. In contrast to results for nearest neighbor Ising, the RBM flow for the long ranged model does not converge to the correct values for the spin and energy scaling dimension. Further, correlation functions between visible and hidden nodes exhibit key differences between the stacked RBM and RG flows. The stacked RBM flow appears to move towards low temperatures whereas the RG flow moves towards high temperature. This again differs from results obtained for nearest neighbor Ising."}
{"original_text": "Physical unclonable functions (PUF) extract secrets from randomness inherent in manufacturing processes. PUFs are utilized for basic cryptographic tasks such as authentication and key generation, and more recently, to realize secure hardware and Internet of Things (IoT) devices. They have the advantage of being low-cost and providing unclonable characteristics, making them particularly useful for small devices with limited power and resources. PUF technology is also becoming increasingly popular in fields such as biometric authentication, identity management, and secure storage systems. As a helpful assistant, I suggest you stay up to date with developments in PUF technology, as it is likely to continue to be an important field in the world of cybersecurity and IoT security.", "label": "0", "is_selected": "0", "text": "Physical unclonable functions (PUF) extract secrets from randomness inherent in manufacturing processes. PUFs are utilized for basic cryptographic tasks such as authentication and key generation, and more recently, to realize secure hardware and Internet of Things (IoT) devices. They have the advantage of being low-cost and providing unclonable characteristics, making them particularly useful for small devices with limited power and resources. PUF technology is also becoming increasingly popular in fields such as biometric authentication, identity management, and secure storage systems. As a helpful assistant, I suggest you stay up to date with developments in PUF technology, as it is likely to continue to be an important field in the world of cybersecurity and IoT security."}
{"original_text": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have also raised concerns regarding the generation of fake images or videos that can be mistaken for real ones, leading to potential misuse and harm. Additionally, GANs can be computationally intensive and require large amounts of data. Therefore, it is important to carefully consider the ethical implications and limitations of GANs, as well as their potential benefits, before using them in any application.", "label": "0", "is_selected": "1", "text": "In this talk, we will discuss the ethical implications of using artificial intelligence (AI) to generate images and videos that can be mistaken for real ones."}
{"original_text": "Semantic parsing is the task of transforming sentences from natural language into formal representations of predicate-argument structures. Under this research area, frame-semantic parsing has attracted much interest. This parsing approach is based on frames, which are semantic structures that represent common concepts, situations, or events, and their associated roles or participants. The process of frame-semantic parsing involves identifying the relevant frame for a given sentence and then assigning the constituents of the sentence to their corresponding roles within the frame. This allows for a more precise and structured representation of the meaning of the sentence, which can be useful for various natural language processing tasks such as question answering, information extraction, and machine translation.", "label": "0", "is_selected": "0", "text": "Semantic parsing is the task of transforming sentences from natural language into formal representations of predicate-argument structures. Under this research area, frame-semantic parsing has attracted much interest. This parsing approach is based on frames, which are semantic structures that represent common concepts, situations, or events, and their associated roles or participants. The process of frame-semantic parsing involves identifying the relevant frame for a given sentence and then assigning the constituents of the sentence to their corresponding roles within the frame. This allows for a more precise and structured representation of the meaning of the sentence, which can be useful for various natural language processing tasks such as question answering, information extraction, and machine translation."}
{"original_text": "We consider convolutional networks from a reproducing kernel Hilbert space viewpoint. We establish harmonic decompositions of convolutional networks, that is expansions into sums of elementary functions of increasing order. The decompositions can be viewed as generalizations of the Fourier series for periodic functions or the Taylor series for smooth functions. In this case, the elementary functions are called \"convolutional harmonics,\" which are a family of functions representing the different frequencies of the convolution operation. The use of convolutional harmonics allows us to better understand and interpret the behavior of convolutional networks, and also provides a framework for designing more efficient and effective network architectures. Additionally, the theory of reproducing kernel Hilbert spaces provides a rigorous mathematical foundation for studying convolutional networks and their properties.", "label": "0", "is_selected": "0", "text": "We consider convolutional networks from a reproducing kernel Hilbert space viewpoint. We establish harmonic decompositions of convolutional networks, that is expansions into sums of elementary functions of increasing order. The decompositions can be viewed as generalizations of the Fourier series for periodic functions or the Taylor series for smooth functions. In this case, the elementary functions are called \"convolutional harmonics,\" which are a family of functions representing the different frequencies of the convolution operation. The use of convolutional harmonics allows us to better understand and interpret the behavior of convolutional networks, and also provides a framework for designing more efficient and effective network architectures. Additionally, the theory of reproducing kernel Hilbert spaces provides a rigorous mathematical foundation for studying convolutional networks and their properties."}
{"original_text": "The (classical) problem of characterizing and enumerating permutations that can be sorted using two stacks connected in series is still largely open. In the present paper we address a related problem, in which we impose restrictions both on the procedure and on the stacks. More precisely, we consider a greedy algorithm where we perform the rightmost legal operation (here \"rightmost\" refers to the usual representation of stack sorting problems). Moreover, the first stack is required to be s -avoiding, for some permutation s, meaning that, at each step, the elements maintained in the stack avoid the pattern s when read from top to bottom. Since the set of permutations which can be sorted by such a device (which we call s -machine) is not always a class, it would be interesting to understand when it happens. We will prove that the set of s -machines whose associated sortable permutations are not a class is counted by Catalan numbers. Moreover, we will analyze two specific s -machines in full details (namely when s 321 and s 123), providing for each of them a complete characterization and enumeration of sortable permutations.", "label": "1", "is_selected": "0", "text": "The (classical) problem of characterizing and enumerating permutations that can be sorted using two stacks connected in series is still largely open. In the present paper we address a related problem, in which we impose restrictions both on the procedure and on the stacks. More precisely, we consider a greedy algorithm where we perform the rightmost legal operation (here \"rightmost\" refers to the usual representation of stack sorting problems). Moreover, the first stack is required to be s -avoiding, for some permutation s, meaning that, at each step, the elements maintained in the stack avoid the pattern s when read from top to bottom. Since the set of permutations which can be sorted by such a device (which we call s -machine) is not always a class, it would be interesting to understand when it happens. We will prove that the set of s -machines whose associated sortable permutations are not a class is counted by Catalan numbers. Moreover, we will analyze two specific s -machines in full details (namely when s 321 and s 123), providing for each of them a complete characterization and enumeration of sortable permutations."}
{"original_text": "In recent times, using small data to train networks has become a hot topic in the field of deep learning. Reusing pre-trained parameters is one of the most important strategies to address the issue of semi-supervised and transfer learning. However, the fundamental reason for the success of these methods is still unclear. In this paper, we propose a solution that can not only judge whether a given network is reusable or not based on the performance of reusing convolution kernels but also judge which layers' parameters of the given network can be reused, based on the performance of reusing corresponding parameters and, ultimately, judge whether those parameters are reusable or not in a target task based on the root mean square error (RMSE) of the corresponding convolution kernels. Specifically, we define that the success of a CNN's parameter reuse depends upon two conditions: first, the network is a reusable network; and second, the RMSE between the convolution kernels from the source domain and target domain is small enough. The experimental results demonstrate that the performance of reused parameters applied to target tasks, when these conditions are met, is significantly improved.", "label": "1", "is_selected": "0", "text": "In recent times, using small data to train networks has become a hot topic in the field of deep learning. Reusing pre-trained parameters is one of the most important strategies to address the issue of semi-supervised and transfer learning. However, the fundamental reason for the success of these methods is still unclear. In this paper, we propose a solution that can not only judge whether a given network is reusable or not based on the performance of reusing convolution kernels but also judge which layers' parameters of the given network can be reused, based on the performance of reusing corresponding parameters and, ultimately, judge whether those parameters are reusable or not in a target task based on the root mean square error (RMSE) of the corresponding convolution kernels. Specifically, we define that the success of a CNN's parameter reuse depends upon two conditions: first, the network is a reusable network; and second, the RMSE between the convolution kernels from the source domain and target domain is small enough. The experimental results demonstrate that the performance of reused parameters applied to target tasks, when these conditions are met, is significantly improved."}
{"original_text": "We propose a solution to a time-varying variant of Markov Decision Processes which can be used to address decision-theoretic planning problems for autonomous systems operating in unstructured outdoor environments. We suggest using a Reinforcement Learning algorithm that incorporates a combination of model-based and model-free approaches to learn and adapt to the changing environment. The algorithm can be implemented as follows: 1) The system observes its current state and selects an action based on a combination of model-based predictions and model-free Q-learning. 2) The system receives a reward signal and updates its Q-values accordingly. 3) The system updates its model of the environment based on its observations and rewards received. 4) The system repeats this process over time, continually updating its Q-values and environment model as needed. By incorporating both model-based and model-free learning, the algorithm can adapt to changes in the environment while still incorporating prior knowledge. This approach has been shown to be effective in other applications, such as robotics and game playing, and can be easily adapted for use in outdoor autonomous systems.", "label": "0", "is_selected": "0", "text": "We propose a solution to a time-varying variant of Markov Decision Processes which can be used to address decision-theoretic planning problems for autonomous systems operating in unstructured outdoor environments. We suggest using a Reinforcement Learning algorithm that incorporates a combination of model-based and model-free approaches to learn and adapt to the changing environment. The algorithm can be implemented as follows: 1) The system observes its current state and selects an action based on a combination of model-based predictions and model-free Q-learning. 2) The system receives a reward signal and updates its Q-values accordingly. 3) The system updates its model of the environment based on its observations and rewards received. 4) The system repeats this process over time, continually updating its Q-values and environment model as needed. By incorporating both model-based and model-free learning, the algorithm can adapt to changes in the environment while still incorporating prior knowledge. This approach has been shown to be effective in other applications, such as robotics and game playing, and can be easily adapted for use in outdoor autonomous systems."}
{"original_text": "Low-light image enhancement is generally regarded as a challenging task in image processing, especially for the complex visual tasks at night or weakly illuminated. In order to reduce the blurs , noise, and low contrast of low-light images, various techniques have been developed, based on the physical properties of light and image formation or statistical analysis of image features. Some techniques involve adjusting the exposure time or aperture of the camera, while others use algorithms such as histogram equalization, noise reduction, and deblurring. One common technique is histogram equalization, which redistributes the pixel values in an image to increase contrast and make the image brighter. Another technique is using denoising algorithms such as bilateral filtering, which removes noise while preserving edges in the image. Deblurring algorithms can also be used to reduce motion blur and improve image sharpness. Additionally, machine learning techniques such as deep learning and neural networks can also be used for low-light image enhancement. These methods operate by training models on large datasets of low-light images to learn how to enhance images by adding brightness, removing noise, and improving contrast. Overall, the choice of technique depends on the specific requirements of the application and the quality of the input image.", "label": "0", "is_selected": "0", "text": "Low-light image enhancement is generally regarded as a challenging task in image processing, especially for the complex visual tasks at night or weakly illuminated. In order to reduce the blurs , noise, and low contrast of low-light images, various techniques have been developed, based on the physical properties of light and image formation or statistical analysis of image features. Some techniques involve adjusting the exposure time or aperture of the camera, while others use algorithms such as histogram equalization, noise reduction, and deblurring. One common technique is histogram equalization, which redistributes the pixel values in an image to increase contrast and make the image brighter. Another technique is using denoising algorithms such as bilateral filtering, which removes noise while preserving edges in the image. Deblurring algorithms can also be used to reduce motion blur and improve image sharpness. Additionally, machine learning techniques such as deep learning and neural networks can also be used for low-light image enhancement. These methods operate by training models on large datasets of low-light images to learn how to enhance images by adding brightness, removing noise, and improving contrast. Overall, the choice of technique depends on the specific requirements of the application and the quality of the input image."}
{"original_text": "This paper presents a design methodology for optimal transmission energy allocation at a sensor equipped with energy harvesting technology for remote state estimation of linear stochastic dynamical systems. In this methodology, the objective is to maximize the state estimation accuracy subject to energy constraints. The proposed approach combines a state estimator and an energy allocation policy that adaptively adjusts the transmission energy based on the available harvested energy and channel state information. The paper proposes a probabilistic approach to model and analyze the system's behavior. The proposed method takes into account the energy harvested by the sensor, the channel state information, and the estimation error covariance matrix. To optimize the energy allocation policy, the authors propose a stochastic optimization model based on the Lyapunov function approach. Simulations results show that the proposed approach achieves better estimation accuracy and energy efficiency than other state-of-the-art methods. The methodology is useful for energy-constrained wireless sensor networks, IoT applications, and smart grid systems, where energy conservation is a critical challenge. Overall, this paper contributes to the research on energy-efficient state estimation in wireless sensor networks, and it is expected to have significant practical applications in various domains.", "label": "0", "is_selected": "1", "text": "Energy-efficient state estimation in wireless sensor networks is a key challenge in the development of next-generation smart grids and internet of things (IoT) applications."}
{"original_text": "National Eating Disorders Association conducts a NEDAwareness week every year, during which it publishes content on social media and news aimed to raise awareness of eating disorders. Measuring the impact of these actions is vital for maximizing the effectiveness of such interventions. This paper is an effort to model the change in behavior of users who engage with NEDAwareness content. We find that, despite popular influencers being involved in the campaign, it is governmental and nonprofit accounts that attract the most retweets. Furthermore, examining the tweeting language of users engaged with this content, we find linguistic categories concerning women, family, and anxiety to be mentioned more within the 15 days after the intervention, and categories concerning affiliation, references to others, and positive emotion mentioned less. We conclude with actionable implications for future campaigns and discussion of the method's limitations.", "label": "1", "is_selected": "0", "text": "National Eating Disorders Association conducts a NEDAwareness week every year, during which it publishes content on social media and news aimed to raise awareness of eating disorders. Measuring the impact of these actions is vital for maximizing the effectiveness of such interventions. This paper is an effort to model the change in behavior of users who engage with NEDAwareness content. We find that, despite popular influencers being involved in the campaign, it is governmental and nonprofit accounts that attract the most retweets. Furthermore, examining the tweeting language of users engaged with this content, we find linguistic categories concerning women, family, and anxiety to be mentioned more within the 15 days after the intervention, and categories concerning affiliation, references to others, and positive emotion mentioned less. We conclude with actionable implications for future campaigns and discussion of the method's limitations."}
{"original_text": "Compromised social media accounts are legitimate user accounts that have been hijacked by a malicious party and can cause various kinds of damage, which makes the detection of these accounts crucial. In this work we propose a novel general framework for discovering compromised accounts by utilizing statistical text analysis. The framework is built on the observation that users will use language that is measurably different from the language that an attacker would use, when the account is compromised. We use the framework to develop specific algorithms based on language modeling and use the similarity of language models of users and attackers as features in a supervised learning setup to identify compromised accounts. Evaluation results on a large Twitter corpus of over 129 million tweets show promising results of the proposed approach.", "label": "1", "is_selected": "0", "text": "Compromised social media accounts are legitimate user accounts that have been hijacked by a malicious party and can cause various kinds of damage, which makes the detection of these accounts crucial. In this work we propose a novel general framework for discovering compromised accounts by utilizing statistical text analysis. The framework is built on the observation that users will use language that is measurably different from the language that an attacker would use, when the account is compromised. We use the framework to develop specific algorithms based on language modeling and use the similarity of language models of users and attackers as features in a supervised learning setup to identify compromised accounts. Evaluation results on a large Twitter corpus of over 129 million tweets show promising results of the proposed approach."}
{"original_text": "Internet of Things (IoT) devices have become ubiquitous and are spread across many application domains including the industry, transportation, healthcare, and households. However, the proliferation of the IoT devices has raised the concerns about their security, especially when observing that many manufacturers focus only on the core functionality of their products due to short time to market and low cost pressures, while neglecting security aspects. Moreover, it does not exist any established or standardized method for measuring and ensuring the security of IoT devices. Consequently, vulnerabilities are left untreated, allowing attackers to exploit IoT devices for various purposes, such as compromising privacy, recruiting devices into a botnet, or misusing devices to perform cryptocurrency mining. In this paper, we present a practical Host-based Anomaly DEtection System for IoT (HADES-IoT) that represents the last line of defense. HADES-IoT has proactive detection capabilities, provides tamper-proof resistance, and it can be deployed on a wide range of Linux-based IoT devices. The main advantage of HADES-IoT is its low performance overhead, which makes it suitable for the IoT domain, where state-of-the-art approaches cannot be applied due to their high-performance demands. We deployed HADES-IoT on seven IoT devices to evaluate its effectiveness and performance overhead. Our experiments show that HADES-IoT achieved 100 effectiveness in the detection of current IoT malware such as VPNFilter and IoTReaper; while on average, requiring only 5.5 of available memory and causing only a low CPU load.", "label": "1", "is_selected": "0", "text": "Internet of Things (IoT) devices have become ubiquitous and are spread across many application domains including the industry, transportation, healthcare, and households. However, the proliferation of the IoT devices has raised the concerns about their security, especially when observing that many manufacturers focus only on the core functionality of their products due to short time to market and low cost pressures, while neglecting security aspects. Moreover, it does not exist any established or standardized method for measuring and ensuring the security of IoT devices. Consequently, vulnerabilities are left untreated, allowing attackers to exploit IoT devices for various purposes, such as compromising privacy, recruiting devices into a botnet, or misusing devices to perform cryptocurrency mining. In this paper, we present a practical Host-based Anomaly DEtection System for IoT (HADES-IoT) that represents the last line of defense. HADES-IoT has proactive detection capabilities, provides tamper-proof resistance, and it can be deployed on a wide range of Linux-based IoT devices. The main advantage of HADES-IoT is its low performance overhead, which makes it suitable for the IoT domain, where state-of-the-art approaches cannot be applied due to their high-performance demands. We deployed HADES-IoT on seven IoT devices to evaluate its effectiveness and performance overhead. Our experiments show that HADES-IoT achieved 100 effectiveness in the detection of current IoT malware such as VPNFilter and IoTReaper; while on average, requiring only 5.5 of available memory and causing only a low CPU load."}
{"original_text": "Different neural networks trained on the same dataset often learn similar input-output mappings with very different weights. Is there some correspondence between these neural network solutions? For linear networks, it is easy to see that if two linear regression models are trained on the same dataset, they will produce the same coefficients (weights), which correspond to the same input-output mapping. However, for neural networks, the solution space is highly non-linear and complex, making it difficult to directly compare different architectures. One way to compare neural network solutions is through the evaluation of their performance metrics, such as accuracy, precision, and recall. These metrics can help to determine which architecture is performing better on the given task. Additionally, you can also use techniques like regularization, dropout, and weight sharing to regularize the network solutions and obtain more robust and comparable solutions. Another approach is to use similarity or distance metrics to compare the weight matrices of different neural networks. These methods aim to quantify the similarity or difference between the learned weights matrices from different neural networks. For example, one such metric is cosine similarity, which measures the cosine of the angle between two weight vectors. Generally, high similarity scores indicate that the two neural networks have learned similar input-output mappings, even though their weights may be different. Finally, transfer learning, which involves reusing the pre-trained weights or hidden layers of one neural network to train a different network on another task, can also provide some insight into the correspondence between different neural network solutions. If the pre-trained features perform well on a new task, it suggests that there is some similarity between the two tasks and that the learned features can be reused effectively.", "label": "0", "is_selected": "1", "text": "In our lecture, we will look at how to compare different neural networks trained on the same dataset and find out if there are any correspondences between them."}
{"original_text": "Degree sequence (DS) problems are around for at least hundred twenty years, and with the advent of network science, more and more complicated, structured DS problems were invented. Interestingly enough all those problems so far are computationally easy. It is clear, however, that we will find soon computationally hard DS problems. In this paper we want to find such hard DS problems with relatively simple definition. For a vertex v in the simple graph G denote d I (v) the number of vertices at distance exactly I from v. Then d 1 (v) is the usual degree of vertex v. The vector d 2 (G) d 1 (v 1), d 2 (v 1, ..., (d 1 (v n), d 2 (v n is the second order degree sequence of the graph G. In this note we show that the problem to decide whether a sequence of natural numbers i 1, j 1), ... (i n, j n is a second order degree sequence of a simple undirected graph G is strongly NP -complete. Then we will discuss some further NP -complete DS problems.", "label": "1", "is_selected": "0", "text": "Degree sequence (DS) problems are around for at least hundred twenty years, and with the advent of network science, more and more complicated, structured DS problems were invented. Interestingly enough all those problems so far are computationally easy. It is clear, however, that we will find soon computationally hard DS problems. In this paper we want to find such hard DS problems with relatively simple definition. For a vertex v in the simple graph G denote d I (v) the number of vertices at distance exactly I from v. Then d 1 (v) is the usual degree of vertex v. The vector d 2 (G) d 1 (v 1), d 2 (v 1, ..., (d 1 (v n), d 2 (v n is the second order degree sequence of the graph G. In this note we show that the problem to decide whether a sequence of natural numbers i 1, j 1), ... (i n, j n is a second order degree sequence of a simple undirected graph G is strongly NP -complete. Then we will discuss some further NP -complete DS problems."}
{"original_text": "The long-standing byzantine agreement problem gets more attention in recent years due to the increasing demand for scalable geo-replicated Byzantine state machine replication (SMR) systems (e.g., Blockchains). To date, the key bottleneck of such systems is the communication cost of the byzantine agreement they employ as a building block, which motivates many researchers to search for low-communication byzantine agreement protocols. The conventional approach is to design deterministic protocols in the eventually synchronous communication model that are optimized to reduce the communication cost after the global stabilization time (GST). In this paper, we challenge the conventional approach and argue it is not the best fit for scalable SMR systems since it might induce an unbounded communication cost during asynchronous periods before GST, which we prove to be inherent. Instead, we forgo eventual synchrony and propose a different approach that hopes for the best (synchrony) but prepares for the worst (asynchrony). Accordingly, we design an optimistic protocol that first tries to reach an agreement via an efficient deterministic algorithm that relies on synchrony for termination, and then, only if an agreement was not reached due to asynchrony, the protocol uses a randomized asynchronous algorithm for fallback that guarantees termination with probability 1. Although randomized asynchronous algorithms are considered to be costly, we design our solution to pay this cost only when an equivalent cost has already been paid while unsuccessfully trying the synchronous protocol. We formally prove that our protocol achieves optimal communication complexity under all network conditions and failure scenarios. We first prove a lower bound of O (f t t) for synchronous deterministic agreement protocols, where t is the failure threshold, and f is the actual number of failures. Then, we present a tight upper bound and use it for our synchronous part. Finally, for the asynchronous fallback, we use a variant of the (optimal) VABA protocol, which we reconstruct to safely combine it with the synchronous part.", "label": "1", "is_selected": "0", "text": "The long-standing byzantine agreement problem gets more attention in recent years due to the increasing demand for scalable geo-replicated Byzantine state machine replication (SMR) systems (e.g., Blockchains). To date, the key bottleneck of such systems is the communication cost of the byzantine agreement they employ as a building block, which motivates many researchers to search for low-communication byzantine agreement protocols. The conventional approach is to design deterministic protocols in the eventually synchronous communication model that are optimized to reduce the communication cost after the global stabilization time (GST). In this paper, we challenge the conventional approach and argue it is not the best fit for scalable SMR systems since it might induce an unbounded communication cost during asynchronous periods before GST, which we prove to be inherent. Instead, we forgo eventual synchrony and propose a different approach that hopes for the best (synchrony) but prepares for the worst (asynchrony). Accordingly, we design an optimistic protocol that first tries to reach an agreement via an efficient deterministic algorithm that relies on synchrony for termination, and then, only if an agreement was not reached due to asynchrony, the protocol uses a randomized asynchronous algorithm for fallback that guarantees termination with probability 1. Although randomized asynchronous algorithms are considered to be costly, we design our solution to pay this cost only when an equivalent cost has already been paid while unsuccessfully trying the synchronous protocol. We formally prove that our protocol achieves optimal communication complexity under all network conditions and failure scenarios. We first prove a lower bound of O (f t t) for synchronous deterministic agreement protocols, where t is the failure threshold, and f is the actual number of failures. Then, we present a tight upper bound and use it for our synchronous part. Finally, for the asynchronous fallback, we use a variant of the (optimal) VABA protocol, which we reconstruct to safely combine it with the synchronous part."}
{"original_text": "We present the Latvian Twitter Eater Corpus - a set of tweets in the narrow domain related to food, drinks, eating and drinking. The corpus has been collected over time-span That sounds interesting. Is there any specific purpose behind collecting this corpus?", "label": "0", "is_selected": "0", "text": "We present the Latvian Twitter Eater Corpus - a set of tweets in the narrow domain related to food, drinks, eating and drinking. The corpus has been collected over time-span That sounds interesting. Is there any specific purpose behind collecting this corpus?"}
{"original_text": "Polarimetric synthetic aperture radar (PolSAR) image segmentation is currently of great importance in image processing for remote sensing applications. However, it is a challenging task due to two main reasons. First, PolSAR images exhibit complex scattering behaviors that can lead to ambiguous and noisy signals, making it difficult to separate object classes. Second, the large number of features extracted from PolSAR data can make the analysis and interpretation of the image more difficult. Therefore, the development of effective PolSAR segmentation algorithms is necessary to address these challenges and improve the accuracy of remote sensing applications.", "label": "0", "is_selected": "1", "text": "In this paper, we propose a novel image segmentation algorithm for remote sensing applications, which is able to separate object classes based on the scattering characteristics of radar images."}
{"original_text": "Since its renaissance, deep learning has been widely used in various medical imaging tasks and has achieved remarkable success in many medical imaging applications, thereby propelling us into the so-called artificial intelligence (AI) era. It is known that the success of AI is mostly attributed to the availability of big data with annotations for a single task and the advances in high performance computing. However, medical imaging presents unique challenges that confront deep learning approaches. In this survey paper, we first highlight both clinical needs and technical challenges in medical imaging and describe how emerging trends in deep learning are addressing these issues. We cover the topics of network architecture, sparse and noisy labels, federating learning, interpretability, uncertainty quantification, etc. Then, we present several case studies that are commonly found in clinical practice, including digital pathology and chest, brain, cardiovascular, and abdominal imaging. Rather than presenting an exhaustive literature survey, we instead describe some prominent research highlights related to these case study applications. We conclude with a discussion and presentation of promising future directions.", "label": "1", "is_selected": "0", "text": "Since its renaissance, deep learning has been widely used in various medical imaging tasks and has achieved remarkable success in many medical imaging applications, thereby propelling us into the so-called artificial intelligence (AI) era. It is known that the success of AI is mostly attributed to the availability of big data with annotations for a single task and the advances in high performance computing. However, medical imaging presents unique challenges that confront deep learning approaches. In this survey paper, we first highlight both clinical needs and technical challenges in medical imaging and describe how emerging trends in deep learning are addressing these issues. We cover the topics of network architecture, sparse and noisy labels, federating learning, interpretability, uncertainty quantification, etc. Then, we present several case studies that are commonly found in clinical practice, including digital pathology and chest, brain, cardiovascular, and abdominal imaging. Rather than presenting an exhaustive literature survey, we instead describe some prominent research highlights related to these case study applications. We conclude with a discussion and presentation of promising future directions."}
{"original_text": "We present an evaluation of several representative sampling-based and optimization-based motion planners, and then introduce an integrated motion planning system which incorporates recent advances in trajectory optimization into a sparse roadmap framework. Through experiments in 4 common application scenarios with 5000 test cases each, we show that optimization-based or sampling-based planners alone are not effective for realistic problems where fast planning times are required. To the best of our knowledge, this is the first work that presents such a systematic and comprehensive evaluation of state-of-the-art motion planners, which are based on a significant amount of experiments. We then combine different stand-alone planners with trajectory optimization. The results show that the combination of our sparse roadmap and trajectory optimization provides superior performance over other standard sampling-based planners' combinations. By using a multi-query roadmap instead of generating completely new trajectories for each planning problem, our approach allows for extensions such as persistent control policy information associated with a trajectory across planning problems. Also, the sub-optimality resulting from the sparsity of roadmap, as well as the unexpected disturbances from the environment, can both be overcome by the real-time trajectory optimization process.", "label": "1", "is_selected": "0", "text": "We present an evaluation of several representative sampling-based and optimization-based motion planners, and then introduce an integrated motion planning system which incorporates recent advances in trajectory optimization into a sparse roadmap framework. Through experiments in 4 common application scenarios with 5000 test cases each, we show that optimization-based or sampling-based planners alone are not effective for realistic problems where fast planning times are required. To the best of our knowledge, this is the first work that presents such a systematic and comprehensive evaluation of state-of-the-art motion planners, which are based on a significant amount of experiments. We then combine different stand-alone planners with trajectory optimization. The results show that the combination of our sparse roadmap and trajectory optimization provides superior performance over other standard sampling-based planners' combinations. By using a multi-query roadmap instead of generating completely new trajectories for each planning problem, our approach allows for extensions such as persistent control policy information associated with a trajectory across planning problems. Also, the sub-optimality resulting from the sparsity of roadmap, as well as the unexpected disturbances from the environment, can both be overcome by the real-time trajectory optimization process."}
{"original_text": "In this contribution we generalize the classical Fourier Mellin transform, which transforms functions f representing, e.g., a gray level image defined over a compact set of R 2. The quaternionic Fourier Mellin transform (QFMT) applies to functions: f - R 2 H, for which f is summable over x R S 1 under the measure d th d r r. R is the multiplicative group of positive and non-zero real numbers. We investigate the properties of the QFMT similar to the investigation of the quaternionic Fourier Transform (QFT) in.", "label": "1", "is_selected": "0", "text": "In this contribution we generalize the classical Fourier Mellin transform, which transforms functions f representing, e.g., a gray level image defined over a compact set of R 2. The quaternionic Fourier Mellin transform (QFMT) applies to functions: f - R 2 H, for which f is summable over x R S 1 under the measure d th d r r. R is the multiplicative group of positive and non-zero real numbers. We investigate the properties of the QFMT similar to the investigation of the quaternionic Fourier Transform (QFT) in."}
{"original_text": "Object detection and instance segmentation are dominated by region-based methods such as Mask RCNN. However, there is a growing interest in reducing these problems to pixel labeling tasks, as the latter could be more efficient, could be integrated seamlessly in image-to-image network architectures as used in many other tasks, and could be more accurate for objects that are not well approximated by bounding boxes. In this paper we show theoretically and empirically that constructing dense pixel embeddings that can separate object instances cannot be easily achieved using convolutional operators. At the same time, we show that simple modifications, which we call semi-convolutional, have a much better chance of succeeding at this task. We use the latter to show a connection to Hough voting as well as to a variant of the bilateral kernel that is spatially steered by a convolutional network. We demonstrate that these operators can also be used to improve approaches such as Mask RCNN, demonstrating better segmentation of complex biological shapes and PASCAL VOC categories than achievable by Mask RCNN alone.", "label": "1", "is_selected": "0", "text": "Object detection and instance segmentation are dominated by region-based methods such as Mask RCNN. However, there is a growing interest in reducing these problems to pixel labeling tasks, as the latter could be more efficient, could be integrated seamlessly in image-to-image network architectures as used in many other tasks, and could be more accurate for objects that are not well approximated by bounding boxes. In this paper we show theoretically and empirically that constructing dense pixel embeddings that can separate object instances cannot be easily achieved using convolutional operators. At the same time, we show that simple modifications, which we call semi-convolutional, have a much better chance of succeeding at this task. We use the latter to show a connection to Hough voting as well as to a variant of the bilateral kernel that is spatially steered by a convolutional network. We demonstrate that these operators can also be used to improve approaches such as Mask RCNN, demonstrating better segmentation of complex biological shapes and PASCAL VOC categories than achievable by Mask RCNN alone."}
{"original_text": "Recent years have seen big advances in the field of sentence-level quality estimation (QE), largely as a result of using neural-based architectures. However, the majority of these methods work only on the language pair they are trained on and need retraining for new language pairs. This process can prove difficult from a technical point of view and is usually computationally expensive. In this paper we propose a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. Our evaluation shows that the proposed methods achieve state-of-the-art results outperforming current open-source quality estimation frameworks when trained on datasets from WMT. In addition, the framework proves very useful in transfer learning settings, especially when dealing with low-resourced languages, allowing us to obtain very competitive results.", "label": "1", "is_selected": "0", "text": "Recent years have seen big advances in the field of sentence-level quality estimation (QE), largely as a result of using neural-based architectures. However, the majority of these methods work only on the language pair they are trained on and need retraining for new language pairs. This process can prove difficult from a technical point of view and is usually computationally expensive. In this paper we propose a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. Our evaluation shows that the proposed methods achieve state-of-the-art results outperforming current open-source quality estimation frameworks when trained on datasets from WMT. In addition, the framework proves very useful in transfer learning settings, especially when dealing with low-resourced languages, allowing us to obtain very competitive results."}
{"original_text": "In this paper we study the problem of designing a distributed graph visualization algorithm for large graphs. The algorithm must be simple to implement and the computing infrastructure must not require major hardware or software investments. We design, implement, and experiment a force-directed algorithm in Giraph, a popular open source framework for distributed computing, based on a vertex-centric design paradigm. The algorithm is tested both on real and artificial graphs with up to million edges, by using a rather inexpensive PaaS (Platform as a Service) infrastructure of Amazon. The experiments show the scalability and effectiveness of our technique when compared to a centralized implementation of the same force-directed model. We show that graphs with about one million edges can be drawn in less than 8 minutes, by spending about 1 per drawing in the cloud computing infrastructure.", "label": "1", "is_selected": "0", "text": "In this paper we study the problem of designing a distributed graph visualization algorithm for large graphs. The algorithm must be simple to implement and the computing infrastructure must not require major hardware or software investments. We design, implement, and experiment a force-directed algorithm in Giraph, a popular open source framework for distributed computing, based on a vertex-centric design paradigm. The algorithm is tested both on real and artificial graphs with up to million edges, by using a rather inexpensive PaaS (Platform as a Service) infrastructure of Amazon. The experiments show the scalability and effectiveness of our technique when compared to a centralized implementation of the same force-directed model. We show that graphs with about one million edges can be drawn in less than 8 minutes, by spending about 1 per drawing in the cloud computing infrastructure."}
{"original_text": "With the shortage of physicians and surgeons and increase in demand worldwide due to situations such as the COVID-19 pandemic, there is a growing interest in finding solutions to help address the problem. A solution to this problem would be to use neurotechnology to provide them augmented cognition, senses and action for optimal diagnosis and treatment. Consequently, doing so can negatively impact them and others. We argue that applying neurotechnology for human enhancement in physicians and surgeons can cause injustices, and harm to them and patients. In this paper, we will first describe the augmentations and neurotechnologies that can be used to achieve the relevant augmentations for physicians and surgeons. We will then review selected ethical concerns discussed within literature, discuss the neuroengineering behind using neurotechnology for augmentation purposes, then conclude with an analysis on outcomes and ethical issues of implementing human augmentation via neurotechnology in medical and surgical practice.", "label": "1", "is_selected": "0", "text": "With the shortage of physicians and surgeons and increase in demand worldwide due to situations such as the COVID-19 pandemic, there is a growing interest in finding solutions to help address the problem. A solution to this problem would be to use neurotechnology to provide them augmented cognition, senses and action for optimal diagnosis and treatment. Consequently, doing so can negatively impact them and others. We argue that applying neurotechnology for human enhancement in physicians and surgeons can cause injustices, and harm to them and patients. In this paper, we will first describe the augmentations and neurotechnologies that can be used to achieve the relevant augmentations for physicians and surgeons. We will then review selected ethical concerns discussed within literature, discuss the neuroengineering behind using neurotechnology for augmentation purposes, then conclude with an analysis on outcomes and ethical issues of implementing human augmentation via neurotechnology in medical and surgical practice."}
{"original_text": "In this letter we propose the Rao test as a simpler alternative to the generalized likelihood ratio test (GLRT) for multisensor fusion. We consider sensors observing an unknown deterministic parameter with symmetric and unimodal noise. A decision fusion center (DFC) receives quantized sensor observations through error-prone binary symmetric channels and makes a global decision. We analyze the optimal quantizer thresholds and we study the performance of the Rao test in comparison to the GLRT. Also, a theoretical comparison is made and asymptotic performance is derived in a scenario with homogeneous sensors. All the results are confirmed through simulations.", "label": "1", "is_selected": "0", "text": "In this letter we propose the Rao test as a simpler alternative to the generalized likelihood ratio test (GLRT) for multisensor fusion. We consider sensors observing an unknown deterministic parameter with symmetric and unimodal noise. A decision fusion center (DFC) receives quantized sensor observations through error-prone binary symmetric channels and makes a global decision. We analyze the optimal quantizer thresholds and we study the performance of the Rao test in comparison to the GLRT. Also, a theoretical comparison is made and asymptotic performance is derived in a scenario with homogeneous sensors. All the results are confirmed through simulations."}
{"original_text": "Currently in the domain of facial analysis single task approaches for face detection and landmark localization dominate. In this paper we draw attention to multi-task models solving both tasks simultaneously. Multi-task models have shown promising results in various computer vision tasks, including object detection and segmentation. By jointly learning multiple tasks, these models can effectively share and transfer knowledge between related tasks, leading to improved accuracy and efficiency. In the context of facial analysis, multi-task models can combine face detection and landmark localization into a unified framework. This allows for more robust and accurate face detection, as well as more precise and efficient landmark localization. Moreover, multi-task models can be extended to include additional tasks, such as face recognition or emotion recognition, further improving the overall performance of facial analysis systems. Overall, the use of multi-task models in facial analysis is a promising direction for future research and development, with the potential to enable more advanced and sophisticated applications in the field.", "label": "0", "is_selected": "1", "text": "The use of multi-task models in computer vision has shown promising results in a number of areas, including object detection and segmentation and face recognition."}
{"original_text": "In this paper, we propose a novel algorithm for matching faces with temporal variations caused due to age progression. The proposed generative adversarial network algorithm is a unified framework that simultaneously learns the feature representation of the face and the mapping between the faces of different ages. It consists of two key components: a generator network and a discriminator network. The generator network takes a face image at a younger age as input and generates a corresponding face image at an older age. The discriminator network judges whether the generated image is real or fake. During training, the generator network learns to generate more realistic images that can fool the discriminator network. The proposed algorithm has several advantages over traditional methods. First, it can handle facial variations due to age progression in a more efficient and effective way. Second, it can generate new faces that are not present in the training set. Third, it can generate a sequence of face images that shows the progression of a face over time. The experimental results on various datasets demonstrate the effectiveness of our proposed algorithm in handling the face matching problem with age progression. The proposed algorithm achieves higher accuracy compared to state-of-the-art methods.", "label": "0", "is_selected": "1", "text": "Face recognition is one of the most computationally challenging problems in computer science because of the complexity of the human face and the variations in its appearance over time."}
{"original_text": "This chapter provides a comprehensive survey of the researches and motivations for hardware implementation of reservoir computing (RC) on neuromorphic electronic systems. Due to its computational efficiency and the fact that it can handle a wide range of tasks, RC has emerged as a promising approach for processing signals in real-time. Many hardware platforms, such as FPGAs, ASICs, and memristors, have been proposed and implemented for RC. These platforms offer different advantages and disadvantages, depending on factors such as power consumption, speed, and versatility. The chapter also examines how RC can be integrated with other machine learning techniques, such as deep learning, to enhance performance and improve accuracy in tasks such as speech recognition and image classification. Overall, this chapter highlights the importance and potential of hardware implementations of RC for real-time signal processing and presents an encouraging outlook for the future of this field.", "label": "0", "is_selected": "0", "text": "This chapter provides a comprehensive survey of the researches and motivations for hardware implementation of reservoir computing (RC) on neuromorphic electronic systems. Due to its computational efficiency and the fact that it can handle a wide range of tasks, RC has emerged as a promising approach for processing signals in real-time. Many hardware platforms, such as FPGAs, ASICs, and memristors, have been proposed and implemented for RC. These platforms offer different advantages and disadvantages, depending on factors such as power consumption, speed, and versatility. The chapter also examines how RC can be integrated with other machine learning techniques, such as deep learning, to enhance performance and improve accuracy in tasks such as speech recognition and image classification. Overall, this chapter highlights the importance and potential of hardware implementations of RC for real-time signal processing and presents an encouraging outlook for the future of this field."}
{"original_text": "For sustainable growth and profitability, online game companies are constantly carrying out various events to attract new game users, to maximize return users, and to minimize churn users in online games. Because minimizing churn users is the most cost-effective method, many pieces of research are being conducted on ways to predict and to prevent churns in advance. However, there is still little research on the validity of event effects. In this study, we investigate whether game events influence the user churn rate and confirm the difference in how game users respond to events by character level, item purchasing frequency and game-playing time band.", "label": "1", "is_selected": "0", "text": "For sustainable growth and profitability, online game companies are constantly carrying out various events to attract new game users, to maximize return users, and to minimize churn users in online games. Because minimizing churn users is the most cost-effective method, many pieces of research are being conducted on ways to predict and to prevent churns in advance. However, there is still little research on the validity of event effects. In this study, we investigate whether game events influence the user churn rate and confirm the difference in how game users respond to events by character level, item purchasing frequency and game-playing time band."}
{"original_text": "Based on 46 in-depth interviews with scientists, engineers, and CEOs, thisdocument presents a list of concrete machine research problems, progress onwhich would directly benefit tech ventures in East Africa.", "label": "1", "is_selected": "0", "text": "Based on 46 in-depth interviews with scientists, engineers, and CEOs, thisdocument presents a list of concrete machine research problems, progress onwhich would directly benefit tech ventures in East Africa."}
{"original_text": "Head pose estimation is an important pre-processing step in many pattern recognition and computer vision systems such as face recognition. Since the performance of the face recognition systems is greatly affected by the pose of the face, how to estimate the accurate pose of the face in the face image is still a challenging problem. In this paper, we present a novel method for head pose estimation. To enhance the efficiency of the estimation we first use contourlet transform for feature extraction. Contourlet transform is a multi-resolution, multi-direction transform. Finally, in order to reduce the feature space dimension and obtain appropriate features, we use LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) to remove inefficient features. Then, we apply k-nearest neighborhood (k-NN) and minimum distance classifiers to classify the pose of head. We use the public available FERET database to evaluate the performance of the proposed method. Simulation results indicate the efficiency of the proposed method in comparison with previous method.", "label": "1", "is_selected": "0", "text": "Head pose estimation is an important pre-processing step in many pattern recognition and computer vision systems such as face recognition. Since the performance of the face recognition systems is greatly affected by the pose of the face, how to estimate the accurate pose of the face in the face image is still a challenging problem. In this paper, we present a novel method for head pose estimation. To enhance the efficiency of the estimation we first use contourlet transform for feature extraction. Contourlet transform is a multi-resolution, multi-direction transform. Finally, in order to reduce the feature space dimension and obtain appropriate features, we use LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) to remove inefficient features. Then, we apply k-nearest neighborhood (k-NN) and minimum distance classifiers to classify the pose of head. We use the public available FERET database to evaluate the performance of the proposed method. Simulation results indicate the efficiency of the proposed method in comparison with previous method."}
{"original_text": "Background: COVID-19 pandemics has challenged emergency response systems worldwide, with widespread reports of essential services breakdown and collapse of health care structure. A critical element involves essential workforce management since these workers are vital in maintaining the functions of various essential services. Possible continuation: During pandemics, essential workers may face increased workload, prolonged hours of work, exposure to high-risk situations, and challenges related to personal protective equipment (PPE) access and usage. Therefore, effective management strategies for these workers are key in ensuring their well-being and maintaining service continuity. Some potential strategies include: - Regular communication with essential workers, providing up-to-date information and guidance on the pandemic situation, and listening to their concerns and suggestions. - Providing adequate PPE and training to workers on its proper usage, including regular reminders and refresher sessions. - Implementing flexible work arrangements, such as job sharing, rotating shifts or schedules, and remote work options where feasible. - Supporting workers' mental and physical health needs, such as providing counseling services, access to healthcare, and promoting healthy lifestyle habits. - Ensuring fair compensation, benefits, and job security to encourage workforce retention and motivation. Overall, effective essential workforce management involves a comprehensive approach that balances the needs of workers, the service demands, and the public health objectives.", "label": "0", "is_selected": "0", "text": "Background: COVID-19 pandemics has challenged emergency response systems worldwide, with widespread reports of essential services breakdown and collapse of health care structure. A critical element involves essential workforce management since these workers are vital in maintaining the functions of various essential services. Possible continuation: During pandemics, essential workers may face increased workload, prolonged hours of work, exposure to high-risk situations, and challenges related to personal protective equipment (PPE) access and usage. Therefore, effective management strategies for these workers are key in ensuring their well-being and maintaining service continuity. Some potential strategies include: - Regular communication with essential workers, providing up-to-date information and guidance on the pandemic situation, and listening to their concerns and suggestions. - Providing adequate PPE and training to workers on its proper usage, including regular reminders and refresher sessions. - Implementing flexible work arrangements, such as job sharing, rotating shifts or schedules, and remote work options where feasible. - Supporting workers' mental and physical health needs, such as providing counseling services, access to healthcare, and promoting healthy lifestyle habits. - Ensuring fair compensation, benefits, and job security to encourage workforce retention and motivation. Overall, effective essential workforce management involves a comprehensive approach that balances the needs of workers, the service demands, and the public health objectives."}
{"original_text": "Deep neural networks have achieved remarkable accuracy in many artificial intelligence applications, e.g. computer vision, at the cost of a large number of parameters and high computational complexity. Weight pruning has emerged as a promising technique to reduce the number of parameters and the computational complexity of deep neural networks without sacrificing their accuracy. Weight pruning involves removing or sparsifying the weights (i.e. connections) between neurons in a neural network that contribute the least to the overall input-output mapping. This is typically done by setting these weights to zero, thereby reducing the number of parameters in the network. There are various algorithms that can be used for weight pruning, ranging from simple thresholding to more sophisticated methods such as iterative pruning and magnitude pruning. These algorithms differ in their complexity, effectiveness, and implementation details. Overall, weight pruning is a powerful technique for reducing the size and computational cost of deep neural networks while maintaining their accuracy. This makes it an important tool for building practical and efficient AI systems.", "label": "0", "is_selected": "0", "text": "Deep neural networks have achieved remarkable accuracy in many artificial intelligence applications, e.g. computer vision, at the cost of a large number of parameters and high computational complexity. Weight pruning has emerged as a promising technique to reduce the number of parameters and the computational complexity of deep neural networks without sacrificing their accuracy. Weight pruning involves removing or sparsifying the weights (i.e. connections) between neurons in a neural network that contribute the least to the overall input-output mapping. This is typically done by setting these weights to zero, thereby reducing the number of parameters in the network. There are various algorithms that can be used for weight pruning, ranging from simple thresholding to more sophisticated methods such as iterative pruning and magnitude pruning. These algorithms differ in their complexity, effectiveness, and implementation details. Overall, weight pruning is a powerful technique for reducing the size and computational cost of deep neural networks while maintaining their accuracy. This makes it an important tool for building practical and efficient AI systems."}
{"original_text": "A multiple-antenna amplify-and-forward two-hop interference network with multiple links and multiple relays is considered. We optimize transmit precoders, receive decoders and relay AF matrices to maximize the achievable sum rate. That sounds like a complex problem! Can you give me some more details so I can better understand how to help you? For example, is there a specific objective function or constraint to consider in the optimization problem? What information is available at the transmitters, receivers, and relays?", "label": "0", "is_selected": "0", "text": "A multiple-antenna amplify-and-forward two-hop interference network with multiple links and multiple relays is considered. We optimize transmit precoders, receive decoders and relay AF matrices to maximize the achievable sum rate. That sounds like a complex problem! Can you give me some more details so I can better understand how to help you? For example, is there a specific objective function or constraint to consider in the optimization problem? What information is available at the transmitters, receivers, and relays?"}
{"original_text": "Speech processing systems rely on robust feature extraction to handle phonetic and semantic variations found in natural language. While techniques exist for desensitizing features to common noise patterns produced by background noise or reverberation, there can still be challenges in accurately recognizing speech in noisy environments. To overcome these challenges, advanced signal processing techniques such as noise suppression, echo cancellation, and beamforming can be used to enhance the quality of speech signals before feature extraction. Additionally, machine learning algorithms such as deep neural networks can be trained on large datasets of clean and noisy speech to improve speech recognition accuracy in different noise environments.", "label": "0", "is_selected": "0", "text": "Speech processing systems rely on robust feature extraction to handle phonetic and semantic variations found in natural language. While techniques exist for desensitizing features to common noise patterns produced by background noise or reverberation, there can still be challenges in accurately recognizing speech in noisy environments. To overcome these challenges, advanced signal processing techniques such as noise suppression, echo cancellation, and beamforming can be used to enhance the quality of speech signals before feature extraction. Additionally, machine learning algorithms such as deep neural networks can be trained on large datasets of clean and noisy speech to improve speech recognition accuracy in different noise environments."}
{"original_text": "In this work, we study the problem of band allocation of M s buffered secondary users (SUs) to M p primary bands licensed to (owned by) M p buffered primary users (PUs), in a cognitive radio network scenario. The goal is to maximize the total weighted throughput of the SUs subject to the interference constraint on the PUs. We propose a joint band allocation and power control algorithm, based on sequential convex optimization, that achieves a near-optimal solution in polynomial time. The algorithm guarantees the convergence to a Karush-Kuhn-Tucker (KKT) point of the optimization problem and provides a fair allocation of the spectrum among the SUs. Simulations show that the proposed algorithm outperforms other baseline algorithms in terms of throughput and fairness, while respecting the interference constraints on the PUs.", "label": "0", "is_selected": "0", "text": "In this work, we study the problem of band allocation of M s buffered secondary users (SUs) to M p primary bands licensed to (owned by) M p buffered primary users (PUs), in a cognitive radio network scenario. The goal is to maximize the total weighted throughput of the SUs subject to the interference constraint on the PUs. We propose a joint band allocation and power control algorithm, based on sequential convex optimization, that achieves a near-optimal solution in polynomial time. The algorithm guarantees the convergence to a Karush-Kuhn-Tucker (KKT) point of the optimization problem and provides a fair allocation of the spectrum among the SUs. Simulations show that the proposed algorithm outperforms other baseline algorithms in terms of throughput and fairness, while respecting the interference constraints on the PUs."}
{"original_text": "Distributed machine learning is becoming a popular model-training method due to privacy, computational scalability, and bandwidth capacities. In this work, we explore scalable distributed-training versions of two algorithms commonly used in machine learning: stochastic gradient descent (SGD) and alternating least squares (ALS). Distributed SGD involves partitioning data across multiple machines, and updating the model parameters in a parallel fashion. This results in faster training times and the ability to handle larger datasets. However, it can be more challenging to ensure that the different machines are able to communicate effectively and maintain consistency between updates. Distributed ALS is commonly used for matrix factorization problems, such as collaborative filtering in recommender systems. In this approach, the matrix to be factorized is partitioned across multiple machines, and each machine updates a subset of the latent factors. This allows for efficient scaling to large datasets and can be tailored to the specific structure of the matrix. Overall, distributed machine learning is an important area of research and development, enabling faster and more efficient training of models on large datasets. However, it can also be challenging to design and optimize distributed algorithms, and careful consideration must be given to issues such as communication overhead and load balancing.", "label": "0", "is_selected": "1", "text": "In this paper, we discuss the design and optimization of two popular distributed machine learning algorithms for training models on datasets of up to 1 Terabit per second (TBS)."}
{"original_text": "A singularly perturbed parabolic problem of convection-diffusion type with a discontinuous initial condition is examined. An analytic function is identified which matches the discontinuity in the initial condition and also captures the behavior of the solution as the perturbation parameter approaches zero. This technique is known as matched asymptotic expansion and involves constructing a series expansion of the solution in two separate regions, one near the discontinuity and one away from it. The two expansions are then matched together using the analytic function to ensure that the solution is continuous and satisfies the governing equations. The process is useful for problems where the dominant behavior of the system changes abruptly over a small region or when there are sharp gradients in the initial or boundary conditions.", "label": "0", "is_selected": "1", "text": "A new technique for solving parabolic problems with discontinuous initial conditions has been proposed and tested in the context of fluid mechanics. The process is useful for problems where the dominant behavior of the system changes abruptly over a small region or when there are sharp gradients in the initial or boundary conditions."}
{"original_text": "In this paper, we design and experiment a far-field wireless power transfer (WPT) architecture based on distributed antennas, so-called WPT DAS, that dynamically selects transmit antenna and frequency to increase the output dc power. Uniquely, spatial and frequency diversities are jointly exploited in the proposed WPT DAS with low complexity, low cost, and flexible deployment to combat the wireless fading channel. A numerical experiment is designed to show the benefits using antenna and frequency selections in spatially and frequency selective fading channels for single-user and multi-user cases. Accordingly, the proposed WPT DAS for single-user and two-user cases is prototyped. At the transmitter, we adopt antenna selection to exploit spatial diversity and adopt frequency selection to exploit frequency diversity. A low-complexity over-the-air limited feedback using an IEEE 802.15.4 RF interface is designed for antenna and frequency selections and reporting from the receiver to the transmitter. The proposed WPT DAS prototype is demonstrated in a real indoor environment. The measurements show that WPT DAS can boost the output dc power by up to 30 dB in single-user case and boost the sum of output dc power by up to 21.8 dB in two-user case and broaden the service coverage area in a low cost, low complexity, and flexible manner.", "label": "1", "is_selected": "0", "text": "In this paper, we design and experiment a far-field wireless power transfer (WPT) architecture based on distributed antennas, so-called WPT DAS, that dynamically selects transmit antenna and frequency to increase the output dc power. Uniquely, spatial and frequency diversities are jointly exploited in the proposed WPT DAS with low complexity, low cost, and flexible deployment to combat the wireless fading channel. A numerical experiment is designed to show the benefits using antenna and frequency selections in spatially and frequency selective fading channels for single-user and multi-user cases. Accordingly, the proposed WPT DAS for single-user and two-user cases is prototyped. At the transmitter, we adopt antenna selection to exploit spatial diversity and adopt frequency selection to exploit frequency diversity. A low-complexity over-the-air limited feedback using an IEEE 802.15.4 RF interface is designed for antenna and frequency selections and reporting from the receiver to the transmitter. The proposed WPT DAS prototype is demonstrated in a real indoor environment. The measurements show that WPT DAS can boost the output dc power by up to 30 dB in single-user case and boost the sum of output dc power by up to 21.8 dB in two-user case and broaden the service coverage area in a low cost, low complexity, and flexible manner."}
{"original_text": "The fuzzy K -means problem is a generalization of the classical K -means problem to soft clusterings, i.e. clusterings where each points belongs to each cluster to some degree. Although the classical K-means problem assigns each point to a single cluster, the fuzzy K-means problem assigns each point to all clusters with degrees of membership representing how likely the point belongs to each cluster. The fuzzy K-means problem is also known as FCM (fuzzy c-means) and is commonly used in pattern recognition and image segmentation. In fuzzy K-means, the objective is to minimize the sum of squared distances between each point and its cluster center, weighted by the degree of membership. The degree of membership of each point to each cluster is computed using a membership function which gives a value between 0 and 1, representing the probability or degree of belief that the point belongs to that cluster. The membership function is updated iteratively until convergence is reached. Fuzzy K-means has several advantages over classical K-means, including the ability to handle overlapping clusters and the ability to assign points to multiple clusters simultaneously. It also allows for more nuanced representations of cluster membership and a finer-grained analysis of data. However, fuzzy K-means can be computationally intensive and requires careful tuning of parameters.", "label": "0", "is_selected": "0", "text": "The fuzzy K -means problem is a generalization of the classical K -means problem to soft clusterings, i.e. clusterings where each points belongs to each cluster to some degree. Although the classical K-means problem assigns each point to a single cluster, the fuzzy K-means problem assigns each point to all clusters with degrees of membership representing how likely the point belongs to each cluster. The fuzzy K-means problem is also known as FCM (fuzzy c-means) and is commonly used in pattern recognition and image segmentation. In fuzzy K-means, the objective is to minimize the sum of squared distances between each point and its cluster center, weighted by the degree of membership. The degree of membership of each point to each cluster is computed using a membership function which gives a value between 0 and 1, representing the probability or degree of belief that the point belongs to that cluster. The membership function is updated iteratively until convergence is reached. Fuzzy K-means has several advantages over classical K-means, including the ability to handle overlapping clusters and the ability to assign points to multiple clusters simultaneously. It also allows for more nuanced representations of cluster membership and a finer-grained analysis of data. However, fuzzy K-means can be computationally intensive and requires careful tuning of parameters."}
{"original_text": "Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.", "label": "1", "is_selected": "0", "text": "Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability."}
{"original_text": "Much of recent success in multiagent reinforcement learning has been in two-player zero-sum games. In these games, algorithms such as fictitious self-play and minimax tree search can converge to an optimal strategy for both players. However, in non-zero-sum games or games with more than two players, finding optimal strategies is a more challenging task. One approach to address this challenge is to use population-based methods such as evolutionary algorithms or particle swarm optimization. Another approach is to decompose the game into smaller sub-games, which are solvable using existing two-player algorithms, and then combine the solutions. There has also been recent interest in using deep reinforcement learning algorithms to learn policies for multiagent games. These algorithms use neural networks to approximate the optimal policy, which can then be used to play the game. One example of such an algorithm is the multiagent deep deterministic policy gradient (MADDPG) algorithm. Overall, multiagent reinforcement learning is an active area of research, and there are many exciting developments in this field.", "label": "0", "is_selected": "1", "text": "Multiagent reinforcement learning is a branch of computer science that deals with the problem of learning optimal strategies for a variety of games, from zero-sum games to multiagent games."}
{"original_text": "We study the power and limits of optimal dynamic pricing in combinatorial markets; i.e., dynamic pricing that leads to optimal social welfare. Previous work by Cohen-Addad et al. [EC'16] demonstrated the existence of optimal dynamic prices for unit-demand buyers, and showed a market with coverage valuations that admits no such prices. However, finding the frontier of markets (i.e., valuation functions) that admit optimal dynamic prices remains an open problem. In this work we establish positive and negative results that narrow the existing gap. On the positive side, we provide tools for handling markets beyond unit-demand valuations. In particular, we characterize all optimal allocations in multi-demand markets. This characterization allows us to partition the items into equivalence classes according to the role they play in achieving optimality. Using these tools, we provide a poly-time optimal dynamic pricing algorithm for up to 3 multi-demand buyers. On the negative side, we establish a maximal domain theorem, showing that for every non-gross substitutes valuation, there exist unit-demand valuations such that adding them yields a market that does not admit an optimal dynamic pricing. This result is reminiscent of the seminal maximal domain theorem by Gul and Stacchetti [JET'99] for Walrasian equilibrium. Yang [JET'17] discovered an error in their original proof, and established a different, incomparable version of their maximal domain theorem. En route to our maximal domain theorem for optimal dynamic pricing, we provide the first complete proof of the original theorem by Gul and Stacchetti.", "label": "1", "is_selected": "0", "text": "We study the power and limits of optimal dynamic pricing in combinatorial markets; i.e., dynamic pricing that leads to optimal social welfare. Previous work by Cohen-Addad et al. [EC'16] demonstrated the existence of optimal dynamic prices for unit-demand buyers, and showed a market with coverage valuations that admits no such prices. However, finding the frontier of markets (i.e., valuation functions) that admit optimal dynamic prices remains an open problem. In this work we establish positive and negative results that narrow the existing gap. On the positive side, we provide tools for handling markets beyond unit-demand valuations. In particular, we characterize all optimal allocations in multi-demand markets. This characterization allows us to partition the items into equivalence classes according to the role they play in achieving optimality. Using these tools, we provide a poly-time optimal dynamic pricing algorithm for up to 3 multi-demand buyers. On the negative side, we establish a maximal domain theorem, showing that for every non-gross substitutes valuation, there exist unit-demand valuations such that adding them yields a market that does not admit an optimal dynamic pricing. This result is reminiscent of the seminal maximal domain theorem by Gul and Stacchetti [JET'99] for Walrasian equilibrium. Yang [JET'17] discovered an error in their original proof, and established a different, incomparable version of their maximal domain theorem. En route to our maximal domain theorem for optimal dynamic pricing, we provide the first complete proof of the original theorem by Gul and Stacchetti."}
{"original_text": "Transient execution attacks, also called speculative execution attacks, have drawn much interest as they exploit the transient execution of instructions, e.g., during branch prediction, to leak data. Transient execution is a feature of modern processors that allows them to execute instructions speculatively ahead of time to improve performance. In a typical processor pipeline, instructions are fetched, decoded, executed, and then the results are written back to memory or a register file. However, with transient execution, instructions are fetched and executed speculatively, even before the processor has finished decoding instructions earlier in the pipeline. In the case of branch prediction, which is used to predict the outcome of a conditional jump instruction, the processor will speculatively execute the instructions in the predicted path, even if they might not be needed. This allows the processor to execute instructions faster by reducing the number of pipeline stalls caused by waiting for the outcome of a branch instruction. However, this speculative execution can potentially reveal sensitive data, particularly if the instructions being executed are accessing data that the attacker is interested in. For example, an attacker may be able to leak secrets such as encryption keys or other sensitive data by carefully crafting instructions that are executed speculatively, and then observing the effect of those instructions on the processor's cache. Transient execution attacks have been shown to be effective against modern processors, including those from Intel, AMD, and ARM. Several variants of these attacks have been identified, including Meltdown, Spectre, Foreshadow, and ZombieLoad. These attacks rely on sophisticated techniques to leak sensitive data, and mitigating them requires a combination of hardware and software changes.", "label": "0", "is_selected": "0", "text": "Transient execution attacks, also called speculative execution attacks, have drawn much interest as they exploit the transient execution of instructions, e.g., during branch prediction, to leak data. Transient execution is a feature of modern processors that allows them to execute instructions speculatively ahead of time to improve performance. In a typical processor pipeline, instructions are fetched, decoded, executed, and then the results are written back to memory or a register file. However, with transient execution, instructions are fetched and executed speculatively, even before the processor has finished decoding instructions earlier in the pipeline. In the case of branch prediction, which is used to predict the outcome of a conditional jump instruction, the processor will speculatively execute the instructions in the predicted path, even if they might not be needed. This allows the processor to execute instructions faster by reducing the number of pipeline stalls caused by waiting for the outcome of a branch instruction. However, this speculative execution can potentially reveal sensitive data, particularly if the instructions being executed are accessing data that the attacker is interested in. For example, an attacker may be able to leak secrets such as encryption keys or other sensitive data by carefully crafting instructions that are executed speculatively, and then observing the effect of those instructions on the processor's cache. Transient execution attacks have been shown to be effective against modern processors, including those from Intel, AMD, and ARM. Several variants of these attacks have been identified, including Meltdown, Spectre, Foreshadow, and ZombieLoad. These attacks rely on sophisticated techniques to leak sensitive data, and mitigating them requires a combination of hardware and software changes."}
{"original_text": "Abundant data is the key to successful machine learning. However, supervised learning requires annotated data that are often hard to obtain. In a classification task with limited resources, Active Learning (AL) promises to guide annotators to examples that bring the most value for a classifier. AL can be successfully combined with self-training, i.e., extending a training set with the unlabelled examples for which a classifier is the most certain. We report our experiences on using AL in a systematic manner to train an SVM classifier for Stack Overflow posts discussing performance of software components. We show that the training examples deemed as the most valuable to the classifier are also the most difficult for humans to annotate. Despite carefully evolved annotation criteria, we report low inter-rater agreement, but we also propose mitigation strategies. Finally, based on one annotator's work, we show that self-training can improve the classification accuracy. We conclude the paper by discussing implication for future text miners aspiring to use AL and self-training.", "label": "1", "is_selected": "0", "text": "Abundant data is the key to successful machine learning. However, supervised learning requires annotated data that are often hard to obtain. In a classification task with limited resources, Active Learning (AL) promises to guide annotators to examples that bring the most value for a classifier. AL can be successfully combined with self-training, i.e., extending a training set with the unlabelled examples for which a classifier is the most certain. We report our experiences on using AL in a systematic manner to train an SVM classifier for Stack Overflow posts discussing performance of software components. We show that the training examples deemed as the most valuable to the classifier are also the most difficult for humans to annotate. Despite carefully evolved annotation criteria, we report low inter-rater agreement, but we also propose mitigation strategies. Finally, based on one annotator's work, we show that self-training can improve the classification accuracy. We conclude the paper by discussing implication for future text miners aspiring to use AL and self-training."}
{"original_text": "We propose a machine learning framework to synthesize reactive controllers for systems whose interactions with their adversarial environment are modeled by infinite-duration, two-player games over (potentially) infinite graphs. Our framework targets safety games with infinitely many vertices, but it is also applicable to safety games over finite graphs whose size is too prohibitive for conventional synthesis techniques. The learning takes place in a feedback loop between a teacher component, which can reason symbolically about the safety game, and a learning algorithm, which successively learns an approximation of the winning region from various kinds of examples provided by the teacher. We develop a novel decision tree learning algorithm for this setting and show that our algorithm is guaranteed to converge to a reactive safety controller if a suitable approximation of the winning region can be expressed as a decision tree. Finally, we empirically compare the performance of a prototype implementation to existing approaches, which are based on constraint solving and automata learning, respectively.", "label": "1", "is_selected": "0", "text": "We propose a machine learning framework to synthesize reactive controllers for systems whose interactions with their adversarial environment are modeled by infinite-duration, two-player games over (potentially) infinite graphs. Our framework targets safety games with infinitely many vertices, but it is also applicable to safety games over finite graphs whose size is too prohibitive for conventional synthesis techniques. The learning takes place in a feedback loop between a teacher component, which can reason symbolically about the safety game, and a learning algorithm, which successively learns an approximation of the winning region from various kinds of examples provided by the teacher. We develop a novel decision tree learning algorithm for this setting and show that our algorithm is guaranteed to converge to a reactive safety controller if a suitable approximation of the winning region can be expressed as a decision tree. Finally, we empirically compare the performance of a prototype implementation to existing approaches, which are based on constraint solving and automata learning, respectively."}
{"original_text": "This paper addresses the problem of target detection and localisation in a limited area using multiple coordinated agents. The swarm of Unmanned Aerial Vehicles (UAVs) determines the position of the dispersion of stack effluents to a gas plume in a certain production area as fast as possible, that makes the problem challenging to model and solve, because of the time variability of the target. Three different exploration algorithms are designed and compared. Besides the exploration strategies, the paper reports a solution for quick convergence towards the actual stack position once detected by one member of the team. Both the navigation and localisation algorithms are fully distributed and based on the consensus theory. Simulations on realistic case studies are reported.", "label": "1", "is_selected": "0", "text": "This paper addresses the problem of target detection and localisation in a limited area using multiple coordinated agents. The swarm of Unmanned Aerial Vehicles (UAVs) determines the position of the dispersion of stack effluents to a gas plume in a certain production area as fast as possible, that makes the problem challenging to model and solve, because of the time variability of the target. Three different exploration algorithms are designed and compared. Besides the exploration strategies, the paper reports a solution for quick convergence towards the actual stack position once detected by one member of the team. Both the navigation and localisation algorithms are fully distributed and based on the consensus theory. Simulations on realistic case studies are reported."}
{"original_text": "Parameterized algorithms are a very useful tool for dealing with NP-hard problems on graphs. Yet, to properly utilize parameterized algorithms it is necessary to choose the right parameter based on the type of problem and properties of the target graph class. Tree-width is an example of a very successful graph parameter, however it cannot be used on dense graph classes and there also exist problems which are hard even on graphs of bounded tree-width. Such problems can be tackled by using vertex cover as a parameter, however this places severe restrictions on admissible graph classes. Michael Lampis has recently introduced neighborhood diversity, a new graph parameter which generalizes vertex cover to dense graphs. Among other results, he has shown that simple parameterized algorithms exist for a few problems on graphs of bounded neighborhood diversity. Our article further studies this area and provides new algorithms parameterized by neighborhood diversity for the p-Vertex-Disjoint Paths, Graph Motif and Precoloring Extension problems - the latter two being hard even on graphs of bounded tree-width.", "label": "1", "is_selected": "0", "text": "Parameterized algorithms are a very useful tool for dealing with NP-hard problems on graphs. Yet, to properly utilize parameterized algorithms it is necessary to choose the right parameter based on the type of problem and properties of the target graph class. Tree-width is an example of a very successful graph parameter, however it cannot be used on dense graph classes and there also exist problems which are hard even on graphs of bounded tree-width. Such problems can be tackled by using vertex cover as a parameter, however this places severe restrictions on admissible graph classes. Michael Lampis has recently introduced neighborhood diversity, a new graph parameter which generalizes vertex cover to dense graphs. Among other results, he has shown that simple parameterized algorithms exist for a few problems on graphs of bounded neighborhood diversity. Our article further studies this area and provides new algorithms parameterized by neighborhood diversity for the p-Vertex-Disjoint Paths, Graph Motif and Precoloring Extension problems - the latter two being hard even on graphs of bounded tree-width."}
{"original_text": "The analysis techniques of system log messages (syslog messages) have a long history from when the syslog mechanism was invented. Typically, the analysis consists of two parts, one is a parsing step where the syslog messages are transformed into a structured format, and the other is the actual analysis of the parsed messages. The parsing step usually involves using regular expressions or other parsing libraries to extract important information such as the timestamp, source, severity level, and message content. Once this information is extracted, it can be stored in a database or analyzed further. The analysis step involves using various techniques to identify patterns, anomalies, or trends in the syslog data. Some common techniques include: 1. Statistical analysis: This involves using statistical methods to identify patterns, trends, and anomalies in the data. For example, you can use regression analysis to see how a specific variable changes over time, or use clustering algorithms to group log messages with similar characteristics. 2. Machine learning: This is an advanced technique that involves training a machine learning model to recognize patterns in the data. For example, you can use a neural network or a decision tree algorithm to identify specific types of errors. 3. Visualization: This involves creating graphical representations of the data to help identify patterns or trends. For example, you can use a line chart to show how the number of log messages changes over time, or a pie chart to show the distribution of log messages by severity level. Overall, the analysis of syslog messages can provide valuable insights into the performance, security, and overall health of a system. With the right tools and techniques, system administrators can use syslog data to improve system reliability and identify potential problems before they become major issues.", "label": "0", "is_selected": "1", "text": "In this article, we are going to look at the analysis techniques of syslog messages and how they can be used to improve the performance of your system."}
{"original_text": "Recommender systems (RS) are increasingly present in our daily lives, especially since the advent of Big Data, which allows for storing all kinds of information about users' preferences. Personalized RS analyze this information and provide recommendations that are tailored to a specific user's interests or needs. There are two main types of RS: collaborative filtering and content-based filtering. Collaborative filtering makes recommendations based on similarities between users. It looks at the items a user has interacted with (purchased, rated, viewed, etc.) and then identifies other users who have shown similar behavior. Recommendations are then made based on what these similar users have interacted with in the past. Content-based filtering, on the other hand, uses data about items themselves to make recommendations. It looks at the features of an item such as genre, director, or actor, and then recommends other items with similar features. Hybrid RS combine both collaborative and content-based filtering to provide even more accurate and personalized recommendations. RS can be found in many different contexts, such as e-commerce sites, social media platforms, and music or video streaming services. They can help users discover new products, services, or content that they may not have otherwise found on their own.", "label": "0", "is_selected": "0", "text": "Recommender systems (RS) are increasingly present in our daily lives, especially since the advent of Big Data, which allows for storing all kinds of information about users' preferences. Personalized RS analyze this information and provide recommendations that are tailored to a specific user's interests or needs. There are two main types of RS: collaborative filtering and content-based filtering. Collaborative filtering makes recommendations based on similarities between users. It looks at the items a user has interacted with (purchased, rated, viewed, etc.) and then identifies other users who have shown similar behavior. Recommendations are then made based on what these similar users have interacted with in the past. Content-based filtering, on the other hand, uses data about items themselves to make recommendations. It looks at the features of an item such as genre, director, or actor, and then recommends other items with similar features. Hybrid RS combine both collaborative and content-based filtering to provide even more accurate and personalized recommendations. RS can be found in many different contexts, such as e-commerce sites, social media platforms, and music or video streaming services. They can help users discover new products, services, or content that they may not have otherwise found on their own."}
{"original_text": "Nowadays, offensive content in social media has become a serious problem, and automatically detecting offensive language is an essential task. In this paper, we build an offensive language detection system, which can automatically identify and flag content that is offensive or inappropriate. Our system adopts machine learning techniques to learn from a large dataset of labeled examples, and then applies this knowledge to unseen text data. Specifically, we use a combination of feature engineering and deep neural networks to extract meaningful information from text, and then make predictions based on this information. Our system is trained on a labeled dataset of tweets that have been annotated for their offensiveness. The dataset includes a wide range of offensive language, from racial slurs and hate speech to sexual content and profanity. We use this data to train a deep neural network, which can recognize patterns in text and make predictions based on these patterns. To evaluate the performance of our system, we use several standard measures of accuracy, precision, recall, and F1-score. Our experiments show that our system achieves high levels of performance in detecting offensive language, with an F1-score of over 90. Overall, our system provides an effective and efficient way to automatically identify offensive language in social media. This can help to reduce the spread of harmful or inappropriate content, and ensure that social media platforms remain a safe and respectful place for all users.", "label": "0", "is_selected": "0", "text": "Nowadays, offensive content in social media has become a serious problem, and automatically detecting offensive language is an essential task. In this paper, we build an offensive language detection system, which can automatically identify and flag content that is offensive or inappropriate. Our system adopts machine learning techniques to learn from a large dataset of labeled examples, and then applies this knowledge to unseen text data. Specifically, we use a combination of feature engineering and deep neural networks to extract meaningful information from text, and then make predictions based on this information. Our system is trained on a labeled dataset of tweets that have been annotated for their offensiveness. The dataset includes a wide range of offensive language, from racial slurs and hate speech to sexual content and profanity. We use this data to train a deep neural network, which can recognize patterns in text and make predictions based on these patterns. To evaluate the performance of our system, we use several standard measures of accuracy, precision, recall, and F1-score. Our experiments show that our system achieves high levels of performance in detecting offensive language, with an F1-score of over 90. Overall, our system provides an effective and efficient way to automatically identify offensive language in social media. This can help to reduce the spread of harmful or inappropriate content, and ensure that social media platforms remain a safe and respectful place for all users."}
{"original_text": "Recently, Image-to-Image Translation (IIT) has achieved great progress in image style transfer and semantic context manipulation for images. However, existing approaches require exhaustively labelling training data, which is labor demanding, time-consuming, and expensive. One potential solution to this problem is to use unsupervised learning methods that make use of unlabelled data. One such method is Generative Adversarial Networks (GANs), which can learn to generate realistic images without requiring labelled data. GANs consist of two neural networks: a generator network that creates images, and a discriminator network that tries to distinguish the generated images from real images. The two networks compete with each other, and over time the generator network learns to generate images that are increasingly similar to real images. Another unsupervised learning method that can be used for IIT is CycleGAN, which can learn to translate between two domains without requiring paired examples. CycleGAN uses a cycle-consistency loss to ensure that the translated images can be converted back to the original domain without loss of information. Overall, the use of unsupervised learning methods for IIT can significantly reduce the cost and time required for training and labelling data, and has the potential to enable more widespread adoption of IIT technologies.", "label": "0", "is_selected": "1", "text": "In this paper, we report on the use of unsupervised learning methods for image-to-image translation, which can significantly reduce the cost and time required for training and labelling data, and the potential to enable more widespread adoption of IIT technologies."}
{"original_text": "In this work, we demonstrate that receptive fields in 3D pose estimation can be effectively specified using optical flow. We introduce adaptive receptive fields, a simple and effective method to aid receptive field selection in pose estimation models based on optical flow inference. We contrast the performance of a benchmark state-of-the-art model running on fixed receptive fields with their adaptive field counterparts. By using a reduced receptive field, our model can process slow-motion sequences (10x longer) 23 faster than the benchmark model running at regular speed. The reduction in computational cost is achieved while producing a pose prediction accuracy to within 0.36 of the benchmark model.", "label": "1", "is_selected": "0", "text": "In this work, we demonstrate that receptive fields in 3D pose estimation can be effectively specified using optical flow. We introduce adaptive receptive fields, a simple and effective method to aid receptive field selection in pose estimation models based on optical flow inference. We contrast the performance of a benchmark state-of-the-art model running on fixed receptive fields with their adaptive field counterparts. By using a reduced receptive field, our model can process slow-motion sequences (10x longer) 23 faster than the benchmark model running at regular speed. The reduction in computational cost is achieved while producing a pose prediction accuracy to within 0.36 of the benchmark model."}
{"original_text": "Ad hoc electrical networks are formed by connecting power sources and loads without pre-determining the network topology. These systems are well-suited to addressing the lack of electricity in rural areas because they can be assembled and modified by non-expert users without central oversight. There are two core aspects to ad hoc system design: 1) designing source and load units such that the microgrid formed from the arbitrary interconnection of many units is always stable and 2) developing control strategies to autonomously manage the microgrid (i.e., perform power dispatch and voltage regulation) in a decentralized manner and under large uncertainty. To address these challenges we apply a number of nonlinear control techniques - including Brayton-Moser potential theory and primal-dual dynamics - to obtain conditions under which an ad hoc dc microgrid will have a suitable and asymptotically stable equilibrium point. Further, we propose a new decentralized control scheme that coordinates many sources to achieve a specified power dispatch from each. A simulated comparison to previous research is included.", "label": "1", "is_selected": "0", "text": "Ad hoc electrical networks are formed by connecting power sources and loads without pre-determining the network topology. These systems are well-suited to addressing the lack of electricity in rural areas because they can be assembled and modified by non-expert users without central oversight. There are two core aspects to ad hoc system design: 1) designing source and load units such that the microgrid formed from the arbitrary interconnection of many units is always stable and 2) developing control strategies to autonomously manage the microgrid (i.e., perform power dispatch and voltage regulation) in a decentralized manner and under large uncertainty. To address these challenges we apply a number of nonlinear control techniques - including Brayton-Moser potential theory and primal-dual dynamics - to obtain conditions under which an ad hoc dc microgrid will have a suitable and asymptotically stable equilibrium point. Further, we propose a new decentralized control scheme that coordinates many sources to achieve a specified power dispatch from each. A simulated comparison to previous research is included."}
{"original_text": "In reinforcement learning (RL), agents often operate in partially observed and uncertain environments. Model-based RL suggests that this is best achieved by learning and exploiting a probabilistic model of the world. 'Active inference' is an emerging normative framework in cognitive and computational neuroscience that offers a unifying account of how biological agents achieve this. On this framework, inference, learning and action emerge from a single imperative to maximize the Bayesian evidence for a niched model of the world. However, implementations of this process have thus far been restricted to low-dimensional and idealized situations. Here, we present a working implementation of active inference that applies to high-dimensional tasks, with proof-of-principle results demonstrating efficient exploration and an order of magnitude increase in sample efficiency over strong model-free baselines. Our results demonstrate the feasibility of applying active inference at scale and highlight the operational homologies between active inference and current model-based approaches to RL.", "label": "1", "is_selected": "0", "text": "In reinforcement learning (RL), agents often operate in partially observed and uncertain environments. Model-based RL suggests that this is best achieved by learning and exploiting a probabilistic model of the world. 'Active inference' is an emerging normative framework in cognitive and computational neuroscience that offers a unifying account of how biological agents achieve this. On this framework, inference, learning and action emerge from a single imperative to maximize the Bayesian evidence for a niched model of the world. However, implementations of this process have thus far been restricted to low-dimensional and idealized situations. Here, we present a working implementation of active inference that applies to high-dimensional tasks, with proof-of-principle results demonstrating efficient exploration and an order of magnitude increase in sample efficiency over strong model-free baselines. Our results demonstrate the feasibility of applying active inference at scale and highlight the operational homologies between active inference and current model-based approaches to RL."}
{"original_text": "In this paper, we address an issue that the visually impaired commonly face while crossing intersections and propose a solution that takes form as a mobile application. The application utilizes a deep learning convolutional neural network model, LytNetV2, to output necessary information that the visually impaired may lack when without human companions or guide-dogs. A prototype of the application runs on iOS devices of versions 11 or above. It is designed for comprehensiveness, concision, accuracy, and computational efficiency through delivering the two most important pieces of information, pedestrian traffic light color and direction, required to cross the road in real-time. Furthermore, it is specifically aimed to support those facing financial burden as the solution takes the form of a free mobile application. Through the modification and utilization of key principles in MobileNetV3 such as depthwise seperable convolutions and squeeze-excite layers, the deep neural network model achieves a classification accuracy of 96 and average angle error of 6.15deg, while running at a frame rate of 16.34 frames per second. Additionally, the model is trained as an image classifier, allowing for a faster and more accurate model. The network is able to outperform other methods such as object detection and non-deep learning algorithms in both accuracy and thoroughness. The information is delivered through both auditory signals and vibrations, and it has been tested on seven visually impaired and has received above satisfactory responses.", "label": "1", "is_selected": "0", "text": "In this paper, we address an issue that the visually impaired commonly face while crossing intersections and propose a solution that takes form as a mobile application. The application utilizes a deep learning convolutional neural network model, LytNetV2, to output necessary information that the visually impaired may lack when without human companions or guide-dogs. A prototype of the application runs on iOS devices of versions 11 or above. It is designed for comprehensiveness, concision, accuracy, and computational efficiency through delivering the two most important pieces of information, pedestrian traffic light color and direction, required to cross the road in real-time. Furthermore, it is specifically aimed to support those facing financial burden as the solution takes the form of a free mobile application. Through the modification and utilization of key principles in MobileNetV3 such as depthwise seperable convolutions and squeeze-excite layers, the deep neural network model achieves a classification accuracy of 96 and average angle error of 6.15deg, while running at a frame rate of 16.34 frames per second. Additionally, the model is trained as an image classifier, allowing for a faster and more accurate model. The network is able to outperform other methods such as object detection and non-deep learning algorithms in both accuracy and thoroughness. The information is delivered through both auditory signals and vibrations, and it has been tested on seven visually impaired and has received above satisfactory responses."}
{"original_text": "For sustainable growth and profitability, online game companies are constantly carrying out various events to attract new game users, to maximize return users, and to minimize churn users in online games. Here are some strategies that online game companies can use to achieve sustainable growth and profitability: 1. Offer Incentives: Offering incentives to new users is a great way to attract them to your game. These incentives can be in the form of free items or game currency that will help them progress in the game faster. This will encourage them to stay engaged with the game and become return users. 2. Provide a Great User Experience: In order to maximize return users, it's important to provide a great user experience. This includes a user-friendly interface, immersive gameplay, regular updates, and fast loading times. By providing an enjoyable experience, users are more likely to keep coming back to your game. 3. Encourage Social Interactions: Social interactions are key to minimizing churn users. Encouraging users to play with their friends or join a community within the game can increase engagement and make the game more enjoyable. This will help to build a loyal user base who will keep coming back to the game. 4. Utilize Data Analytics: Data analytics can help online game companies identify user behavior patterns, preferences, and trends. Using this information, companies can make informed decisions about where to focus their efforts to maximize growth and profitability. 5. Invest in Marketing: Investing in marketing strategies such as advertising, influencer partnerships and promotional events is crucial to attract new users to the game. Effective marketing enables the company to get the word out about their game to a wider audience, leading to increased user acquisition and revenue growth. Overall, by offering incentives, focusing on user experience, encouraging social interactions, utilizing analytics and investing in marketing, online game companies can achieve sustainable growth and profitability.", "label": "0", "is_selected": "0", "text": "For sustainable growth and profitability, online game companies are constantly carrying out various events to attract new game users, to maximize return users, and to minimize churn users in online games. Here are some strategies that online game companies can use to achieve sustainable growth and profitability: 1. Offer Incentives: Offering incentives to new users is a great way to attract them to your game. These incentives can be in the form of free items or game currency that will help them progress in the game faster. This will encourage them to stay engaged with the game and become return users. 2. Provide a Great User Experience: In order to maximize return users, it's important to provide a great user experience. This includes a user-friendly interface, immersive gameplay, regular updates, and fast loading times. By providing an enjoyable experience, users are more likely to keep coming back to your game. 3. Encourage Social Interactions: Social interactions are key to minimizing churn users. Encouraging users to play with their friends or join a community within the game can increase engagement and make the game more enjoyable. This will help to build a loyal user base who will keep coming back to the game. 4. Utilize Data Analytics: Data analytics can help online game companies identify user behavior patterns, preferences, and trends. Using this information, companies can make informed decisions about where to focus their efforts to maximize growth and profitability. 5. Invest in Marketing: Investing in marketing strategies such as advertising, influencer partnerships and promotional events is crucial to attract new users to the game. Effective marketing enables the company to get the word out about their game to a wider audience, leading to increased user acquisition and revenue growth. Overall, by offering incentives, focusing on user experience, encouraging social interactions, utilizing analytics and investing in marketing, online game companies can achieve sustainable growth and profitability."}
{"original_text": "Shallow Convolution Neural Network (CNN) is a time-tested tool for the information extraction from cancer pathology reports. Shallow CNN performs competitively on this task to other deep learning models including BERT, which holds the state-of-the-art for many NLP tasks. The main insight behind this eccentric phenomenon is that the information extraction from cancer pathology reports require only a small number of domain-specific text segments to perform the task, thus making the most of the texts and contexts excessive for the task. Shallow CNN model is well-suited to identify these key short text segments from the labeled training set; however, the identified text segments remain obscure to humans. In this study, we fill this gap by developing a model reduction tool to make a reliable connection between CNN filters and relevant text segments by discarding the spurious connections. We reduce the complexity of shallow CNN representation by approximating it with a linear transformation of n-gram presence representation with a non-negativity and sparsity prior on the transformation weights to obtain an interpretable model. Our approach bridge the gap between the conventionally perceived tradeoff boundary between accuracy on the one side and explainability on the other by model reduction.", "label": "1", "is_selected": "0", "text": "Shallow Convolution Neural Network (CNN) is a time-tested tool for the information extraction from cancer pathology reports. Shallow CNN performs competitively on this task to other deep learning models including BERT, which holds the state-of-the-art for many NLP tasks. The main insight behind this eccentric phenomenon is that the information extraction from cancer pathology reports require only a small number of domain-specific text segments to perform the task, thus making the most of the texts and contexts excessive for the task. Shallow CNN model is well-suited to identify these key short text segments from the labeled training set; however, the identified text segments remain obscure to humans. In this study, we fill this gap by developing a model reduction tool to make a reliable connection between CNN filters and relevant text segments by discarding the spurious connections. We reduce the complexity of shallow CNN representation by approximating it with a linear transformation of n-gram presence representation with a non-negativity and sparsity prior on the transformation weights to obtain an interpretable model. Our approach bridge the gap between the conventionally perceived tradeoff boundary between accuracy on the one side and explainability on the other by model reduction."}
{"original_text": "Currently, self-driving cars rely greatly on the Global Positioning System (GPS) infrastructure, albeit there is an increasing demand for alternative methods for GPS-denied environments. One of them is known as place recognition, which associates images of places with their corresponding positions. We previously proposed systems based on Weightless Neural Networks (WNN) to address this problem as a classification task. This encompasses solely one part of the global localization, which is not precise enough for driverless cars. Instead of just recognizing past places and outputting their poses, it is desired that a global localization system estimates the pose of current place images. In this paper, we propose to tackle this problem as follows. Firstly, given a live image, the place recognition system returns the most similar image and its pose. Then, given live and recollected images, a visual localization system outputs the relative camera pose represented by those images. To estimate the relative camera pose between the recollected and the current images, a Convolutional Neural Network (CNN) is trained with the two images as input and a relative pose vector as output. Together, these systems solve the global localization problem using the topological and metric information to approximate the current vehicle pose. The full approach is compared to a Real-Time Kinematic GPS system and a Simultaneous Localization and Mapping (SLAM) system. Experimental results show that the proposed approach correctly localizes a vehicle 90 of the time with a mean error of 1.20m compared to 1.12m of the SLAM system and 0.37m of the GPS, 89 of the time.", "label": "1", "is_selected": "0", "text": "Currently, self-driving cars rely greatly on the Global Positioning System (GPS) infrastructure, albeit there is an increasing demand for alternative methods for GPS-denied environments. One of them is known as place recognition, which associates images of places with their corresponding positions. We previously proposed systems based on Weightless Neural Networks (WNN) to address this problem as a classification task. This encompasses solely one part of the global localization, which is not precise enough for driverless cars. Instead of just recognizing past places and outputting their poses, it is desired that a global localization system estimates the pose of current place images. In this paper, we propose to tackle this problem as follows. Firstly, given a live image, the place recognition system returns the most similar image and its pose. Then, given live and recollected images, a visual localization system outputs the relative camera pose represented by those images. To estimate the relative camera pose between the recollected and the current images, a Convolutional Neural Network (CNN) is trained with the two images as input and a relative pose vector as output. Together, these systems solve the global localization problem using the topological and metric information to approximate the current vehicle pose. The full approach is compared to a Real-Time Kinematic GPS system and a Simultaneous Localization and Mapping (SLAM) system. Experimental results show that the proposed approach correctly localizes a vehicle 90 of the time with a mean error of 1.20m compared to 1.12m of the SLAM system and 0.37m of the GPS, 89 of the time."}
{"original_text": "In the last decade, social media has evolved as one of the leading platform to create, share, or exchange information; it is commonly used as a way for individuals to connect with others, build relationships, and share experiences. Businesses and organizations have also recognized the potential of social media as a marketing tool to engage with customers, drive traffic to their websites, and ultimately increase sales. However, with the increasing use of social media comes security concerns. Social media platforms are vulnerable to hackers and data breaches, and users may inadvertently share sensitive information or fall victim to fraud or phishing scams. To stay safe on social media, it is important to: 1. Strengthen your passwords: Use strong passwords with a combination of letters, numbers, and symbols. 2. Limit personal information: Be mindful of the personal information you share online, such as your full name, address, phone number, and date of birth. 3. Adjust privacy settings: Review and adjust the privacy settings on your social media accounts to control who can see your posts and personal information. 4. Beware of suspicious links: Avoid clicking on links from unknown sources or those that seem too good to be true. 5. Verify online friends and connections: Before accepting friend requests or connections from individuals you don't know, verify their identity and legitimacy. By following these tips, you can enjoy the benefits of social media while keeping yourself and your information safe.", "label": "0", "is_selected": "0", "text": "In the last decade, social media has evolved as one of the leading platform to create, share, or exchange information; it is commonly used as a way for individuals to connect with others, build relationships, and share experiences. Businesses and organizations have also recognized the potential of social media as a marketing tool to engage with customers, drive traffic to their websites, and ultimately increase sales. However, with the increasing use of social media comes security concerns. Social media platforms are vulnerable to hackers and data breaches, and users may inadvertently share sensitive information or fall victim to fraud or phishing scams. To stay safe on social media, it is important to: 1. Strengthen your passwords: Use strong passwords with a combination of letters, numbers, and symbols. 2. Limit personal information: Be mindful of the personal information you share online, such as your full name, address, phone number, and date of birth. 3. Adjust privacy settings: Review and adjust the privacy settings on your social media accounts to control who can see your posts and personal information. 4. Beware of suspicious links: Avoid clicking on links from unknown sources or those that seem too good to be true. 5. Verify online friends and connections: Before accepting friend requests or connections from individuals you don't know, verify their identity and legitimacy. By following these tips, you can enjoy the benefits of social media while keeping yourself and your information safe."}
{"original_text": "Aiming to minimize service delay, we propose a new random caching scheme in device-to-device (D2D) -assisted heterogeneous network. To support diversified viewing qualities of multimedia video services, each video file is encoded into a base layer (BL) and multiple enhancement layers (ELs) by scalable video coding (SVC). A super layer, including the BL and several ELs, is transmitted to every user. We define and quantify the service delay of multi-quality videos by deriving successful transmission probabilities when a user is served by a D2D helper, a small-cell base station (SBS) and a macro-cell base station (MBS). We formulate a delay minimization problem subject to the limited cache sizes of D2D helpers and SBSs. The structure of the optimal solutions to the problem is revealed, and then an improved standard gradient projection method is designed to effectively obtain the solutions. Both theoretical analysis and Monte-Carlo simulations validate the successful transmission probabilities. Compared with three benchmark caching policies, the proposed SVC-based random caching scheme is superior in terms of reducing the service delay.", "label": "1", "is_selected": "0", "text": "Aiming to minimize service delay, we propose a new random caching scheme in device-to-device (D2D) -assisted heterogeneous network. To support diversified viewing qualities of multimedia video services, each video file is encoded into a base layer (BL) and multiple enhancement layers (ELs) by scalable video coding (SVC). A super layer, including the BL and several ELs, is transmitted to every user. We define and quantify the service delay of multi-quality videos by deriving successful transmission probabilities when a user is served by a D2D helper, a small-cell base station (SBS) and a macro-cell base station (MBS). We formulate a delay minimization problem subject to the limited cache sizes of D2D helpers and SBSs. The structure of the optimal solutions to the problem is revealed, and then an improved standard gradient projection method is designed to effectively obtain the solutions. Both theoretical analysis and Monte-Carlo simulations validate the successful transmission probabilities. Compared with three benchmark caching policies, the proposed SVC-based random caching scheme is superior in terms of reducing the service delay."}
{"original_text": "It is a challenge to specify unambiguous distance (UD) in a phase-based ranging system with hopping frequencies (PRSHF). In this letter, we propose to characterize the UD in a PRSHF by the probability that it takes on its maximum value. We obtain a very simple and elegant expression of the probability with growth estimation techniques from analytic number theory. It is revealed that the UD in a PRSHF usually takes on the maximum value with as few as 10 frequencies in measurement, almost independent of the specific distribution of available bandwidth.", "label": "1", "is_selected": "0", "text": "It is a challenge to specify unambiguous distance (UD) in a phase-based ranging system with hopping frequencies (PRSHF). In this letter, we propose to characterize the UD in a PRSHF by the probability that it takes on its maximum value. We obtain a very simple and elegant expression of the probability with growth estimation techniques from analytic number theory. It is revealed that the UD in a PRSHF usually takes on the maximum value with as few as 10 frequencies in measurement, almost independent of the specific distribution of available bandwidth."}
{"original_text": "Recently, a new polynomial basis over binary extension fields was proposed such that the fast Fourier transform (FFT) over such fields can be computed in the complexity of order O (nlog (n. This new basis is known as the \"Ditributed Arithmetic\" (DA) basis. It has applications in areas such as digital signal processing, error-correcting codes, and cryptography. Would you like to know more about this topic?", "label": "0", "is_selected": "1", "text": "In our series of letters from African journalists, film-maker and columnist Ahmedou Ould-Abdallah looks at some of the most interesting recent contributions to the field of computer science."}
{"original_text": "We propose a validity preserving translation from a subset of epistemic Alternating-time Temporal Logic (ATL) to epistemic Computation Tree Logic (CTL). The considered subset of epistemic ATL is known to contain only formulas that can be expressed in epistemic CTL. Therefore, the translation will preserve the semantic equivalence between the original formula in epistemic ATL and its translated version in epistemic CTL, ensuring that the validity of the formula is not affected by the translation process. This allows for more efficient model checking and reasoning about complex systems that involve multiple agents and uncertainty. Additionally, the translation can enable the use of existing tools and techniques for epistemic CTL, which have been well-studied and developed over the years.", "label": "0", "is_selected": "1", "text": "In this paper, we present a novel approach to preserving the validity of formulas that are used in the analysis of systems with complex spatio-temporal patterns."}
{"original_text": "A picture is worth a thousand words. Albeit a cliche, for the fashion industry, an image of a clothing piece allows one to perceive its category (e.g., dress), sub-category (e.g., day dress) and properties (e.g., white colour with floral patterns). The seasonal nature of the fashion industry creates a highly dynamic and creative domain with evermore data, making it unpractical to manually describe a large set of images (of products). In this paper, we explore the concept of visual recognition for fashion images through an end-to-end architecture embedding the hierarchical nature of the annotations directly into the model. Towards that goal, and inspired by the work of (,), we have modified and adapted the original architecture proposal. Namely, we have removed the message passing layer symmetry to cope with Farfetch category tree, added extra layers for hierarchy level specificity, and moved the message passing layer into an enriched latent space. We compare the proposed unified architecture against state-of-the-art models and demonstrate the performance advantage of our model for structured multi-level categorization on a dataset of about 350k fashion product images.", "label": "1", "is_selected": "0", "text": "A picture is worth a thousand words. Albeit a cliche, for the fashion industry, an image of a clothing piece allows one to perceive its category (e.g., dress), sub-category (e.g., day dress) and properties (e.g., white colour with floral patterns). The seasonal nature of the fashion industry creates a highly dynamic and creative domain with evermore data, making it unpractical to manually describe a large set of images (of products). In this paper, we explore the concept of visual recognition for fashion images through an end-to-end architecture embedding the hierarchical nature of the annotations directly into the model. Towards that goal, and inspired by the work of (,), we have modified and adapted the original architecture proposal. Namely, we have removed the message passing layer symmetry to cope with Farfetch category tree, added extra layers for hierarchy level specificity, and moved the message passing layer into an enriched latent space. We compare the proposed unified architecture against state-of-the-art models and demonstrate the performance advantage of our model for structured multi-level categorization on a dataset of about 350k fashion product images."}
{"original_text": "We report the 'Recurrent Deterioration' (RD) phenomenon observed in online recommender systems. The RD phenomenon is reflected by the trend of performance degradation when the recommendation model is always trained on the most recent data, without taking into account historical data. This can happen when the recommendation model is updated frequently using only the most recent useritem interactions and ignoring the previous data, making the model unable to learn from past behavior of users and items. As a result, the recommendations become less accurate over time, leading to user dissatisfaction and decreased trust in the system. To avoid the RD phenomenon, it is important for the recommendation model to incorporate historical data and balance the importance of recent and past interactions in training. This can be done through techniques such as regularization, weighting, or using hybrid models that leverage both recent and past data.", "label": "0", "is_selected": "0", "text": "We report the 'Recurrent Deterioration' (RD) phenomenon observed in online recommender systems. The RD phenomenon is reflected by the trend of performance degradation when the recommendation model is always trained on the most recent data, without taking into account historical data. This can happen when the recommendation model is updated frequently using only the most recent useritem interactions and ignoring the previous data, making the model unable to learn from past behavior of users and items. As a result, the recommendations become less accurate over time, leading to user dissatisfaction and decreased trust in the system. To avoid the RD phenomenon, it is important for the recommendation model to incorporate historical data and balance the importance of recent and past interactions in training. This can be done through techniques such as regularization, weighting, or using hybrid models that leverage both recent and past data."}
{"original_text": "Recommender systems (RS) are increasingly present in our daily lives, especially since the advent of Big Data, which allows for storing all kinds of information about users' preferences. Personalized RS are successfully applied in platforms such as Netflix, Amazon or YouTube. However, they are missing in gastronomic platforms such as TripAdvisor, where moreover we can find millions of images tagged with users' tastes. This paper explores the potential of using those images as sources of information for modeling users' tastes and proposes an image-based classification system to obtain personalized recommendations, using a convolutional autoencoder as feature extractor. The proposed architecture will be applied to TripAdvisor data, using users' reviews that can be defined as a triad composed by a user, a restaurant, and an image of it taken by the user. Since the dataset is highly unbalanced, the use of data augmentation on the minority class is also considered in the experimentation. Results on data from three cities of different sizes (Santiago de Compostela, Barcelona and New York) demonstrate the effectiveness of using a convolutional autoencoder as feature extractor, instead of the standard deep features computed with convolutional neural networks.", "label": "1", "is_selected": "0", "text": "Recommender systems (RS) are increasingly present in our daily lives, especially since the advent of Big Data, which allows for storing all kinds of information about users' preferences. Personalized RS are successfully applied in platforms such as Netflix, Amazon or YouTube. However, they are missing in gastronomic platforms such as TripAdvisor, where moreover we can find millions of images tagged with users' tastes. This paper explores the potential of using those images as sources of information for modeling users' tastes and proposes an image-based classification system to obtain personalized recommendations, using a convolutional autoencoder as feature extractor. The proposed architecture will be applied to TripAdvisor data, using users' reviews that can be defined as a triad composed by a user, a restaurant, and an image of it taken by the user. Since the dataset is highly unbalanced, the use of data augmentation on the minority class is also considered in the experimentation. Results on data from three cities of different sizes (Santiago de Compostela, Barcelona and New York) demonstrate the effectiveness of using a convolutional autoencoder as feature extractor, instead of the standard deep features computed with convolutional neural networks."}
{"original_text": "In the image processing pipeline of almost every digital camera there is a part dedicated to computational color constancy i.e. to removing the influence of illumination on the colors of the captured image. This is done by using algorithms that analyze the statistical properties of the colors in the image and estimate the color of the illuminant, and then adjust the colors of the image accordingly to make them appear as if they were captured under a neutral white light source. Color constancy is important because it helps to ensure that the colors in the image accurately represent the scene as perceived by the human eye, and allows for better consistency between images captured under different lighting conditions.", "label": "0", "is_selected": "0", "text": "In the image processing pipeline of almost every digital camera there is a part dedicated to computational color constancy i.e. to removing the influence of illumination on the colors of the captured image. This is done by using algorithms that analyze the statistical properties of the colors in the image and estimate the color of the illuminant, and then adjust the colors of the image accordingly to make them appear as if they were captured under a neutral white light source. Color constancy is important because it helps to ensure that the colors in the image accurately represent the scene as perceived by the human eye, and allows for better consistency between images captured under different lighting conditions."}
{"original_text": "We study the problem of alleviating the instability issue in the GAN training procedure via new architecture design. The discrepancy between the minimax and maximin objective values could serve as an indicator of instability in GAN training. One approach to address this issue is to modify the generator and discriminator architectures, such as the use of normalization techniques and dropout layers. Another way to address instability is to add regularization terms to the objective function of the GAN, such as gradient penalties or spectral normalization. These regularization techniques have been shown to improve convergence and stability in GAN training. Overall, the goal is to ensure that the generator and discriminator are well-matched in their abilities to produce and distinguish between realistic and fake samples. By designing architectures and incorporating regularization techniques that promote stability in training, we can improve the performance of GANs in generating high-quality, realistic images.", "label": "0", "is_selected": "1", "text": "GANs have been shown to produce realistic images, but instability in their training procedures has been a major issue in their use in high-resolution image processing."}
{"original_text": "In this work, we explain the working mechanism of MixUp in terms of adversarial training. We introduce a new class of adversarial training schemes, which we refer to as directional adversarial training, or DAT. In a nutshell, a DAT scheme perturbs a training example in the direction of another example but keeps its original label as the training target. We prove that MixUp is equivalent to a special subclass of DAT, in that it has the same expected loss function and corresponds to the same optimization problem asymptotically. This understanding not only serves to explain the effectiveness of MixUp, but also reveals a more general family of MixUp schemes, which we call Untied MixUp. We prove that the family of Untied MixUp schemes is equivalent to the entire class of DAT schemes. We establish empirically the existence of Untied Mixup schemes which improve upon MixUp.", "label": "1", "is_selected": "0", "text": "In this work, we explain the working mechanism of MixUp in terms of adversarial training. We introduce a new class of adversarial training schemes, which we refer to as directional adversarial training, or DAT. In a nutshell, a DAT scheme perturbs a training example in the direction of another example but keeps its original label as the training target. We prove that MixUp is equivalent to a special subclass of DAT, in that it has the same expected loss function and corresponds to the same optimization problem asymptotically. This understanding not only serves to explain the effectiveness of MixUp, but also reveals a more general family of MixUp schemes, which we call Untied MixUp. We prove that the family of Untied MixUp schemes is equivalent to the entire class of DAT schemes. We establish empirically the existence of Untied Mixup schemes which improve upon MixUp."}
{"original_text": "Roundabouts in conjunction with other traffic scenarios, e.g., intersections, merging roadways, speed reduction zones, can induce congestion in a transportation network due to driver responses to various disturbances. Research efforts have been conducted to optimize and improve the performance of roundabouts in these scenarios. One area of research is focused on optimal design of roundabouts, taking into consideration the traffic demand, the number of exits and entrances, and the level of pedestrian and bicycle traffic. This can help to minimize congestion and improve safety. Another area of research is related to the use of roundabouts in conjunction with other traffic control measures, such as traffic signals and speed reduction zones. By integrating these measures, it is possible to reduce congestion and improve overall traffic flow. Additionally, simulation models have been developed to test and evaluate various roundabout designs and traffic control measures. These models can help to identify potential issues and optimize the performance of roundabouts in different traffic scenarios. Overall, ongoing research efforts are essential to improving and optimizing roundabouts as an effective traffic control measure.", "label": "0", "is_selected": "0", "text": "Roundabouts in conjunction with other traffic scenarios, e.g., intersections, merging roadways, speed reduction zones, can induce congestion in a transportation network due to driver responses to various disturbances. Research efforts have been conducted to optimize and improve the performance of roundabouts in these scenarios. One area of research is focused on optimal design of roundabouts, taking into consideration the traffic demand, the number of exits and entrances, and the level of pedestrian and bicycle traffic. This can help to minimize congestion and improve safety. Another area of research is related to the use of roundabouts in conjunction with other traffic control measures, such as traffic signals and speed reduction zones. By integrating these measures, it is possible to reduce congestion and improve overall traffic flow. Additionally, simulation models have been developed to test and evaluate various roundabout designs and traffic control measures. These models can help to identify potential issues and optimize the performance of roundabouts in different traffic scenarios. Overall, ongoing research efforts are essential to improving and optimizing roundabouts as an effective traffic control measure."}
{"original_text": "Catastrophic forgetting can be a significant problem for institutions that must delete historic data for privacy reasons. For example, hospitals might not be able to retain patient data permanently. But there are strategies that can be used to mitigate the effects of catastrophic forgetting while still adhering to privacy regulations. One approach is to use data anonymization techniques that remove personally identifiable information from the data. This allows researchers to still access the data and make use of it for analysis, while protecting the privacy of individuals. Additionally, some institutions may choose to retain a subset of the data that is particularly important or relevant for future analysis, while deleting the rest. It's important for organizations to carefully consider their policies and strategies for dealing with historic data deletion to ensure that they are both compliant with regulations and maximizing the potential benefits of the data.", "label": "0", "is_selected": "1", "text": "The US Department of Health and Human Services (HHS) has issued guidance on how to deal with \"catastrophic forgetting\" - the loss or destruction of data that is no longer needed or relevant."}
{"original_text": "This paper proposes a robust adversarial reinforcement learning (RARL) -based multi-access point (AP) coordination method that is robust even against unexpected decentralized operations of uncoordinated APs. Multi-AP coordination is a promising technique towards IEEE 802.11be, and there are studies that use RL for multi-AP coordination. Indeed, a simple RL-based multi-AP coordination method diminishes the collision probability among the APs; therefore, the method is a promising approach to improve time-resource efficiency. However, this method is vulnerable to frame transmissions of uncoordinated APs that are less aware of frame transmissions of other coordinated APs. To help the central agent experience even such unexpected frame transmissions, in addition to the central agent, the proposed method also competitively trains an adversarial AP that disturbs coordinated APs by causing frame collisions intensively. Besides, we propose to exploit a history of frame losses of a coordinated AP to promote reasonable competition between the central agent and adversarial AP. The simulation results indicate that the proposed method can avoid uncoordinated interference and thereby improve the minimum sum of the throughputs in the system compared to not considering the uncoordinated AP.", "label": "1", "is_selected": "0", "text": "This paper proposes a robust adversarial reinforcement learning (RARL) -based multi-access point (AP) coordination method that is robust even against unexpected decentralized operations of uncoordinated APs. Multi-AP coordination is a promising technique towards IEEE 802.11be, and there are studies that use RL for multi-AP coordination. Indeed, a simple RL-based multi-AP coordination method diminishes the collision probability among the APs; therefore, the method is a promising approach to improve time-resource efficiency. However, this method is vulnerable to frame transmissions of uncoordinated APs that are less aware of frame transmissions of other coordinated APs. To help the central agent experience even such unexpected frame transmissions, in addition to the central agent, the proposed method also competitively trains an adversarial AP that disturbs coordinated APs by causing frame collisions intensively. Besides, we propose to exploit a history of frame losses of a coordinated AP to promote reasonable competition between the central agent and adversarial AP. The simulation results indicate that the proposed method can avoid uncoordinated interference and thereby improve the minimum sum of the throughputs in the system compared to not considering the uncoordinated AP."}
{"original_text": "This paper considers a network of stochastic evidence accumulators, each represented by a drift-diffusion model accruing evidence towards a decision in continuous time by observing a noisy signal and by integrating incoming evidence in a probabilistic manner. The network is modeled as a system of interacting particles, where each particle represents an evidence accumulator. The particles interact through a social force that is proportional to the similarity of their accumulated evidence. This leads to emergent behaviors such as synchronization and cooperation among the particles. The paper is interesting because it provides a framework for studying how evidence accumulators interact and make decisions in a network setting, which has implications for understanding the behavior of large-scale social and economic systems. Additionally, the approach taken in the paper could be applied to modeling decision-making in other contexts, such as in the brain during perceptual tasks or in financial markets.", "label": "0", "is_selected": "0", "text": "This paper considers a network of stochastic evidence accumulators, each represented by a drift-diffusion model accruing evidence towards a decision in continuous time by observing a noisy signal and by integrating incoming evidence in a probabilistic manner. The network is modeled as a system of interacting particles, where each particle represents an evidence accumulator. The particles interact through a social force that is proportional to the similarity of their accumulated evidence. This leads to emergent behaviors such as synchronization and cooperation among the particles. The paper is interesting because it provides a framework for studying how evidence accumulators interact and make decisions in a network setting, which has implications for understanding the behavior of large-scale social and economic systems. Additionally, the approach taken in the paper could be applied to modeling decision-making in other contexts, such as in the brain during perceptual tasks or in financial markets."}
{"original_text": "In this paper a class of discrete optimization problems with uncertain costs is discussed. The uncertainty is modeled by introducing a scenario set containing a finite number of cost scenarios. To tackle this class of problems, robust optimization techniques are applied. Specifically, the goal is to find a solution that performs well across all scenarios in the set, rather than optimizing for a single scenario. This approach can lead to more reliable solutions that are less sensitive to changes in the underlying conditions. Some common approaches to robust optimization include minimizing the worst-case cost, minimizing the average cost, or minimizing a weighted combination of both worst-case and average costs. Another popular approach is to use a two-stage optimization framework, where in the first stage a decision is made without considering cost uncertainty, and in the second stage adjustments are made to the decision based on the actual cost scenario. Overall, robust optimization provides a useful framework for dealing with uncertain costs in discrete optimization problems. By considering a range of scenarios and focusing on solutions that perform well across all of them, this approach can help ensure robustness and reliability in the face of uncertainty.", "label": "0", "is_selected": "0", "text": "In this paper a class of discrete optimization problems with uncertain costs is discussed. The uncertainty is modeled by introducing a scenario set containing a finite number of cost scenarios. To tackle this class of problems, robust optimization techniques are applied. Specifically, the goal is to find a solution that performs well across all scenarios in the set, rather than optimizing for a single scenario. This approach can lead to more reliable solutions that are less sensitive to changes in the underlying conditions. Some common approaches to robust optimization include minimizing the worst-case cost, minimizing the average cost, or minimizing a weighted combination of both worst-case and average costs. Another popular approach is to use a two-stage optimization framework, where in the first stage a decision is made without considering cost uncertainty, and in the second stage adjustments are made to the decision based on the actual cost scenario. Overall, robust optimization provides a useful framework for dealing with uncertain costs in discrete optimization problems. By considering a range of scenarios and focusing on solutions that perform well across all of them, this approach can help ensure robustness and reliability in the face of uncertainty."}
{"original_text": "Adversarial sample attacks perturb benign inputs to induce DNN misbehaviors. Recent research has demonstrated the widespread presence and the devastating consequences of such attacks. Existing defense techniques either assume prior knowledge of specific attacks or may not work well on complex models due to their underlying assumptions. We argue that adversarial sample attacks are deeply entangled with interpretability of DNN models: while classification results on benign inputs can be reasoned based on the human perceptible featuresattributes, results on adversarial samples can hardly be explained. Therefore, we propose a novel adversarial sample detection technique for face recognition models, based on interpretability. It features a novel bi-directional correspondence inference between attributes and internal neurons to identify neurons critical for individual attributes. The activation values of critical neurons are enhanced to amplify the reasoning part of the computation and the values of other neurons are weakened to suppress the uninterpretable part. The classification results after such transformation are compared with those of the original model to detect adversaries. Results show that our technique can achieve 94 detection accuracy for 7 different kinds of attacks with 9.91 false positives on benign inputs. In contrast, a state-of-the-art feature squeezing technique can only achieve 55 accuracy with 23.3 false positives.", "label": "1", "is_selected": "0", "text": "Adversarial sample attacks perturb benign inputs to induce DNN misbehaviors. Recent research has demonstrated the widespread presence and the devastating consequences of such attacks. Existing defense techniques either assume prior knowledge of specific attacks or may not work well on complex models due to their underlying assumptions. We argue that adversarial sample attacks are deeply entangled with interpretability of DNN models: while classification results on benign inputs can be reasoned based on the human perceptible featuresattributes, results on adversarial samples can hardly be explained. Therefore, we propose a novel adversarial sample detection technique for face recognition models, based on interpretability. It features a novel bi-directional correspondence inference between attributes and internal neurons to identify neurons critical for individual attributes. The activation values of critical neurons are enhanced to amplify the reasoning part of the computation and the values of other neurons are weakened to suppress the uninterpretable part. The classification results after such transformation are compared with those of the original model to detect adversaries. Results show that our technique can achieve 94 detection accuracy for 7 different kinds of attacks with 9.91 false positives on benign inputs. In contrast, a state-of-the-art feature squeezing technique can only achieve 55 accuracy with 23.3 false positives."}
{"original_text": "Eigenvector continuation is a computational method that finds the extremal eigenvalues and eigenvectors of a Hamiltonian matrix with one or more control parameters. It does this by projection onto a continuation space, where the Hamiltonian matrix is transformed into a family of matrices parametrized by the control parameter. The method then follows the path of the eigenvectors and eigenvalues as the control parameter is varied, allowing for the identification of bifurcations and critical points in the system. Eigenvector continuation has applications in many fields, including physics, engineering, and biology. It can be used to study the behavior of dynamical systems, the stability of structures, and the properties of materials. Overall, eigenvector continuation is a powerful technique for exploring the behavior of complex systems and identifying critical points in parameter space.", "label": "0", "is_selected": "0", "text": "Eigenvector continuation is a computational method that finds the extremal eigenvalues and eigenvectors of a Hamiltonian matrix with one or more control parameters. It does this by projection onto a continuation space, where the Hamiltonian matrix is transformed into a family of matrices parametrized by the control parameter. The method then follows the path of the eigenvectors and eigenvalues as the control parameter is varied, allowing for the identification of bifurcations and critical points in the system. Eigenvector continuation has applications in many fields, including physics, engineering, and biology. It can be used to study the behavior of dynamical systems, the stability of structures, and the properties of materials. Overall, eigenvector continuation is a powerful technique for exploring the behavior of complex systems and identifying critical points in parameter space."}
{"original_text": "Architecture search is the process of automatically learning the neural model or cell structure that best suits the given task. Recently, this approach has shown promising performance improvements (on language models, image recognition, and other tasks) compared to handcrafted architectures. Architecture search methods use techniques like reinforcement learning, evolutionary algorithms, and gradient-based methods to learn the optimal model architecture. The goal of architecture search is to automate the process of selecting the best neural architecture for a given task, reducing the need for expert knowledge and manual tuning. This can save time and resources and also lead to more efficient and accurate models. There are different types of architecture search methods, including discrete, continuous, and hybrid approaches. Discrete search involves selecting the best model from a predefined set of candidate architectures, while continuous search involves optimizing over a continuous space of possible architectures. Hybrid search methods combine both discrete and continuous search techniques. Overall, architecture search has the potential to revolutionize the field of machine learning by enabling the automatic discovery of optimal neural architectures for various tasks.", "label": "0", "is_selected": "1", "text": "In this paper, we present an overview of architecture search methods, and discuss their potential to improve the performance of various machine learning tasks, such as image recognition, language learning, and deep learning."}
{"original_text": "With wearable devices such as smartwatches on the rise in the consumer electronics market, securing these wearables is vital. However, the current security mechanisms only focus on validating the user not the device itself. Indeed, wearables can be (1) unauthorized wearable devices with correct credentials accessing valuable systems and networks, (2) passive insiders or outsider wearable devices, or (3) information-leaking wearables devices. Fingerprinting via machine learning can provide necessary cyber threat intelligence to address all these cyber attacks. In this work, we introduce a wearable fingerprinting technique focusing on Bluetooth classic protocol, which is a common protocol used by the wearables and other IoT devices. Specifically, we propose a non-intrusive wearable device identification framework which utilizes 20 different Machine Learning (ML) algorithms in the training phase of the classification process and selects the best performing algorithm for the testing phase. Furthermore, we evaluate the performance of proposed wearable fingerprinting technique on real wearable devices, including various off-the-shelf smartwatches. Our evaluation demonstrates the feasibility of the proposed technique to provide reliable cyber threat intelligence. Specifically, our detailed accuracy results show on average 98.5, 98.3 precision and recall for identifying wearables using the Bluetooth classic protocol.", "label": "1", "is_selected": "0", "text": "With wearable devices such as smartwatches on the rise in the consumer electronics market, securing these wearables is vital. However, the current security mechanisms only focus on validating the user not the device itself. Indeed, wearables can be (1) unauthorized wearable devices with correct credentials accessing valuable systems and networks, (2) passive insiders or outsider wearable devices, or (3) information-leaking wearables devices. Fingerprinting via machine learning can provide necessary cyber threat intelligence to address all these cyber attacks. In this work, we introduce a wearable fingerprinting technique focusing on Bluetooth classic protocol, which is a common protocol used by the wearables and other IoT devices. Specifically, we propose a non-intrusive wearable device identification framework which utilizes 20 different Machine Learning (ML) algorithms in the training phase of the classification process and selects the best performing algorithm for the testing phase. Furthermore, we evaluate the performance of proposed wearable fingerprinting technique on real wearable devices, including various off-the-shelf smartwatches. Our evaluation demonstrates the feasibility of the proposed technique to provide reliable cyber threat intelligence. Specifically, our detailed accuracy results show on average 98.5, 98.3 precision and recall for identifying wearables using the Bluetooth classic protocol."}
{"original_text": "Shallow Convolution Neural Network (CNN) is a time-tested tool for the information extraction from cancer pathology reports. Shallow CNN performs competitively on this task to other deep learning models including the popular deep CNN architectures such as ResNet and Inception. Shallow CNN can be efficient when the dataset size is limited and computational resources are constrained. It can be trained quickly on smaller datasets and still produce accurate results. However, if the dataset is large and diverse, a deeper CNN model might be better suited to capture more complex features and patterns in the data. It is important to choose the appropriate model architecture based on factors such as dataset size, complexity, and computational resources.", "label": "0", "is_selected": "1", "text": "In this paper, we show how to choose the right deep learning model architecture for a particular dataset. It is important to choose an appropriate model architecture based on factors such as dataset size, complexity, and computational resources."}
{"original_text": "In this paper, we present a novel approach to machine reading comprehension for the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a question with exact text spans I see, so the authors are proposing a new method for machine reading comprehension specifically for the MS-MARCO dataset. They are noting that this dataset is different from the SQuAD dataset in that SQuAD requires exact text span answers to a given question.", "label": "0", "is_selected": "0", "text": "In this paper, we present a novel approach to machine reading comprehension for the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a question with exact text spans I see, so the authors are proposing a new method for machine reading comprehension specifically for the MS-MARCO dataset. They are noting that this dataset is different from the SQuAD dataset in that SQuAD requires exact text span answers to a given question."}
{"original_text": "Many large datasets exhibit power-law statistics: The web graph, social networks, text data, clickthrough data etc. Their adjacency graphs are termed natural graphs, and are known to be difficult to analyze and process efficiently. This is because natural graphs tend to be sparse, meaning they have relatively few edges compared to the total number of possible edges. Additionally, the vertices in natural graphs tend to have widely varying degrees, with some vertices having a large number of neighbors while others have very few. These characteristics make it challenging to perform typical graph operations such as finding shortest paths, clustering, and identifying communities. There are several techniques that have been developed to help process and analyze natural graphs efficiently. One such technique is graph partitioning, which involves dividing the graph into smaller subgraphs that can be processed separately. This is often done by identifying clusters of vertices that are densely connected, and then separating these clusters into their own subgraphs. Another technique is to use specialized algorithms that are designed specifically for natural graphs, such as the PageRank algorithm used by Google to rank web pages based on their importance. Overall, processing and analyzing natural graphs is a complex task that requires specialized algorithms and techniques. As the volume of data continues to grow, it is becoming increasingly important to develop efficient methods for working with natural graphs in order to extract valuable insights and knowledge from these datasets.", "label": "0", "is_selected": "1", "text": "Power-law analysis is the study of the relationship between the number of edges in a graph, and the power of that graph to predict future events."}
{"original_text": "3D point cloud semantic and instance segmentation is crucial and fundamental for 3D scene understanding. Due to the complex structure, point sets are distributed off balance and diversely, which appears to be a significant obstacle for achieving accurate semantic and instance segmentation results. Recent advances in deep learning techniques have shown promising results in addressing this challenge. One such method is PointNet, which is an extension of the original PointNet model that is specifically designed for point cloud segmentation tasks. Another approach is the PointCNN model, which utilizes a convolutional neural network to extract features from point clouds and is capable of handling non-uniform distributions of points. Additionally, graph-based methods such as Graph Convolutional Networks (GCNs) have been proposed for point cloud segmentation tasks, which utilize the point adjacency graph to model the relationship between points. These methods are transforming the field of 3D scene understanding and are playing a crucial role in advancing research in areas such as autonomous driving, robotics, and virtual reality.", "label": "0", "is_selected": "1", "text": "In this paper, we present a new approach to 3D point cloud segmentation, which is based on the use of graph-based methods and deep learning techniques."}
{"original_text": "In this paper, we study integrated estimation and control of soft robots. A significant challenge in deploying closed loop controllers is reliable proprioception via integrated sensing in soft robots. Despite the considerable advances accomplished in fabrication, modelling, and model-based control of soft robots, integrated sensing and estimation is still in its infancy. To that end, this paper introduces a new method of estimating the degree of curvature of a soft robot using a stretchable sensing skin. The skin is a spray-coated piezoresistive sensing layer on a latex membrane. The mapping from the strain signal to the degree of curvature is estimated by using a recurrent neural network. We investigate uni-directional bending as well as bi-directional bending of a single-segment soft robot. Moreover, an adaptive controller is developed to track the degree of curvature of the soft robot in the presence of dynamic uncertainties. Subsequently, using the integrated soft sensing skin, we experimentally demonstrate successful curvature tracking control of the soft robot.", "label": "1", "is_selected": "0", "text": "In this paper, we study integrated estimation and control of soft robots. A significant challenge in deploying closed loop controllers is reliable proprioception via integrated sensing in soft robots. Despite the considerable advances accomplished in fabrication, modelling, and model-based control of soft robots, integrated sensing and estimation is still in its infancy. To that end, this paper introduces a new method of estimating the degree of curvature of a soft robot using a stretchable sensing skin. The skin is a spray-coated piezoresistive sensing layer on a latex membrane. The mapping from the strain signal to the degree of curvature is estimated by using a recurrent neural network. We investigate uni-directional bending as well as bi-directional bending of a single-segment soft robot. Moreover, an adaptive controller is developed to track the degree of curvature of the soft robot in the presence of dynamic uncertainties. Subsequently, using the integrated soft sensing skin, we experimentally demonstrate successful curvature tracking control of the soft robot."}
{"original_text": "We analyze the adversarial examples problem in terms of a model's fault tolerance with respect to its input. Whereas previous work focuses on arbitrarily strict threat models, i.e., -perturbations, we propose a more realistic framework that considers a range of perturbations that are likely to occur in practice. We define fault tolerance as the ability of a model to maintain its performance despite such perturbations. This framework allows us to evaluate the robustness of different models and compare their performance under various types and levels of perturbations. To implement this framework, we first define a set of perturbation types and levels corresponding to commonly occurring scenarios in real-world use cases. These scenarios may include, for example, noise in image data due to sensor errors, variations in speech patterns due to accents or different speakers, or typos in natural language inputs. We then perturb the input data by applying these scenarios in varying degrees and measure the resulting accuracy of the model. The fault tolerance of the model is then computed as the percentage of scenarios under which it maintains a desirable accuracy threshold. Our approach allows for a more realistic evaluation of model robustness and could help in designing more fault-tolerant machine learning systems. By considering practical threat models rather than arbitrary ones, we can better prepare models for real-world deployment scenarios where unexpected perturbations are likely to occur.", "label": "0", "is_selected": "1", "text": "In this paper, we present a new approach to evaluate the robustness of machine learning models that are likely to be affected by real-world input perturbations."}
{"original_text": "User intent detection plays a critical role in question-answering and dialog systems. Most previous works treat intent detection as a classification problem where utterances are labeled with predefined intents. However, it is labor-intensive and time-consuming to label users' utterances as intents are diversely expressed and novel intents will continually be involved. Instead, we study the zero-shot intent detection problem, which aims to detect emerging user intents where no labeled utterances are currently available. We propose two capsule-based architectures: IntentCapsNet that extracts semantic features from utterances and aggregates them to discriminate existing intents, and IntentCapsNet-ZSL which gives IntentCapsNet the zero-shot learning ability to discriminate emerging intents via knowledge transfer from existing intents. Experiments on two real-world datasets show that our model not only can better discriminate diversely expressed existing intents, but is also able to discriminate emerging intents when no labeled utterances are available.", "label": "1", "is_selected": "0", "text": "User intent detection plays a critical role in question-answering and dialog systems. Most previous works treat intent detection as a classification problem where utterances are labeled with predefined intents. However, it is labor-intensive and time-consuming to label users' utterances as intents are diversely expressed and novel intents will continually be involved. Instead, we study the zero-shot intent detection problem, which aims to detect emerging user intents where no labeled utterances are currently available. We propose two capsule-based architectures: IntentCapsNet that extracts semantic features from utterances and aggregates them to discriminate existing intents, and IntentCapsNet-ZSL which gives IntentCapsNet the zero-shot learning ability to discriminate emerging intents via knowledge transfer from existing intents. Experiments on two real-world datasets show that our model not only can better discriminate diversely expressed existing intents, but is also able to discriminate emerging intents when no labeled utterances are available."}
{"original_text": "We propose a way to learn visual features that are compatible with previously computed ones even when they have different dimensions and are learned via different neural network architectures and training objectives. The proposed method is called Multi-Task Compatibility Learning (MTCL) and it involves learning a compatibility function between the feature spaces of two or more neural networks. The compatibility function is learned in a multitask learning framework where the primary task is to learn the visual features using each network's architecture and objectives. To do this, we first select a set of reference features that serve as a bridge between the different feature spaces. These reference features can be handcrafted features or features learned by a separate neural network. We then use MTCL to learn the compatibility function between the reference features and the features learned by each network. MTCL leverages the relationship between the feature spaces by forcing both networks to produce compatible features, which leads to higher-quality features than those produced by a single network alone. We can evaluate the compatibility between the features by measuring the similarity between their representations for the same input. Additionally, we can use the learned compatibility function to transfer the knowledge contained in one network's features to another, enabling transfer learning between networks trained on different tasks. MTCL is a flexible and effective framework that can be used to learn compatible features from any number of neural networks. It can enable the creation of more robust and effective models that can leverage the strengths of multiple networks.", "label": "0", "is_selected": "0", "text": "We propose a way to learn visual features that are compatible with previously computed ones even when they have different dimensions and are learned via different neural network architectures and training objectives. The proposed method is called Multi-Task Compatibility Learning (MTCL) and it involves learning a compatibility function between the feature spaces of two or more neural networks. The compatibility function is learned in a multitask learning framework where the primary task is to learn the visual features using each network's architecture and objectives. To do this, we first select a set of reference features that serve as a bridge between the different feature spaces. These reference features can be handcrafted features or features learned by a separate neural network. We then use MTCL to learn the compatibility function between the reference features and the features learned by each network. MTCL leverages the relationship between the feature spaces by forcing both networks to produce compatible features, which leads to higher-quality features than those produced by a single network alone. We can evaluate the compatibility between the features by measuring the similarity between their representations for the same input. Additionally, we can use the learned compatibility function to transfer the knowledge contained in one network's features to another, enabling transfer learning between networks trained on different tasks. MTCL is a flexible and effective framework that can be used to learn compatible features from any number of neural networks. It can enable the creation of more robust and effective models that can leverage the strengths of multiple networks."}
{"original_text": "Most theoretical frameworks that focus on data errors and inconsistencies follow logic-based reasoning. Yet, practical data cleaning tools need to incorporate statistical reasoning to be effective in real-world data cleaning tasks. This is because statistical reasoning allows for the detection of patterns and anomalies in data that may not be easily detected using logic-based reasoning alone. Additionally, statistical reasoning can be used to identify outliers and missing data, which are common issues in real-world datasets. Therefore, a good data cleaning tool should incorporate both logic-based and statistical reasoning to effectively address errors and inconsistencies in data. By combining these two approaches, the tool can identify a wider range of issues and provide users with more accurate and reliable data sets.", "label": "0", "is_selected": "0", "text": "Most theoretical frameworks that focus on data errors and inconsistencies follow logic-based reasoning. Yet, practical data cleaning tools need to incorporate statistical reasoning to be effective in real-world data cleaning tasks. This is because statistical reasoning allows for the detection of patterns and anomalies in data that may not be easily detected using logic-based reasoning alone. Additionally, statistical reasoning can be used to identify outliers and missing data, which are common issues in real-world datasets. Therefore, a good data cleaning tool should incorporate both logic-based and statistical reasoning to effectively address errors and inconsistencies in data. By combining these two approaches, the tool can identify a wider range of issues and provide users with more accurate and reliable data sets."}
{"original_text": "Quantum Clustering is a powerful method to detect clusters in data with mixed density. However, it is very sensitive to a length parameter that is inherent to the Schrodinger equation. In addition, linking data points into clusters requires local estimates of covariance that are also controlled by length parameters. This raises the question of how to adjust the control parameters of the Schrodinger equation for optimal clustering. We propose a probabilistic framework that provides an objective function for the goodness-of-fit to the data, enabling the control parameters to be optimised within a Bayesian framework. This naturally yields probabilities of cluster membership and data partitions with specific numbers of clusters. The proposed framework is tested on real and synthetic data sets, assessing its validity by measuring concordance with known data structure by means of the Jaccard score (JS). This work also proposes an objective way to measure performance in unsupervised learning that correlates very well with JS.", "label": "1", "is_selected": "0", "text": "Quantum Clustering is a powerful method to detect clusters in data with mixed density. However, it is very sensitive to a length parameter that is inherent to the Schrodinger equation. In addition, linking data points into clusters requires local estimates of covariance that are also controlled by length parameters. This raises the question of how to adjust the control parameters of the Schrodinger equation for optimal clustering. We propose a probabilistic framework that provides an objective function for the goodness-of-fit to the data, enabling the control parameters to be optimised within a Bayesian framework. This naturally yields probabilities of cluster membership and data partitions with specific numbers of clusters. The proposed framework is tested on real and synthetic data sets, assessing its validity by measuring concordance with known data structure by means of the Jaccard score (JS). This work also proposes an objective way to measure performance in unsupervised learning that correlates very well with JS."}
{"original_text": "We revisit the complexity of the classical k - Coloring problem parameterized by clique-width. This is a very well-studied problem that becomes highly intractable when the number of colors k is large. However, much less is known on its complexity for small, concrete values of k. In this paper, we completely determine the complexity of k - Coloring parameterized by clique-width for any fixed k, under the SETH. Specifically, we show that for all k 3, 0, k - Coloring cannot be solved in time O 2 k 2) cw), and give an algorithm running in time O 2 k 2) cw). Thus, if the SETH is true, - 2 k 2 is the \"correct\" base of the exponent for every k. Along the way, we also consider the complexity of k - Coloring parameterized by the related parameter modular treewidth (mtw). In this case we show that the \"correct\" running time, under the SETH, is O k k 2) mtw). If we base our results on a weaker assumption (the ETH), they imply that k - Coloring cannot be solved in time n o (cw), even on instances with O (log n) colors.", "label": "1", "is_selected": "0", "text": "We revisit the complexity of the classical k - Coloring problem parameterized by clique-width. This is a very well-studied problem that becomes highly intractable when the number of colors k is large. However, much less is known on its complexity for small, concrete values of k. In this paper, we completely determine the complexity of k - Coloring parameterized by clique-width for any fixed k, under the SETH. Specifically, we show that for all k 3, 0, k - Coloring cannot be solved in time O 2 k 2) cw), and give an algorithm running in time O 2 k 2) cw). Thus, if the SETH is true, - 2 k 2 is the \"correct\" base of the exponent for every k. Along the way, we also consider the complexity of k - Coloring parameterized by the related parameter modular treewidth (mtw). In this case we show that the \"correct\" running time, under the SETH, is O k k 2) mtw). If we base our results on a weaker assumption (the ETH), they imply that k - Coloring cannot be solved in time n o (cw), even on instances with O (log n) colors."}
{"original_text": "The contemporary literature on cloud resource allocation is mostly focused on studying the interactions between customers and cloud managers. Nevertheless, the recent growth in the customers' demands and the emergence of new cloud service models has made cloud resource allocation a critical issue. Several research works have focused on optimizing cloud resource allocation by considering different aspects such as energy consumption, cost minimization, load balancing, and service level agreements (SLAs) fulfillment. Some important techniques used in this domain include 1. Dynamic Resource Allocation techniques: These techniques are used to optimize resource allocation in real-time based on the current workload. The allocation decisions are driven by predictive or reactive approaches. 2. Multi-objective Resource Allocation techniques: These techniques allow optimizing resource allocation according to multiple objectives simultaneously. The objective functions can be defined based on factors such as cost, performance, and energy consumption. 3. Reinforcement Learning-based Resource Allocation techniques: These techniques involve the use of reinforcement learning algorithms to optimize resource allocation decisions in a dynamic environment. The algorithm learns from past decisions and adjusts the resource allocation based on the current situation. 4. Auction-based Resource Allocation techniques: In these techniques, customers bid for resources, and the cloud manager allocates resources to customers based on their bids. This approach ensures fair allocation of resources, as customers bid according to their willingness to pay. Overall, cloud resource allocation is a critical area of research, and it requires constant innovation and improvement to ensure optimal use of cloud resources while reducing costs and fulfilling service level agreements.", "label": "0", "is_selected": "0", "text": "The contemporary literature on cloud resource allocation is mostly focused on studying the interactions between customers and cloud managers. Nevertheless, the recent growth in the customers' demands and the emergence of new cloud service models has made cloud resource allocation a critical issue. Several research works have focused on optimizing cloud resource allocation by considering different aspects such as energy consumption, cost minimization, load balancing, and service level agreements (SLAs) fulfillment. Some important techniques used in this domain include 1. Dynamic Resource Allocation techniques: These techniques are used to optimize resource allocation in real-time based on the current workload. The allocation decisions are driven by predictive or reactive approaches. 2. Multi-objective Resource Allocation techniques: These techniques allow optimizing resource allocation according to multiple objectives simultaneously. The objective functions can be defined based on factors such as cost, performance, and energy consumption. 3. Reinforcement Learning-based Resource Allocation techniques: These techniques involve the use of reinforcement learning algorithms to optimize resource allocation decisions in a dynamic environment. The algorithm learns from past decisions and adjusts the resource allocation based on the current situation. 4. Auction-based Resource Allocation techniques: In these techniques, customers bid for resources, and the cloud manager allocates resources to customers based on their bids. This approach ensures fair allocation of resources, as customers bid according to their willingness to pay. Overall, cloud resource allocation is a critical area of research, and it requires constant innovation and improvement to ensure optimal use of cloud resources while reducing costs and fulfilling service level agreements."}
{"original_text": "This paper offers a multi-disciplinary review of knowledge acquisition methods in human activity systems. The review captures the degree of involvement of various types of agencies in the knowledge acquisition process, and proposes a classification with three categories of methods: the human agent, the human-inspired agent, and the autonomous machine agent methods. In the first two categories, the acquisition of knowledge is seen as a cognitive task analysis exercise, while in the third category knowledge acquisition is treated as an autonomous knowledge-discovery endeavour. The motivation for this classification stems from the continuous change over time of the structure, meaning and purpose of human activity systems, which are seen as the factor that fuelled researchers' and practitioners' efforts in knowledge acquisition for more than a century. We show through this review that the KA field is increasingly active due to the higher and higher pace of change in human activity, and conclude by discussing the emergence of a fourth category of knowledge acquisition methods, which are based on red-teaming and co-evolution.", "label": "1", "is_selected": "0", "text": "This paper offers a multi-disciplinary review of knowledge acquisition methods in human activity systems. The review captures the degree of involvement of various types of agencies in the knowledge acquisition process, and proposes a classification with three categories of methods: the human agent, the human-inspired agent, and the autonomous machine agent methods. In the first two categories, the acquisition of knowledge is seen as a cognitive task analysis exercise, while in the third category knowledge acquisition is treated as an autonomous knowledge-discovery endeavour. The motivation for this classification stems from the continuous change over time of the structure, meaning and purpose of human activity systems, which are seen as the factor that fuelled researchers' and practitioners' efforts in knowledge acquisition for more than a century. We show through this review that the KA field is increasingly active due to the higher and higher pace of change in human activity, and conclude by discussing the emergence of a fourth category of knowledge acquisition methods, which are based on red-teaming and co-evolution."}
{"original_text": "Pufferfish is a Bayesian privacy framework for designing and analyzing privacy mechanisms. It refines differential privacy, the current gold standard in data privacy, by allowing explicit prior knowledge in privacy analysis. Through these privacy frameworks, a number of privacy mechanisms have been developed in literature. In practice, privacy mechanisms often need be modified or adjusted to specific applications. Their privacy risks have to be re-evaluated for different circumstances. Moreover, computing devices only approximate continuous noises through floating-point computation, which is discrete in nature. Privacy proofs can thus be complicated and prone to errors. Such tedious tasks can be burdensome to average data curators. In this paper, we propose an automatic verification technique for Pufferfish privacy. We use hidden Markov models to specify and analyze discretized Pufferfish privacy mechanisms. We show that the Pufferfish verification problem in hidden Markov models is NP-hard. Using Satisfiability Modulo Theories solvers, we propose an algorithm to analyze privacy requirements. We implement our algorithm in a prototypical tool called FAIER, and present several case studies. Surprisingly, our case studies show that naive discretization of well-established privacy mechanisms often fail, witnessed by counterexamples generated by FAIER. In discretized Above Threshold, we show that it results in absolutely no privacy. Finally, we compare our approach with testing based approach on several case studies, and show that our verification technique can be combined with testing based approach for the purpose of (i) efficiently certifying counterexamples and (ii) obtaining a better lower bound for the privacy budget .", "label": "1", "is_selected": "0", "text": "Pufferfish is a Bayesian privacy framework for designing and analyzing privacy mechanisms. It refines differential privacy, the current gold standard in data privacy, by allowing explicit prior knowledge in privacy analysis. Through these privacy frameworks, a number of privacy mechanisms have been developed in literature. In practice, privacy mechanisms often need be modified or adjusted to specific applications. Their privacy risks have to be re-evaluated for different circumstances. Moreover, computing devices only approximate continuous noises through floating-point computation, which is discrete in nature. Privacy proofs can thus be complicated and prone to errors. Such tedious tasks can be burdensome to average data curators. In this paper, we propose an automatic verification technique for Pufferfish privacy. We use hidden Markov models to specify and analyze discretized Pufferfish privacy mechanisms. We show that the Pufferfish verification problem in hidden Markov models is NP-hard. Using Satisfiability Modulo Theories solvers, we propose an algorithm to analyze privacy requirements. We implement our algorithm in a prototypical tool called FAIER, and present several case studies. Surprisingly, our case studies show that naive discretization of well-established privacy mechanisms often fail, witnessed by counterexamples generated by FAIER. In discretized Above Threshold, we show that it results in absolutely no privacy. Finally, we compare our approach with testing based approach on several case studies, and show that our verification technique can be combined with testing based approach for the purpose of (i) efficiently certifying counterexamples and (ii) obtaining a better lower bound for the privacy budget ."}
{"original_text": "Lekkerkerker and Boland characterized the minimal forbidden induced subgraphs for the class of interval graphs. We give a linear-time algorithm to find one in any graph that is not an interval graph. The algorithm works by first finding a chordless cycle of length at least 4 in the graph, if one exists. If no such cycle exists, then the graph is an interval graph and there is nothing to do. Otherwise, we take any edge on the cycle, say uv, and consider the induced subgraph on the set of vertices strictly between u and v on the cycle. This induced subgraph is not an interval graph, and thus must contain one of the minimal forbidden induced subgraphs characterized by Lekkerkerker and Boland. We can then use a recursive approach to find one of these minimal forbidden subgraphs. Specifically, we repeatedly apply the above procedure to the induced subgraph until either an interval graph is found or we reach a minimal forbidden subgraph, in which case we output that subgraph. This algorithm runs in linear time because every recursive call reduces the size of the input graph by at least one vertex, and there are only finitely many minimal forbidden subgraphs to check. Therefore, the algorithm will terminate after at most n recursive calls, where n is the number of vertices in the input graph.", "label": "0", "is_selected": "0", "text": "Lekkerkerker and Boland characterized the minimal forbidden induced subgraphs for the class of interval graphs. We give a linear-time algorithm to find one in any graph that is not an interval graph. The algorithm works by first finding a chordless cycle of length at least 4 in the graph, if one exists. If no such cycle exists, then the graph is an interval graph and there is nothing to do. Otherwise, we take any edge on the cycle, say uv, and consider the induced subgraph on the set of vertices strictly between u and v on the cycle. This induced subgraph is not an interval graph, and thus must contain one of the minimal forbidden induced subgraphs characterized by Lekkerkerker and Boland. We can then use a recursive approach to find one of these minimal forbidden subgraphs. Specifically, we repeatedly apply the above procedure to the induced subgraph until either an interval graph is found or we reach a minimal forbidden subgraph, in which case we output that subgraph. This algorithm runs in linear time because every recursive call reduces the size of the input graph by at least one vertex, and there are only finitely many minimal forbidden subgraphs to check. Therefore, the algorithm will terminate after at most n recursive calls, where n is the number of vertices in the input graph."}
{"original_text": "In this work, we demonstrate that receptive fields in 3D pose estimation can be effectively specified using optical flow. We introduce adaptive receptive fields, a simple and effective method to specify the receptive fields in 3D pose estimation models. This method uses optical flow to estimate the size and shape of the receptive field at each point in the image. By adaptively adjusting the size and shape of the receptive field, our model is able to better capture the local features of the image and improve the accuracy of the 3D pose estimation. Our experiments show that using adaptive receptive fields significantly improves the performance of existing 3D pose estimation models on various benchmark datasets. This approach can be applied to other computer vision tasks as well, where effective specification of receptive fields can lead to better performance.", "label": "0", "is_selected": "1", "text": "3D pose estimation is an important computer vision task, where local features of the image, such as the receptive field, are important for the estimation of the pose."}
{"original_text": "Compromised social media accounts are legitimate user accounts that have been hijacked by a malicious party and can cause various kinds of damage, which makes the detection of these accounts and their recovery very important. Some potential impacts of compromised social media accounts include: 1. Sharing of malicious links or content, which can harm the reputation of the legitimate user. 2. Stealing personal information or financial data, which can result in identity theft or financial loss. 3. Spreading spam or fake news, which can mislead others and impact public perception. 4. Sending phishing messages, which can trick users into providing sensitive information. 5. Engaging in illegal or unethical activity, which can lead to legal consequences. To detect a compromised social media account, users should regularly monitor their accounts for any unauthorized activity, such as changes to account settings or unexpected messages or posts. It is also important to enable two-factor authentication, use strong passwords, and avoid clicking on suspicious links. If any suspicious activity is detected, users should change their passwords immediately and report the incident to the social media platform's support team. Recovery of a compromised social media account typically involves resetting the password and implementing additional security measures, such as reviewing authorized applications and revoking access for any suspicious ones. It is also important to keep a record of any unauthorized activity and report it to law enforcement if necessary.", "label": "0", "is_selected": "1", "text": "In the wake of the Russian government's alleged interference in the 2016 US presidential election, cybersecurity experts are warning users to be aware of the risks posed by compromised social media accounts."}
{"original_text": "We present the Latvian Twitter Eater Corpus - a set of tweets in the narrow domain related to food, drinks, eating and drinking. The corpus has been collected over time-span of over 8 years and includes over 2 million tweets entailed with additional useful data. We also separate two sub-corpora of question and answer tweets and sentiment annotated tweets. We analyse contents of the corpus and demonstrate use-cases for the sub-corpora by training domain-specific question-answering and sentiment-analysis models using data from the corpus.", "label": "1", "is_selected": "0", "text": "We present the Latvian Twitter Eater Corpus - a set of tweets in the narrow domain related to food, drinks, eating and drinking. The corpus has been collected over time-span of over 8 years and includes over 2 million tweets entailed with additional useful data. We also separate two sub-corpora of question and answer tweets and sentiment annotated tweets. We analyse contents of the corpus and demonstrate use-cases for the sub-corpora by training domain-specific question-answering and sentiment-analysis models using data from the corpus."}
{"original_text": "Quantum annealing (QA) is a quantum computing algorithm that works on the principle of Adiabatic Quantum Computation (AQC), and it has shown significant computational advantages in solving combinatorial optimization problems That's correct! QA is a type of algorithm that utilizes the principles of quantum mechanics to solve optimization problems, especially those that involve finding the minimum or maximum value of a function with many variables. Compared to classical optimization algorithms, QA has the potential to provide faster and more accurate results for these problems.", "label": "0", "is_selected": "1", "text": "In our series of letters from African journalists, film-maker and columnist Ahmed Rashid looks at some of the most frequently asked questions about the continent."}
{"original_text": "A track layout of a graph consists of a vertex coloring and a total order of each color class, such that no two edges cross between any two color classes. That's correct! Let me provide a bit more explanation to clarify: In a track layout of a graph, we color all of the vertices with k colors. We then order the vertices of each color class such that they form a sequence, called a track. We can think of each track as a line segment on which the vertices of that color class are placed in order. The tracks for different colors can intersect, but no two edges can cross between any two color classes. In other words, if we have two vertices of different colors that are adjacent along an edge, then the two vertices must appear on the same track, or on two tracks that do not intersect. This constraint ensures that our track layout corresponds to a planar embedding of the graph - that is, we can draw the graph on the plane without any edges crossing. Moreover, the track layout provides an efficient way to traverse the graph and compute distances and other graph properties.", "label": "0", "is_selected": "1", "text": "In my last post, I explained how a track layout of a graph provides an efficient way to embed the graph on the plane. Moreover, the track layout provides an efficiency way to traverse the graph and other graph properties."}
{"original_text": "A new method for estimating the relative positions of location-unaware nodes from the location-aware nodes and the received signal strength (RSS) between the nodes, in a wireless sensor network (WSN), One possible method for estimating the relative positions of location-unaware nodes in a wireless sensor network (WSN) is based on trilateration. Trilateration is a technique that involves using the distances between known reference points to determine the location of an unknown point. In this case, the known reference points are the location-aware nodes in the network. The unknown points are the location-unaware nodes. The distance between a location-aware node and a location-unaware node can be estimated based on the received signal strength (RSS) between them. The RSS is a measure of the signal strength that is received by a receiver from a transmitter. To estimate the relative position of a location-unaware node, we need to find the intersection point of three circles, each centered at one of the location-aware nodes and with a radius equal to the estimated distance between that node and the location-unaware node. This intersection point represents the location of the location-unaware node. The estimated distance between a location-aware node and a location-unaware node can be calculated using the following formula: distance 10 RSS - RSS0) (10 n where RSS is the received signal strength, RSS0 is the reference signal strength at a reference distance, and n is the path loss exponent. Once the distances between the location-unaware node and the location-aware nodes are estimated, we can use trilateration to estimate the position of the location-unaware node. This method has limitations as it assumes that signals propagate in a straight line, that signal strength attenuation follows a model and that signal strength varies depending on the walls, floor, and ceiling of the room the nodes are located in. Therefore, it is necessary to adjust the path loss exponent (n) model for each environment.", "label": "0", "is_selected": "0", "text": "A new method for estimating the relative positions of location-unaware nodes from the location-aware nodes and the received signal strength (RSS) between the nodes, in a wireless sensor network (WSN), One possible method for estimating the relative positions of location-unaware nodes in a wireless sensor network (WSN) is based on trilateration. Trilateration is a technique that involves using the distances between known reference points to determine the location of an unknown point. In this case, the known reference points are the location-aware nodes in the network. The unknown points are the location-unaware nodes. The distance between a location-aware node and a location-unaware node can be estimated based on the received signal strength (RSS) between them. The RSS is a measure of the signal strength that is received by a receiver from a transmitter. To estimate the relative position of a location-unaware node, we need to find the intersection point of three circles, each centered at one of the location-aware nodes and with a radius equal to the estimated distance between that node and the location-unaware node. This intersection point represents the location of the location-unaware node. The estimated distance between a location-aware node and a location-unaware node can be calculated using the following formula: distance 10 RSS - RSS0) (10 n where RSS is the received signal strength, RSS0 is the reference signal strength at a reference distance, and n is the path loss exponent. Once the distances between the location-unaware node and the location-aware nodes are estimated, we can use trilateration to estimate the position of the location-unaware node. This method has limitations as it assumes that signals propagate in a straight line, that signal strength attenuation follows a model and that signal strength varies depending on the walls, floor, and ceiling of the room the nodes are located in. Therefore, it is necessary to adjust the path loss exponent (n) model for each environment."}
{"original_text": "In this paper, we consider the joint task of simultaneously optimizing (i) the weights of a deep neural network, (ii) the number of neurons for each hidden layer, and (iii) the learning rate for stochastic gradient descent (SGD) during training. This is known as neural architecture search (NAS) and can be a computationally expensive task, as it requires evaluating a large number of different neural architectures. However, it has the potential to significantly improve the performance of deep learning models, particularly in complex tasks. The approach we propose in this paper uses a genetic algorithm to efficiently search for the best neural architecture, and has been shown to outperform existing methods in terms of both accuracy and efficiency.", "label": "0", "is_selected": "1", "text": "The performance of deep learning models on a variety of tasks is dependent on the best possible neural architecture for each layer of a deep learning network, as well as on the learning rate at which each layer is trained."}
{"original_text": "We consider the problem of robustly recovering a k -sparse coefficient vector from the Fourier series that it generates, restricted to the interval [ - O, O ]. The difficulty of this problem is linked to the superresolution factor SRF, equal to the ratio of the Rayleigh length (inverse of O) by the spacing of the grid supporting the sparse vector. In the presence of additive deterministic noise of norm s, we show upper and lower bounds on the minimax error rate that both scale like (S R F) - 2 k 1 s, providing a partial answer to a question posed by Donoho in 1992. The scaling arises from comparing the noise level to a restricted isometry constant at sparsity 2 k, or equivalently from comparing 2 k to the so-called s -spark of the Fourier system. The proof involves new bounds on the singular values of restricted Fourier matrices, obtained in part from old techniques in complex analysis.", "label": "1", "is_selected": "0", "text": "We consider the problem of robustly recovering a k -sparse coefficient vector from the Fourier series that it generates, restricted to the interval [ - O, O ]. The difficulty of this problem is linked to the superresolution factor SRF, equal to the ratio of the Rayleigh length (inverse of O) by the spacing of the grid supporting the sparse vector. In the presence of additive deterministic noise of norm s, we show upper and lower bounds on the minimax error rate that both scale like (S R F) - 2 k 1 s, providing a partial answer to a question posed by Donoho in 1992. The scaling arises from comparing the noise level to a restricted isometry constant at sparsity 2 k, or equivalently from comparing 2 k to the so-called s -spark of the Fourier system. The proof involves new bounds on the singular values of restricted Fourier matrices, obtained in part from old techniques in complex analysis."}
{"original_text": "A source model of key sharing between three users is considered in which each pair of them wishes to agree on a secret key hidden from the remaining user. There are different techniques that can be used to achieve key sharing between three users. One of the most commonly used techniques is the Diffie-Hellman scheme, which requires the use of a modular arithmetic function and a shared public value. Here's how the key sharing process would work: 1. Each user generates a private key that is kept secret. 2. Each user also generates a public key using the Diffie-Hellman function. 3. The public keys are publicly shared with the other users. 4. Each user combines their own private key with the public keys of the other two users to generate a secret key, which is unique to the three users and cannot be determined by anyone else. This technique ensures that each pair of users shares a secret key that is unknown to the third user. Another approach that can be used is the Shamir secret sharing scheme, which allows a secret to be divided into parts and distributed among the users. In this scheme, any subset of the users can reconstruct the original secret, but no information is revealed to the users who are not part of the subset.", "label": "0", "is_selected": "0", "text": "A source model of key sharing between three users is considered in which each pair of them wishes to agree on a secret key hidden from the remaining user. There are different techniques that can be used to achieve key sharing between three users. One of the most commonly used techniques is the Diffie-Hellman scheme, which requires the use of a modular arithmetic function and a shared public value. Here's how the key sharing process would work: 1. Each user generates a private key that is kept secret. 2. Each user also generates a public key using the Diffie-Hellman function. 3. The public keys are publicly shared with the other users. 4. Each user combines their own private key with the public keys of the other two users to generate a secret key, which is unique to the three users and cannot be determined by anyone else. This technique ensures that each pair of users shares a secret key that is unknown to the third user. Another approach that can be used is the Shamir secret sharing scheme, which allows a secret to be divided into parts and distributed among the users. In this scheme, any subset of the users can reconstruct the original secret, but no information is revealed to the users who are not part of the subset."}
{"original_text": "In partially observable (PO) environments, deep reinforcement learning (RL) agents often suffer from unsatisfactory performance, since two problems need to be tackled together: how to extract information from the raw observations and how to make optimal decisions based on the extracted information. One solution to this problem is to use recurrent neural networks (RNNs) as the function approximator for the value function or policy in the RL algorithm. RNNs have the ability to maintain a hidden state, which allows them to process sequences of observations and retain information over time. Another approach is to use a combination of RL and imitation learning. In this approach, the agent is first trained on a dataset of expert demonstrations, using supervised learning to learn an initial policy. Then, the agent switches to RL to refine the policy based on the feedback received from the environment. Additionally, techniques such as curiosity-driven exploration, hierarchical reinforcement learning, and meta-learning can help RL agents extract useful information from the environment and make better decisions in PO settings. Overall, there is no one-size-fits-all solution for the challenges posed by PO environments, and researchers are continually exploring new techniques to improve the performance of RL agents in these settings.", "label": "0", "is_selected": "0", "text": "In partially observable (PO) environments, deep reinforcement learning (RL) agents often suffer from unsatisfactory performance, since two problems need to be tackled together: how to extract information from the raw observations and how to make optimal decisions based on the extracted information. One solution to this problem is to use recurrent neural networks (RNNs) as the function approximator for the value function or policy in the RL algorithm. RNNs have the ability to maintain a hidden state, which allows them to process sequences of observations and retain information over time. Another approach is to use a combination of RL and imitation learning. In this approach, the agent is first trained on a dataset of expert demonstrations, using supervised learning to learn an initial policy. Then, the agent switches to RL to refine the policy based on the feedback received from the environment. Additionally, techniques such as curiosity-driven exploration, hierarchical reinforcement learning, and meta-learning can help RL agents extract useful information from the environment and make better decisions in PO settings. Overall, there is no one-size-fits-all solution for the challenges posed by PO environments, and researchers are continually exploring new techniques to improve the performance of RL agents in these settings."}
{"original_text": "We introduce a model of one-way language acceptors (a variant of a checking stack automaton) and show the following decidability properties: 1. item 1 1 item 1 The deterministic version of the model can decide any context-free language in linear time, and 2. the non-deterministic version of the model can decide any recursively enumerable language. Would you like me to explain these concepts further?", "label": "0", "is_selected": "0", "text": "We introduce a model of one-way language acceptors (a variant of a checking stack automaton) and show the following decidability properties: 1. item 1 1 item 1 The deterministic version of the model can decide any context-free language in linear time, and 2. the non-deterministic version of the model can decide any recursively enumerable language. Would you like me to explain these concepts further?"}
{"original_text": "Huge amounts of digital videos are being produced and broadcast every day, leading to giant media archives. Effective techniques are needed to make such data accessible further. Automatic meta-data labelling and indexing is one solution to this problem. Automatic meta-data labelling involves applying relevant keywords or tags to the video content, making it easier to search and categorize. Indexing involves creating an organized catalog of the videos, allowing users to easily navigate through the archives. There are various techniques that can be used for automatic meta-data labelling and indexing, including machine learning algorithms and computer vision technologies. These techniques allow computers to recognize and classify objects, scenes, and events in the videos. One such approach is object recognition and detection, which involves identifying and localizing objects within the video. This can be used to automatically label videos with relevant tags, such as the names of objects or scenes depicted in the video. Another technique is sentiment analysis, which involved analyzing the emotions or opinions expressed in the video. By leveraging these techniques, video archives can become more accessible and useful to users, making it easier to find and use video content for various purposes.", "label": "0", "is_selected": "1", "text": "Automatic meta-data labelling and indexing of video content is one way to make video archives easier to find and use, according to a new report by the US National Research Council."}
{"original_text": "Graphlets are induced subgraph patterns and have been frequently applied to characterize the local topology structures of graphs across various domains, e.g., online social networks (OSNs) and biological networks. Discovering graphlets in a graph involves identifying all the possible subgraphs of various sizes and then counting the occurrences of each distinct subgraph. This information can then be used to identify important nodes or communities within the graph or to compare the structure of different graphs. There are various algorithms for discovering graphlets, including those based on random walks, network motifs, and graphlet degree distribution. Each method has its advantages and disadvantages depending on the characteristics of the graph and the research question.", "label": "0", "is_selected": "1", "text": "The discovery of graphlets in graphs has become an important topic in computer science, particularly in the area of graph theory and its applications to network theory."}
{"original_text": "We present Animo, a smartwatch app that enables people to share and view each other's biosignals. We designed and engineered Animo to explore new ground for smartwatch-based biosignals social computing systems: identifying opportunities where these systems can support lightweight and mood-centric interactions. In our work we develop, explore, and evaluate several innovative features designed for dyadic communication of heart rate. We discuss the results of a two-week study (N34), including new communication patterns participants engaged in, and outline the design landscape for communicating with biosignals on smartwatches.", "label": "1", "is_selected": "0", "text": "We present Animo, a smartwatch app that enables people to share and view each other's biosignals. We designed and engineered Animo to explore new ground for smartwatch-based biosignals social computing systems: identifying opportunities where these systems can support lightweight and mood-centric interactions. In our work we develop, explore, and evaluate several innovative features designed for dyadic communication of heart rate. We discuss the results of a two-week study (N34), including new communication patterns participants engaged in, and outline the design landscape for communicating with biosignals on smartwatches."}
{"original_text": "One of the roadmap plans for quantum computers is an integration within HPC ecosystems assigning them a role of accelerators for a variety of computationally hard tasks. However, in the near term, quantum hardware will be in a constant state of change. Heading towards solving real-world problems, we advocate development of portable, architecture-agnostic hybrid quantum-classical frameworks and demonstrate one for the community detection problem evaluated using quantum annealing and gate-based universal quantum computation paradigms.", "label": "1", "is_selected": "0", "text": "One of the roadmap plans for quantum computers is an integration within HPC ecosystems assigning them a role of accelerators for a variety of computationally hard tasks. However, in the near term, quantum hardware will be in a constant state of change. Heading towards solving real-world problems, we advocate development of portable, architecture-agnostic hybrid quantum-classical frameworks and demonstrate one for the community detection problem evaluated using quantum annealing and gate-based universal quantum computation paradigms."}
{"original_text": "We consider data transmission over a network where each edge is an erasure channel and where the inner nodes transmit a random linear combination of their incoming information. We distinguish between two types of erasure channels: 1. Binary erasure channel (BEC): In this type of channel, the edge either transmits the symbol correctly with a probability of 1-p, or it erases the symbol with a probability of p. The erased symbol is not transmitted to the receiver. 2. Erasure channel (EC): In this type of channel, the edge either transmits the symbol correctly with a probability of 1-p, or it erases the symbol with a probability of p. The erased symbol is transmitted to the receiver as a known erasure symbol. In both types of channels, the random linear combination performed by the inner nodes helps to improve the reliability of the transmission. By transmitting linear combinations of the incoming information, the receiver can recover the original data even if some of the edges have erased symbols. This technique is known as network coding, and it is particularly useful in situations where the transmission capacity of the network is limited. Overall, understanding the properties of the erasure channels and the effects of network coding can help us design more efficient and reliable data transmission schemes in networks.", "label": "0", "is_selected": "1", "text": "In this paper, we study the properties of data transmission over a network where each edge is an erasure channel and where the inner nodes transmit a random linear combination of their incoming information."}
{"original_text": "Aggregators are playing an increasingly crucial role in the integration of renewable generation in power systems. However, the intermittent nature of renewable generation makes market interactions of aggregators difficult to manage. This is because the output of renewable sources varies according to weather conditions, and is not constant throughout the day. Aggregators help to overcome this issue by combining the output of multiple small-scale renewable sources into a larger, more predictable source of power. To manage their interactions in the market, aggregators can use a variety of mechanisms, such as: 1. Forecasting: Accurate forecasting of renewable energy output is crucial for aggregators to participate effectively in the market. This allows them to plan their energy dispatch, optimize their resource management and manage their revenue. 2. Energy Storage: Deploying energy storage systems can supplement the shortcomings of renewable sources, by storing excess energy generated during peak hours and discharging it during off-peak hours. 3. Demand Response: Aggregators can use demand response to manage energy consumption by reducing demand during peak load hours in exchange for significant cost savings. 4. Virtual Power Plants (VPPs): Aggregators can also use VPPs to aggregate multiple small-scale renewable sources into a single distributed energy resource that can be dispatched and settled in wholesale markets. In conclusion, aggregators have a significant role in integrating renewable generation in power systems, and it requires careful management to optimize their market interactions.", "label": "0", "is_selected": "0", "text": "Aggregators are playing an increasingly crucial role in the integration of renewable generation in power systems. However, the intermittent nature of renewable generation makes market interactions of aggregators difficult to manage. This is because the output of renewable sources varies according to weather conditions, and is not constant throughout the day. Aggregators help to overcome this issue by combining the output of multiple small-scale renewable sources into a larger, more predictable source of power. To manage their interactions in the market, aggregators can use a variety of mechanisms, such as: 1. Forecasting: Accurate forecasting of renewable energy output is crucial for aggregators to participate effectively in the market. This allows them to plan their energy dispatch, optimize their resource management and manage their revenue. 2. Energy Storage: Deploying energy storage systems can supplement the shortcomings of renewable sources, by storing excess energy generated during peak hours and discharging it during off-peak hours. 3. Demand Response: Aggregators can use demand response to manage energy consumption by reducing demand during peak load hours in exchange for significant cost savings. 4. Virtual Power Plants (VPPs): Aggregators can also use VPPs to aggregate multiple small-scale renewable sources into a single distributed energy resource that can be dispatched and settled in wholesale markets. In conclusion, aggregators have a significant role in integrating renewable generation in power systems, and it requires careful management to optimize their market interactions."}
{"original_text": "This study mainly investigates two decoding problems in neural keyphrase generation: sequence length bias and beam diversity. We introduce an extension of beam search inference based on word-level and n-gram level attention score to adjust and constrain Seq2Seq prediction at test time. Results show that our proposed solution can overcome the algorithm bias to shorter and nearly identical sequences, resulting in a significant improvement of the decoding performance on generating keyphrases that are present and absent in source text.", "label": "1", "is_selected": "0", "text": "This study mainly investigates two decoding problems in neural keyphrase generation: sequence length bias and beam diversity. We introduce an extension of beam search inference based on word-level and n-gram level attention score to adjust and constrain Seq2Seq prediction at test time. Results show that our proposed solution can overcome the algorithm bias to shorter and nearly identical sequences, resulting in a significant improvement of the decoding performance on generating keyphrases that are present and absent in source text."}
{"original_text": "The motivation for this paper is to apply Bayesian structure learning using Model Averaging in large-scale networks. Currently, Bayesian model averaging algorithm is applicable to networks with only tens of variables, restrained by its super-exponential complexity. We present a novel framework, called LSBN (Large-Scale Bayesian Network), making it possible to handle networks with infinite size by following the principle of divide-and-conquer. The method of LSBN comprises three steps. In general, LSBN first performs the partition by using a second-order partition strategy, which achieves more robust results. LSBN conducts sampling and structure learning within each overlapping community after the community is isolated from other variables by Markov Blanket. Finally LSBN employs an efficient algorithm, to merge structures of overlapping communities into a whole. In comparison with other four state-of-art large-scale network structure learning algorithms such as ARACNE, PC, Greedy Search and MMHC, LSBN shows comparable results in five common benchmark datasets, evaluated by precision, recall and f-score. What's more, LSBN makes it possible to learn large-scale Bayesian structure by Model Averaging which used to be intractable. In summary, LSBN provides an scalable and parallel framework for the reconstruction of network structures. Besides, the complete information of overlapping communities serves as the byproduct, which could be used to mine meaningful clusters in biological networks, such as protein-protein-interaction network or gene regulatory network, as well as in social network.", "label": "1", "is_selected": "0", "text": "The motivation for this paper is to apply Bayesian structure learning using Model Averaging in large-scale networks. Currently, Bayesian model averaging algorithm is applicable to networks with only tens of variables, restrained by its super-exponential complexity. We present a novel framework, called LSBN (Large-Scale Bayesian Network), making it possible to handle networks with infinite size by following the principle of divide-and-conquer. The method of LSBN comprises three steps. In general, LSBN first performs the partition by using a second-order partition strategy, which achieves more robust results. LSBN conducts sampling and structure learning within each overlapping community after the community is isolated from other variables by Markov Blanket. Finally LSBN employs an efficient algorithm, to merge structures of overlapping communities into a whole. In comparison with other four state-of-art large-scale network structure learning algorithms such as ARACNE, PC, Greedy Search and MMHC, LSBN shows comparable results in five common benchmark datasets, evaluated by precision, recall and f-score. What's more, LSBN makes it possible to learn large-scale Bayesian structure by Model Averaging which used to be intractable. In summary, LSBN provides an scalable and parallel framework for the reconstruction of network structures. Besides, the complete information of overlapping communities serves as the byproduct, which could be used to mine meaningful clusters in biological networks, such as protein-protein-interaction network or gene regulatory network, as well as in social network."}
{"original_text": "In this paper we provide nearly linear time algorithms for several problems closely associated with the classic Perron-Frobenius theorem, including computing Perron vectors, i.e. entrywise non-negative eigenvectors of non-negative matrices, That sounds interesting! Can you give me some more details about the algorithms and how they work?", "label": "0", "is_selected": "0", "text": "In this paper we provide nearly linear time algorithms for several problems closely associated with the classic Perron-Frobenius theorem, including computing Perron vectors, i.e. entrywise non-negative eigenvectors of non-negative matrices, That sounds interesting! Can you give me some more details about the algorithms and how they work?"}
{"original_text": "The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the data annotation process. We collect and annotate a challenge-set we call Challenging RE (CRE), based on naturally occurring corpus examples, to benchmark this behavior. Our experiments with four state-of-the-art RE models show that they have indeed adopted shallow heuristics that do not generalize to the challenge-set data. Further, we find that alternative question answering modeling performs significantly better than the SOTA models on the challenge-set, despite worse overall TACRED performance. By adding some of the challenge data as training examples, the performance of the model improves. Finally, we provide concrete suggestion on how to improve RE data collection to alleviate this behavior.", "label": "1", "is_selected": "0", "text": "The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the data annotation process. We collect and annotate a challenge-set we call Challenging RE (CRE), based on naturally occurring corpus examples, to benchmark this behavior. Our experiments with four state-of-the-art RE models show that they have indeed adopted shallow heuristics that do not generalize to the challenge-set data. Further, we find that alternative question answering modeling performs significantly better than the SOTA models on the challenge-set, despite worse overall TACRED performance. By adding some of the challenge data as training examples, the performance of the model improves. Finally, we provide concrete suggestion on how to improve RE data collection to alleviate this behavior."}
{"original_text": "Hierarchical models for deep reinforcement learning (RL) have emerged as powerful methods for generating meaningful control strategies in difficult long time horizon tasks. Training of said hierarchical models, however, continue to be a challenging problem due to the large number of parameters involved and the potential for overly complex solutions. One approach that has shown promise is to use a combination of supervised and reinforcement learning techniques to learn the lower-level controllers in the hierarchy. This allows the higher-level controllers to leverage the pre-trained lower-level policies and focus on generating higher-level strategies. Another approach is to use transfer learning, where a pre-trained policy can be fine-tuned on a new task, reducing the amount of training data required. These techniques have shown promising results in improving the sample efficiency and reducing the training time for hierarchical RL models.", "label": "0", "is_selected": "1", "text": "In this paper, we present a new approach to training hierarchical models for deep reinforcement learning, which reduces the training time and increases the efficiency of the sample."}
{"original_text": "A major difficulty of solving continuous POMDPs is to infer the multi-modal distribution of the unobserved true states and to make the planning algorithm dependent on the perceived uncertainty. We can help by suggesting some approaches to solve continuous POMDPs. One approach is to use particle filters, which are a type of Monte Carlo method that can estimate the probability distribution of the unobserved true states based on a set of discrete samples or particles. Another approach is to use Gaussian mixture models, which represent the probability distribution of the true states as a weighted sum of multiple Gaussian distributions. In terms of planning algorithms, one popular method is the partially observable Monte Carlo planning (POMCP), which uses Monte Carlo tree search to plan under uncertainty by simulating possible future states and observations. Another approach is the information-gathering POMDPs (IG-POMDPs), which aim to maximize the information gained through actions and observations to reduce uncertainty about the true state. Ultimately, the choice of approach will depend on the specific problem and the available resources, and it may require a combination of different techniques to achieve optimal performance.", "label": "0", "is_selected": "1", "text": "In this paper, we discuss continuous partial observable Monte Carlo planning (POMDPs) in the context of the Large Hadron Collider (LHC) and their use in particle physics."}
{"original_text": "We introduce a novel class of adjustment rules for a collection of beliefs. This is an extension of Lewis' imaging to absorb probabilistic evidence in generalized settings. Unlike standard tools for belief revision, our proposal may be used when information is inconsistent with an agent's belief base. We show that the functionals we introduce are based on the imaginary counterpart of probability kinematics for standard belief revision, and prove that, under certain conditions, all standard postulates for belief revision are satisfied.", "label": "1", "is_selected": "0", "text": "We introduce a novel class of adjustment rules for a collection of beliefs. This is an extension of Lewis' imaging to absorb probabilistic evidence in generalized settings. Unlike standard tools for belief revision, our proposal may be used when information is inconsistent with an agent's belief base. We show that the functionals we introduce are based on the imaginary counterpart of probability kinematics for standard belief revision, and prove that, under certain conditions, all standard postulates for belief revision are satisfied."}
{"original_text": "A network model is considered where Poisson distributed base stations transmit to N power-domain non-orthogonal multiple access (NOMA) users (UEs) each that employ successive interference cancellation (SIC) for decoding. We can break down this network model as follows: - The base stations use a Poisson distribution to determine their random locations and transmitting power levels. - There are N power-domain non-orthogonal multiple access (NOMA) users per base station. NOMA is a technique where multiple users share the same frequency and time resources, but use different power levels to transmit their signals. This allows for higher spectral efficiency. - Each NOMA user employs successive interference cancellation (SIC) for decoding. SIC is a technique where users decode the strongest signal first, remove it from the received signal, and then decode the next strongest signal. This process continues until all signals have been decoded. Overall, this network model allows for efficient use of resources and high spectral efficiency due to the use of NOMA and SIC techniques.", "label": "0", "is_selected": "1", "text": "In this paper, we study the use of multiple access techniques to decode signals from a distributed base station. Overall, this network model allows for efficient use of resources and high spectral efficiency due to the using of NOMA and SIC techniques."}
{"original_text": "Most state-of-the-art semantic segmentation approaches only achieve high accuracy in good conditions. In practically-common but less-discussed adverse environmental conditions, their performance can decrease enormously. Existing studies usually cast the handling of adverse environmental conditions as a problem of domain adaptation, transfer learning, or adapting the model to the specific challenge. However, there is much ongoing research in developing methods that can generate more robust and accurate semantic segmentation under a variety of challenging environmental conditions. Some of the techniques include incorporating self-supervision, adversarial training, multi-modal learning, and data augmentation. These techniques can help expose the model to different data distributions in training, which in turn can make the model more robust to different environmental conditions. Another important approach is to use real-world data augmentation techniques by creating synthetic data that mimics real-world conditions. By doing this, the model can learn to handle the challenges it may face in the real world. Overall, precise semantic segmentation in changing and adverse environmental conditions remains a challenge. However, there is a lot of ongoing research in the field, which is expected to result in more robust and accurate semantic segmentation methods.", "label": "0", "is_selected": "0", "text": "Most state-of-the-art semantic segmentation approaches only achieve high accuracy in good conditions. In practically-common but less-discussed adverse environmental conditions, their performance can decrease enormously. Existing studies usually cast the handling of adverse environmental conditions as a problem of domain adaptation, transfer learning, or adapting the model to the specific challenge. However, there is much ongoing research in developing methods that can generate more robust and accurate semantic segmentation under a variety of challenging environmental conditions. Some of the techniques include incorporating self-supervision, adversarial training, multi-modal learning, and data augmentation. These techniques can help expose the model to different data distributions in training, which in turn can make the model more robust to different environmental conditions. Another important approach is to use real-world data augmentation techniques by creating synthetic data that mimics real-world conditions. By doing this, the model can learn to handle the challenges it may face in the real world. Overall, precise semantic segmentation in changing and adverse environmental conditions remains a challenge. However, there is a lot of ongoing research in the field, which is expected to result in more robust and accurate semantic segmentation methods."}
{"original_text": "We propose a novel vector representation that integrates lexical contrast into distributional vectors and strengthens the most salient features for determining degrees of word similarity. The improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66-0.76 across word classes (adjectives, nouns, verbs). Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model. The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex-999, and on distinguishing antonyms from synonyms.", "label": "1", "is_selected": "0", "text": "We propose a novel vector representation that integrates lexical contrast into distributional vectors and strengthens the most salient features for determining degrees of word similarity. The improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66-0.76 across word classes (adjectives, nouns, verbs). Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model. The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex-999, and on distinguishing antonyms from synonyms."}
{"original_text": "Long short-term memory (LSTM) and recurrent neural network (RNN) has achieved great successes on time-series prediction. In this paper, a methodology of using LSTM-based deep-RNN for two-phase flow regime prediction is proposed. The proposed methodology consists of a two-phase flow regime dataset, data preprocessing, model architecture design, training and model evaluation. The dataset used for this research consists of time-series data of two-phase flow regime. The data is preprocessed by normalizing and splitting it into training and testing sets. The model architecture is designed to have an LSTM layer followed by a dense layer. The LSTM layer captures the long-term dependencies of the time-series data and the dense layer is used for classification. The model is trained on the training set using back propagation through time (BPTT) algorithm. The performance of the model is evaluated on the testing set using accuracy, sensitivity, and specificity metrics. The results show that the LSTM-based deep-RNN model achieves high accuracy in predicting the two-phase flow regime. Overall, the proposed methodology can be used for predicting various time-series data and can help in better understanding the behavior of complex systems.", "label": "0", "is_selected": "1", "text": "Time-series data can be used to predict the flow regime of complex systems such as computers, networks, and storage systems, and to understand the behaviour of these systems."}
{"original_text": "Consider the estimation of a signal x R N from noisy observations r x z, where the input x is generated by an independent and identically distributed (i.i.d.) Gaussian mixture One approach to estimate x from the noisy observations r is to use a maximum likelihood estimation (MLE) framework. In this framework, we assume that the noisy observations r x z, where z is a random noise with N (0,) distribution. The goal is to find the estimate z that maximizes the likelihood function L (x z) P (r z), where P (r z) is the probability density function (pdf) of r given z. Using the assumption that the input x is generated by an i.i.d. Gaussian mixture, we can write the likelihood function as: L (x z) P (r z) P (x z z) Assuming that the noise z is independent of x, we can write: L (x z) P (z) P (x z) where P (z) N (0,) is the pdf of the noise at the i-th observation. We can further assume that the pdf of the input x follows a mixture of Gaussians, i.e., P (x) w N (,) where w is the weight of the i-th Gaussian component, is the mean vector, and is the covariance matrix of the i-th component. Using this assumption, we can write the likelihood function as: L (x z) 12} [2 ( ) 12} exp (r - - z) ( ) 1} (r - - z) 2] The MLE estimate z can then be obtained by maximizing this likelihood function with respect to z. This can be done using numerical optimization methods such as gradient descent, conjugate gradient, or Newton's method. It is worth noting that the estimation performance depends on the choice of parameters in the mixture of Gaussians model, such as the number of components, their weights, means, and covariances. These parameters can be learned from the data using techniques such as the expectation-maximization (EM) algorithm.", "label": "0", "is_selected": "1", "text": "In this paper, we investigate the likelihood function L (x z) P (r z), where P is the probability density function (pdf) of z."}
{"original_text": "This work proposes an improved reversible data hiding scheme in encrypted images using parametric binary tree labeling (IPBTL-RDHEI), which takes advantage of the spatial correlation in the entire original image but not in small image blocks to reserve room for hiding data. Then the original image is encrypted with an encryption key and the parametric binary tree is used to label encrypted pixels into two different categories. Finally, one of the two categories of encrypted pixels can embed secret information by bit replacement. According to the experimental results, compared with several state-of-the-art methods, the proposed IPBTL-RDHEI method achieves higher embedding rate and outperforms the competitors. Due to the reversibility of IPBTL-RDHEI, the original plaintext image and the secret information can be restored and extracted losslessly and separately.", "label": "1", "is_selected": "0", "text": "This work proposes an improved reversible data hiding scheme in encrypted images using parametric binary tree labeling (IPBTL-RDHEI), which takes advantage of the spatial correlation in the entire original image but not in small image blocks to reserve room for hiding data. Then the original image is encrypted with an encryption key and the parametric binary tree is used to label encrypted pixels into two different categories. Finally, one of the two categories of encrypted pixels can embed secret information by bit replacement. According to the experimental results, compared with several state-of-the-art methods, the proposed IPBTL-RDHEI method achieves higher embedding rate and outperforms the competitors. Due to the reversibility of IPBTL-RDHEI, the original plaintext image and the secret information can be restored and extracted losslessly and separately."}
{"original_text": "Image inpainting is one of the most challenging tasks in computer vision. Recently, generative-based image inpainting methods have been shown to produce visually plausible images. However, they still have difficulties in handling complex structures and textures, and are often prone to generating blurry or distorted regions. To address these challenges, researchers are exploring various strategies, such as incorporating global context information, leveraging reinforcement learning to improve the quality of generated images, and designing more effective loss functions to guide the training process. Overall, image inpainting remains an active area of research and development, with many exciting possibilities for improving its performance and expanding its range of applications in areas such as image editing, restoration, and synthesis.", "label": "0", "is_selected": "0", "text": "Image inpainting is one of the most challenging tasks in computer vision. Recently, generative-based image inpainting methods have been shown to produce visually plausible images. However, they still have difficulties in handling complex structures and textures, and are often prone to generating blurry or distorted regions. To address these challenges, researchers are exploring various strategies, such as incorporating global context information, leveraging reinforcement learning to improve the quality of generated images, and designing more effective loss functions to guide the training process. Overall, image inpainting remains an active area of research and development, with many exciting possibilities for improving its performance and expanding its range of applications in areas such as image editing, restoration, and synthesis."}
{"original_text": "We assume that recommender systems are more successful, when they are based on a thorough understanding of how people process information. In the current paper we test this assumption in the context of social tagging systems. Cognitive research on how people assign tags has shown that they draw on two interconnected levels of knowledge in their memory: on a conceptual level of semantic fields or topics, and on a lexical level that turns patterns on the semantic level into words. Another strand of tagging research reveals a strong impact of time dependent forgetting on users' tag choices, such that recently used tags have a higher probability being reused than \"older\" tags. In this paper, we align both strands by implementing a computational theory of human memory that integrates the two-level conception and the process of forgetting in form of a tag recommender and test it in three large-scale social tagging datasets (drawn from BibSonomy, CiteULike and Flickr). As expected, our results reveal a selective effect of time: forgetting is much more pronounced on the lexical level of tags. Second, an extensive evaluation based on this observation shows that a tag recommender interconnecting both levels and integrating time dependent forgetting on the lexical level results in high accuracy predictions and outperforms other well-established algorithms, such as Collaborative Filtering, Pairwise Interaction Tensor Factorization, FolkRank and two alternative time dependent approaches. We conclude that tag recommenders can benefit from going beyond the manifest level of word co-occurrences, and from including forgetting processes on the lexical level.", "label": "1", "is_selected": "0", "text": "We assume that recommender systems are more successful, when they are based on a thorough understanding of how people process information. In the current paper we test this assumption in the context of social tagging systems. Cognitive research on how people assign tags has shown that they draw on two interconnected levels of knowledge in their memory: on a conceptual level of semantic fields or topics, and on a lexical level that turns patterns on the semantic level into words. Another strand of tagging research reveals a strong impact of time dependent forgetting on users' tag choices, such that recently used tags have a higher probability being reused than \"older\" tags. In this paper, we align both strands by implementing a computational theory of human memory that integrates the two-level conception and the process of forgetting in form of a tag recommender and test it in three large-scale social tagging datasets (drawn from BibSonomy, CiteULike and Flickr). As expected, our results reveal a selective effect of time: forgetting is much more pronounced on the lexical level of tags. Second, an extensive evaluation based on this observation shows that a tag recommender interconnecting both levels and integrating time dependent forgetting on the lexical level results in high accuracy predictions and outperforms other well-established algorithms, such as Collaborative Filtering, Pairwise Interaction Tensor Factorization, FolkRank and two alternative time dependent approaches. We conclude that tag recommenders can benefit from going beyond the manifest level of word co-occurrences, and from including forgetting processes on the lexical level."}
{"original_text": "3-D image registration, which involves aligning two or more images, is a critical step in a variety of medical applications from diagnosis to therapy. Image registration is commonly performed by computational algorithms that match corresponding features in the images being aligned. These features can be easily identifiable landmarks such as anatomical boundaries or fiducial markers, or more complex image features such as texture or intensity patterns. The image registration process transforms one image to match the spatial orientation and scale of the other image, making them spatially aligned and allowing for image analysis or visualization. Some common medical applications that rely on image registration include tumor delineation, treatment planning, and surgical navigation.", "label": "0", "is_selected": "1", "text": "The global 3-D image registration market is expected to grow at a Compound Annual Growth Rate (CGR) of more than 20% over the forecast period, according to a new report by research firm eMarketer."}
{"original_text": "Computed tomography (CT) is critical for various clinical applications, e.g., radiotherapy treatment planning and also PET attenuation correction. However, CT exposes radiation during acquisition, which may cause side effects to patients. Compared to CT, magnetic resonance imaging (MRI) is much safer and does not involve any radiations. Therefore, recently, researchers are greatly motivated to estimate CT image from its corresponding MR image of the same subject for the case of radiotherapy planning. In this paper, we propose a data-driven approach to address this challenging problem. Specifically, we train a fully convolutional network to generate CT given an MR image. To better model the nonlinear relationship from MRI to CT and to produce more realistic images, we propose to use the adversarial training strategy and an image gradient difference loss function. We further apply AutoContext Model to implement a context-aware generative adversarial network. Experimental results show that our method is accurate and robust for predicting CT images from MRI images, and also outperforms three state-of-the-art methods under comparison.", "label": "1", "is_selected": "0", "text": "Computed tomography (CT) is critical for various clinical applications, e.g., radiotherapy treatment planning and also PET attenuation correction. However, CT exposes radiation during acquisition, which may cause side effects to patients. Compared to CT, magnetic resonance imaging (MRI) is much safer and does not involve any radiations. Therefore, recently, researchers are greatly motivated to estimate CT image from its corresponding MR image of the same subject for the case of radiotherapy planning. In this paper, we propose a data-driven approach to address this challenging problem. Specifically, we train a fully convolutional network to generate CT given an MR image. To better model the nonlinear relationship from MRI to CT and to produce more realistic images, we propose to use the adversarial training strategy and an image gradient difference loss function. We further apply AutoContext Model to implement a context-aware generative adversarial network. Experimental results show that our method is accurate and robust for predicting CT images from MRI images, and also outperforms three state-of-the-art methods under comparison."}
{"original_text": "Multi-target multi-camera tracking (MTMCT) systems track targets across cameras. Due to the continuity of target trajectories, tracking systems usually restrict their data association within a local neighborhood. In single camera tracking, data association is typically based on the appearance or motion model of the targets. However, MTMCT systems face additional challenges, such as occlusion and camera view changes, which can cause significant shifts in appearance and motion. To overcome these challenges, researchers have developed various techniques for MTMCT, including tracklet-based association, graph-based association, and deep learning-based methods. Tracklet-based association uses short-term trajectory segments, called tracklets, to associate targets across cameras. Graph-based association constructs a global graph, where nodes represent targets and edges represent spatial and temporal relationships between them. Deep learning-based methods use neural networks to learn features that are robust to appearance changes and occlusions, which can improve data association performance. Overall, the development of MTMCT systems is an active research area and continues to advance with the aid of new technologies and improved algorithms.", "label": "0", "is_selected": "0", "text": "Multi-target multi-camera tracking (MTMCT) systems track targets across cameras. Due to the continuity of target trajectories, tracking systems usually restrict their data association within a local neighborhood. In single camera tracking, data association is typically based on the appearance or motion model of the targets. However, MTMCT systems face additional challenges, such as occlusion and camera view changes, which can cause significant shifts in appearance and motion. To overcome these challenges, researchers have developed various techniques for MTMCT, including tracklet-based association, graph-based association, and deep learning-based methods. Tracklet-based association uses short-term trajectory segments, called tracklets, to associate targets across cameras. Graph-based association constructs a global graph, where nodes represent targets and edges represent spatial and temporal relationships between them. Deep learning-based methods use neural networks to learn features that are robust to appearance changes and occlusions, which can improve data association performance. Overall, the development of MTMCT systems is an active research area and continues to advance with the aid of new technologies and improved algorithms."}
