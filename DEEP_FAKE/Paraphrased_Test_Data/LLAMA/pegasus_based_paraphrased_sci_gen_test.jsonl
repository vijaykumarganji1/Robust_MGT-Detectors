{"original_text": "We show that given a 3-colorable graph, it is NP -hard to find a 3-coloring with (16 17 eps) of the edges bichromatic. In a related result, we show that it is NP-hard to find a 3-coloring with (16 17 eps) of the edges monochromatic. This answers an open problem of Linial [Linial, 1987] and implies that the problem of deciding the existence of a 3-coloring with (16 17 eps) of the edges bichromatic is NP -complete.", "label": "0", "is_selected": "1", "text": "In this paper, we show that the edges of a Linial bichromatic graph are hard to find on a graph that is not hard at all."}
{"original_text": "The behavior of users in social networks is often observed to be affected by the actions of their friends. Bhawalkar et al. introduced a formal mathematical model for user engagement in social networks. They showed that user activity can be represented as a differential equation describing the movement of a particle in a one-dimensional space. A user's position in this space is determined by hisher information spreading capabilities. User engagement can be visualized as a Markov process, in which the user is assumed to jump between the state (i.e., position) of the current node and its neighbors. The transition probabilities depend on the potential of the users' neighbors to spread information. In this paper, we introduce a novel method to model and analyze the user engagement of a social network. In our model, a user is represented by a random walker that has a set of neighbors. The movement of the walker depends on the transition probabilities of the neighbors. We introduce a novel measure called the transition influence that quantifies the influence of a node on its neighbors. We use this measure to find the optimal set of neighbors that maximizes the probability that the user will remain engaged. Our experiments show that our proposed model can identify the users who are most likely to remain engaged for a longer period of time. Our model could be useful in applications such as viral marketing campaigns, which aim to promote a product or a service to a large number of users.", "label": "0", "is_selected": "0", "text": "The behavior of users in social networks is often observed to be affected by the actions of their friends. Bhawalkar et al. introduced a formal mathematical model for user engagement in social networks. They showed that user activity can be represented as a differential equation describing the movement of a particle in a one-dimensional space. A user's position in this space is determined by hisher information spreading capabilities. User engagement can be visualized as a Markov process, in which the user is assumed to jump between the state (i.e., position) of the current node and its neighbors. The transition probabilities depend on the potential of the users' neighbors to spread information. In this paper, we introduce a novel method to model and analyze the user engagement of a social network. In our model, a user is represented by a random walker that has a set of neighbors. The movement of the walker depends on the transition probabilities of the neighbors. We introduce a novel measure called the transition influence that quantifies the influence of a node on its neighbors. We use this measure to find the optimal set of neighbors that maximizes the probability that the user will remain engaged. Our experiments show that our proposed model can identify the users who are most likely to remain engaged for a longer period of time. Our model could be useful in applications such as viral marketing campaigns, which aim to promote a product or a service to a large number of users."}
{"original_text": "Consider a collaborative task carried out by two autonomous agents that can communicate over a noisy channel. Each agent is only aware of its own state, while the accomplishment of the task depends on the value of the joint state of both agents. As an example, both agents must simultaneously reach a certain location of the environment, while only being aware of their own positions. Assuming the presence of feedback in the form of a common reward to the agents, a conventional approach would apply separately: (i) an off-the-shelf coding and decoding scheme in order to enhance the reliability of the communication of the state of one agent to the other; and (ii) a standard multi-agent reinforcement learning strategy to learn how to act in the resulting environment. In this work, it is argued that the performance of the collaborative task can be improved if the agents learn how to jointly communicate and act. In particular, numerical results for a baseline grid world example demonstrate that the jointly learned policy carries out compression and unequal error protection by leveraging information about the action policy.", "label": "1", "is_selected": "0", "text": "Consider a collaborative task carried out by two autonomous agents that can communicate over a noisy channel. Each agent is only aware of its own state, while the accomplishment of the task depends on the value of the joint state of both agents. As an example, both agents must simultaneously reach a certain location of the environment, while only being aware of their own positions. Assuming the presence of feedback in the form of a common reward to the agents, a conventional approach would apply separately: (i) an off-the-shelf coding and decoding scheme in order to enhance the reliability of the communication of the state of one agent to the other; and (ii) a standard multi-agent reinforcement learning strategy to learn how to act in the resulting environment. In this work, it is argued that the performance of the collaborative task can be improved if the agents learn how to jointly communicate and act. In particular, numerical results for a baseline grid world example demonstrate that the jointly learned policy carries out compression and unequal error protection by leveraging information about the action policy."}
{"original_text": "Modern pattern recognition methods are based on convolutional networks since they are able to learn complex patterns that benefit the classification. However, convolutional networks are computationally expensive and require a great deal of training data. In this paper we address these problems by introducing a novel convolutional network with efficient learning and training procedure, called Convolutional Neural Network with Exponential Kernel Function (ConvNekf). ConvNekf is based on Neural Networks with Exponential Kernel Function (Nekf) and convolutions. This combination of Nekf and convolutions results in an efficient and powerful convolutional network. ConvNekf consists of the convolutional layer, the max-pooling layer and the Nekf layer. For the training, the Nekf layer uses an efficient backpropagation algorithm with stochastic learning rate. The Nekf layer uses exponential kernel functions to represent the weights of the neurons. We experimentally demonstrate the performance of the proposed ConvNekf on the MNIST dataset, and on the synthetic dataset. The proposed ConvNekf achieves higher accuracy and lower computational time compared to other convolutional networks. This work is supported by the Federal Ministry of Education and Research of Germany in the project ScaDS (01IS14025A).", "label": "0", "is_selected": "0", "text": "Modern pattern recognition methods are based on convolutional networks since they are able to learn complex patterns that benefit the classification. However, convolutional networks are computationally expensive and require a great deal of training data. In this paper we address these problems by introducing a novel convolutional network with efficient learning and training procedure, called Convolutional Neural Network with Exponential Kernel Function (ConvNekf). ConvNekf is based on Neural Networks with Exponential Kernel Function (Nekf) and convolutions. This combination of Nekf and convolutions results in an efficient and powerful convolutional network. ConvNekf consists of the convolutional layer, the max-pooling layer and the Nekf layer. For the training, the Nekf layer uses an efficient backpropagation algorithm with stochastic learning rate. The Nekf layer uses exponential kernel functions to represent the weights of the neurons. We experimentally demonstrate the performance of the proposed ConvNekf on the MNIST dataset, and on the synthetic dataset. The proposed ConvNekf achieves higher accuracy and lower computational time compared to other convolutional networks. This work is supported by the Federal Ministry of Education and Research of Germany in the project ScaDS (01IS14025A)."}
{"original_text": "Most of the current action recognition algorithms are based on deep networks which stack multiple convolutional, pooling and fully connected layers. While convolutional and fully connected operations have been widely studied in literature, pooling layers are often considered as a black box. In this paper, we aim to bridge this gap by studying pooling layers in depth. We provide a theoretical analysis of pooling operations and propose a novel pooling layer architecture, called Scale-Invariant Pooling (SIP) layer. We also show that SIP layer can be more efficient in terms of computational complexity than Max-Pooling. We show that SIP layers can be used to improve the classification accuracy of a ResNet model in a challenging scene understanding dataset and we propose a new architecture that is more computationally efficient. The authors would like to thank the anonymous reviewers for their valuable feedback and suggestions. This work was funded by the GENESIS European project (H2020-ICT-2014-1, 645134) and the European Union's Horizon 2020 research and innovation programme under grant agreement No 644599 (Marie Curie ITN InVisage).", "label": "0", "is_selected": "0", "text": "Most of the current action recognition algorithms are based on deep networks which stack multiple convolutional, pooling and fully connected layers. While convolutional and fully connected operations have been widely studied in literature, pooling layers are often considered as a black box. In this paper, we aim to bridge this gap by studying pooling layers in depth. We provide a theoretical analysis of pooling operations and propose a novel pooling layer architecture, called Scale-Invariant Pooling (SIP) layer. We also show that SIP layer can be more efficient in terms of computational complexity than Max-Pooling. We show that SIP layers can be used to improve the classification accuracy of a ResNet model in a challenging scene understanding dataset and we propose a new architecture that is more computationally efficient. The authors would like to thank the anonymous reviewers for their valuable feedback and suggestions. This work was funded by the GENESIS European project (H2020-ICT-2014-1, 645134) and the European Union's Horizon 2020 research and innovation programme under grant agreement No 644599 (Marie Curie ITN InVisage)."}
{"original_text": "Visible light communications (VLC) have emerged as strong candidates for meeting the escalating demand for high data rates. Consider a VLC network, where multiple access-points (APs) serve both energy-harvesting users and conventional users, using the light from the APs. We consider a system in which the harvested energy is used for wireless information transfer, and develop a simple analytical framework for evaluating the outage probability (OP) and the average bit energy (ABE) per bit transmitted. In this paper, we develop a simple analytical framework for evaluating the OP and the ABE of the harvest-and-transmit (HAT) scheme. We analyze the impact of the amount of energy harvested by the user, its distance from the APs, and its transmission duration on the system performance. We also derive the optimal transmission duration. Our analytical results are validated by extensive Monte-Carlo simulations. Visible light communications (VLC) have emerged as a promising candidate for meeting the growing demand for high data rates [1-4]. In a typical VLC network, the access point (AP) consists of a light-emitting diode (LED) used to transmit data to the users. A user can harvest energy from the light emitted by the APs to perform wireless information transfer to a destination [4,5]. This form of energy harvesting from the light emitted by the APs is known as harvest-and-transmit (HAT) [6,7]. In the HAT scheme, the harvested energy is used to perform wireless information transfer. The authors in consider a VLC network in which the users are equipped with energy harvesting capability, but they do not consider a system in which the harvested energy is used for wireless information transfer. For the first time, we consider a system in which the harvested energy is used for wireless information transfer, and develop a simple analytical framework for evaluating the outage probability (OP) and the average bit energy (ABE) per bit transmitted. The OP is the probability that a user fails to decode the transmitted information. The ABE is the amount of energy consumed for transmitting a bit of information from the user to the destination. In and , the authors consider a relay network that consists of energy-harvesting nodes, and develop analytical frameworks to evaluate the OP and the ABE of the HAT scheme. In , the authors consider a relay network in which the energy harvested from the ambient RF signals is used for wireless information transfer. The authors in consider a wireless relay network, and present a framework for evaluating the OP and the ABE of the HAT scheme. The authors in consider a VLC network in which the users are equipped with energy harvesting capability, but they do not consider a system in which the harvested energy is used for wireless information transfer. To the best of our knowledge, this is the first work to consider a VLC network in which the harvested energy is used for wireless information transfer. In this paper, we consider a VLC network in which the users are equipped with energy harvesting capability, and analyze the impact of the amount of energy harvested by the user, its distance from the APs, and its transmission duration on the system performance. We derive a closed-form expression for the OP and the ABE of the HAT scheme. In addition, we derive the optimal transmission duration for minimizing the ABE. Our analytical results are validated by extensive Monte-Carlo simulations. The rest of the paper is organized as follows. In Section II, we describe the system model and the HAT scheme. In Section III, we derive a closed-form expression for the OP of the HAT scheme. In Section IV, we derive a closed-form expression for the ABE of the HAT scheme. In Section V, we derive the optimal transmission duration for minimizing the ABE. In Section VI, we evaluate the accuracy of the derived expressions by comparing them with the results of extensive Monte-Carlo simulations. In Section VII, we present our concluding remarks. We consider a VLC network that consists of multiple APs and multiple users. The APs transmit optical signals to the users. A user harvests the energy from the optical signals emitted by the APs. The amount of energy harvested by the user depends on the distance between the user and the APs, and the transmission duration. The harvested energy is used to perform wireless information transfer. A user is assumed to be equipped with a photodetector. The photodetector converts the optical signals emitted by the APs into electrical signals. The output of the photodetector is then used by a", "label": "0", "is_selected": "1", "text": "We consider a system in which the harvested energy is used for wireless information transfer, and develop a simple analytical framework for evaluating the outage probability (OP) and the average bit energy (ABE) per bit transmitted."}
{"original_text": "Fingerprint verification systems are becoming ubiquitous in everyday life. This trend is propelled especially by the proliferation of mobile devices with fingerprint sensors such as smartphones and tablet computers, and by a wide range of potential applications including large-scale national identification, border control, law enforcement, online banking, online e-commerce, etc. In this context, strong protection against biometric spoof attacks is essential. Compared to other biometric modalities (e.g., iris, face, hand geometry), fingerprint spoofing is relatively easy, because 3D fingerprints can be readily captured or even lifted from objects or surfaces and then reproduced to fool a biometric system. Thus, developing robust fingerprint spoof detection systems is critical to protecting fingerprint biometric systems from spoof attacks. In this talk, I will present recent advances in fingerprint spoof detection. In particular, I will discuss fingerprint liveness detection based on physical characteristics of fingerprints that are hard to spoof, such as finger skin texture, finger temperature, and finger pulse. I will also present a cascade fingerprint spoofing detection system, which utilizes fingerprint ridge maps, fingerprint fingerprints, and fingerprint minutiae to achieve spoof detection with a high accuracy. Finally, I will present our efforts to collect a large-scale multimodal spoofing database and our collaborative work with the US FBI to develop a fingerprint spoof detection system for the next-generation fingerprint identification system (NG-FIPS). Dr. Min Jiang is a Distinguished Member of Technical Staff (DMTS) at ATT Labs - Research, Florham Park, NJ, USA, where she leads a team working on security and privacy in cyber-physical systems. Her research interests include biometric spoofing detection, biometrics security, multimodal biometrics, face recognition, and human behavior biometrics. She is the recipient of 2017 IEEE International Conference on Identity, Security and Behavior Analysis Best Paper Award, 2017 IEEE IJCB Best Paper Award, 2017 IEEE IJCB Best Student Paper Award, and 2016 IEEE Intelligent Transportation Systems Magazine Best Paper Award. She has published over 80 papers in the most prestigious international journals and conferences, including IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Information Forensics and Security, IEEE Transactions on Image Processing, IEEE Transactions on Circuits and Systems for Video Technology, IEEE Transactions on Multimedia, IEEE Transactions on Information Forensics and Security, IEEE Transactions on Information Technology in Biomedicine, IEEE Transactions on Biomedical Engineering, IEEE Journal of Selected Topics in Signal Processing, IEEE Transactions on Knowledge and Data Engineering, IEEE Transactions on Neural Networks and Learning Systems, IEEE Transactions on Biomedical Engineering, IEEE Transactions on Circuits and Systems Part I: Regular Papers, IEEE Transactions on Circuits and Systems Part II: Express Briefs, IEEE Transactions on Information Technology in Biomedicine, IEEE Transactions on Cybernetics, IEEE Transactions on Systems, Man, and Cybernetics Part B: Cybernetics, IEEE Transactions on Smart Grid, IEEE Transactions on Vehicular Technology, IEEE Transactions on Instrumentation and Measurement, IEEE Transactions on Image Processing, IEEE Transactions on Information Forensics and Security, IEEE Transactions on Computational Intelligence and AI in Games, IEEE Transactions on Multimedia, IEEE Transactions on Big Data, IEEE Transactions on Knowledge and Data Engineering, IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Information Forensics and Security, IEEE Transactions on Information Technology in Biomedicine, IEEE Transactions on Systems, Man, and Cybernetics: Part B, IEEE Transactions on Affective Computing, IEEE Transactions on Image Processing, IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Systems, Man, and Cybernetics: Part A, IEEE Transactions on Information Forensics and Security, IEEE Transactions on Information Technology in Biomedicine, IEEE Transactions on Affective Computing, IEEE Transactions on Information Forensics and Security, IEEE Transactions on Image Processing, IEEE Transactions on Circuits and Systems for Video Technology, IEEE Transactions on Circuits", "label": "0", "is_selected": "0", "text": "Fingerprint verification systems are becoming ubiquitous in everyday life. This trend is propelled especially by the proliferation of mobile devices with fingerprint sensors such as smartphones and tablet computers, and by a wide range of potential applications including large-scale national identification, border control, law enforcement, online banking, online e-commerce, etc. In this context, strong protection against biometric spoof attacks is essential. Compared to other biometric modalities (e.g., iris, face, hand geometry), fingerprint spoofing is relatively easy, because 3D fingerprints can be readily captured or even lifted from objects or surfaces and then reproduced to fool a biometric system. Thus, developing robust fingerprint spoof detection systems is critical to protecting fingerprint biometric systems from spoof attacks. In this talk, I will present recent advances in fingerprint spoof detection. In particular, I will discuss fingerprint liveness detection based on physical characteristics of fingerprints that are hard to spoof, such as finger skin texture, finger temperature, and finger pulse. I will also present a cascade fingerprint spoofing detection system, which utilizes fingerprint ridge maps, fingerprint fingerprints, and fingerprint minutiae to achieve spoof detection with a high accuracy. Finally, I will present our efforts to collect a large-scale multimodal spoofing database and our collaborative work with the US FBI to develop a fingerprint spoof detection system for the next-generation fingerprint identification system (NG-FIPS). Dr. Min Jiang is a Distinguished Member of Technical Staff (DMTS) at ATT Labs - Research, Florham Park, NJ, USA, where she leads a team working on security and privacy in cyber-physical systems. Her research interests include biometric spoofing detection, biometrics security, multimodal biometrics, face recognition, and human behavior biometrics. She is the recipient of 2017 IEEE International Conference on Identity, Security and Behavior Analysis Best Paper Award, 2017 IEEE IJCB Best Paper Award, 2017 IEEE IJCB Best Student Paper Award, and 2016 IEEE Intelligent Transportation Systems Magazine Best Paper Award. She has published over 80 papers in the most prestigious international journals and conferences, including IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Information Forensics and Security, IEEE Transactions on Image Processing, IEEE Transactions on Circuits and Systems for Video Technology, IEEE Transactions on Multimedia, IEEE Transactions on Information Forensics and Security, IEEE Transactions on Information Technology in Biomedicine, IEEE Transactions on Biomedical Engineering, IEEE Journal of Selected Topics in Signal Processing, IEEE Transactions on Knowledge and Data Engineering, IEEE Transactions on Neural Networks and Learning Systems, IEEE Transactions on Biomedical Engineering, IEEE Transactions on Circuits and Systems Part I: Regular Papers, IEEE Transactions on Circuits and Systems Part II: Express Briefs, IEEE Transactions on Information Technology in Biomedicine, IEEE Transactions on Cybernetics, IEEE Transactions on Systems, Man, and Cybernetics Part B: Cybernetics, IEEE Transactions on Smart Grid, IEEE Transactions on Vehicular Technology, IEEE Transactions on Instrumentation and Measurement, IEEE Transactions on Image Processing, IEEE Transactions on Information Forensics and Security, IEEE Transactions on Computational Intelligence and AI in Games, IEEE Transactions on Multimedia, IEEE Transactions on Big Data, IEEE Transactions on Knowledge and Data Engineering, IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Information Forensics and Security, IEEE Transactions on Information Technology in Biomedicine, IEEE Transactions on Systems, Man, and Cybernetics: Part B, IEEE Transactions on Affective Computing, IEEE Transactions on Image Processing, IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Systems, Man, and Cybernetics: Part A, IEEE Transactions on Information Forensics and Security, IEEE Transactions on Information Technology in Biomedicine, IEEE Transactions on Affective Computing, IEEE Transactions on Information Forensics and Security, IEEE Transactions on Image Processing, IEEE Transactions on Circuits and Systems for Video Technology, IEEE Transactions on Circuits"}
{"original_text": "Analysis of opinion dynamics in social networks plays an important role in today's life. For applications such as predicting users' political preference, it is particularly important to be able to distinguish between popular and unpopular opinions in the network. In this paper we study the problem of detecting popular and unpopular opinions in social networks. Our goal is to design a computationally feasible method, which enables us to quickly detect popular and unpopular opinions in the network, with high probability. We propose a test that allows us to classify each opinion as popular or unpopular, with a given probability of error. Moreover, we show how to compute the probabilities of popularity and unpopularity of each opinion, while keeping the number of probes of each individual's opinion small. We show that our test is close to optimal in terms of the number of probes required to estimate the probability of each opinion being popular or unpopular. We also describe how to detect unpopular opinions with high probability, using a computationally efficient procedure. Ronitt Rubinfeld, Eyal Piatnitsky, On the Detection of Popular and Unpopular Opinions in Social Networks, Proceedings of the 50th Annual Symposium on Foundations of Computer Science, 2019, pp. 310-321.", "label": "0", "is_selected": "0", "text": "Analysis of opinion dynamics in social networks plays an important role in today's life. For applications such as predicting users' political preference, it is particularly important to be able to distinguish between popular and unpopular opinions in the network. In this paper we study the problem of detecting popular and unpopular opinions in social networks. Our goal is to design a computationally feasible method, which enables us to quickly detect popular and unpopular opinions in the network, with high probability. We propose a test that allows us to classify each opinion as popular or unpopular, with a given probability of error. Moreover, we show how to compute the probabilities of popularity and unpopularity of each opinion, while keeping the number of probes of each individual's opinion small. We show that our test is close to optimal in terms of the number of probes required to estimate the probability of each opinion being popular or unpopular. We also describe how to detect unpopular opinions with high probability, using a computationally efficient procedure. Ronitt Rubinfeld, Eyal Piatnitsky, On the Detection of Popular and Unpopular Opinions in Social Networks, Proceedings of the 50th Annual Symposium on Foundations of Computer Science, 2019, pp. 310-321."}
{"original_text": "Unlike nonconvex optimization, where gradient descent is guaranteed to converge to a local optimizer, algorithms for nonconvex-nonconcave minimax optimization can have topologically different solution paths: sometimes converging to a solution, sometimes never converging and instead following a limit cycle, and sometimes diverging. In this paper, we study the limiting behaviors of three classic minimax algorithms: gradient decent ascent (GDA), alternating gradient decent ascent (AGDA), and the extragradient method (EGM). Numerically, we observe that all of these limiting behaviors can arise in Generative Adversarial Networks (GAN) training. To explain these different behaviors, we study the high-order resolution continuous-time dynamics that correspond to each algorithm, which results in the sufficient (and almost necessary) conditions for the local convergence by each method. Moreover, this ODE perspective allows us to characterize the phase transition between these different limiting behaviors caused by introducing regularization in the problem instance.", "label": "1", "is_selected": "0", "text": "Unlike nonconvex optimization, where gradient descent is guaranteed to converge to a local optimizer, algorithms for nonconvex-nonconcave minimax optimization can have topologically different solution paths: sometimes converging to a solution, sometimes never converging and instead following a limit cycle, and sometimes diverging. In this paper, we study the limiting behaviors of three classic minimax algorithms: gradient decent ascent (GDA), alternating gradient decent ascent (AGDA), and the extragradient method (EGM). Numerically, we observe that all of these limiting behaviors can arise in Generative Adversarial Networks (GAN) training. To explain these different behaviors, we study the high-order resolution continuous-time dynamics that correspond to each algorithm, which results in the sufficient (and almost necessary) conditions for the local convergence by each method. Moreover, this ODE perspective allows us to characterize the phase transition between these different limiting behaviors caused by introducing regularization in the problem instance."}
{"original_text": "We propose a machine learning framework to synthesize reactive controllers for systems whose interactions with their adversarial environment are modeled by infinite-duration, two-player games over (potentially) infinite graphs. Our framework targets safety games with infinitely many vertices, but it is also applicable to safety games over finite graphs whose size is too prohibitive for conventional synthesis techniques. The learning takes place in a feedback loop between a teacher component, which can reason symbolically about the safety game, and a learning algorithm, which successively learns an approximation of the winning region from various kinds of examples provided by the teacher. We develop a novel decision tree learning algorithm for this setting and show that our algorithm is guaranteed to converge to a reactive safety controller if a suitable approximation of the winning region can be expressed as a decision tree. Finally, we empirically compare the performance of a prototype implementation to existing approaches, which are based on constraint solving and automata learning, respectively.", "label": "1", "is_selected": "0", "text": "We propose a machine learning framework to synthesize reactive controllers for systems whose interactions with their adversarial environment are modeled by infinite-duration, two-player games over (potentially) infinite graphs. Our framework targets safety games with infinitely many vertices, but it is also applicable to safety games over finite graphs whose size is too prohibitive for conventional synthesis techniques. The learning takes place in a feedback loop between a teacher component, which can reason symbolically about the safety game, and a learning algorithm, which successively learns an approximation of the winning region from various kinds of examples provided by the teacher. We develop a novel decision tree learning algorithm for this setting and show that our algorithm is guaranteed to converge to a reactive safety controller if a suitable approximation of the winning region can be expressed as a decision tree. Finally, we empirically compare the performance of a prototype implementation to existing approaches, which are based on constraint solving and automata learning, respectively."}
{"original_text": "We present RigNet, an end-to-end automated method for producing animation rigs from input character models. Given an input 3D model representing an articulated character, RigNet predicts a skeleton that matches the shape of the character and joint transformations that satisfy the required articulation constraints. We tackle this task by creating a neural network that jointly estimates the global joint transformations and the local muscle transformations, which serve to better approximate the input shape. We demonstrate that our network successfully estimates articulations for various character models and articulation constraints. We show that our approach can be used to deform meshes and produce rigging weights, and outperforms a standard model-to-model pose estimation pipeline.", "label": "0", "is_selected": "0", "text": "We present RigNet, an end-to-end automated method for producing animation rigs from input character models. Given an input 3D model representing an articulated character, RigNet predicts a skeleton that matches the shape of the character and joint transformations that satisfy the required articulation constraints. We tackle this task by creating a neural network that jointly estimates the global joint transformations and the local muscle transformations, which serve to better approximate the input shape. We demonstrate that our network successfully estimates articulations for various character models and articulation constraints. We show that our approach can be used to deform meshes and produce rigging weights, and outperforms a standard model-to-model pose estimation pipeline."}
{"original_text": "We propose a comprehensive nonlinear ODE-based thermo-hydraulic model of a district heating system featuring several heat producers, consumers and storage devices which are interconnected through a distribution network of meshed pipes. The model aims at describing the time evolution of the physical quantities involved in the system and in particular the water temperature. Our aim is to show that the proposed model is sufficiently flexible to be adapted to different situations, and that its solution is accurate enough to provide some indication of the time evolution of the temperature of the system. We present the model and its numerical solution with the aid of an extensive example in which the model is applied to the particular case of a district heating system located in Copenhagen, Denmark. The results are then validated through comparison with experimental data. Supported by: GNCS (grant number 2016-2017) - National Group for Scientific Computing, by the project TISU, funded by Regione Piemonte in the framework of the Fondi Fesr 2014-2020, by the European project HeatNet.", "label": "0", "is_selected": "0", "text": "We propose a comprehensive nonlinear ODE-based thermo-hydraulic model of a district heating system featuring several heat producers, consumers and storage devices which are interconnected through a distribution network of meshed pipes. The model aims at describing the time evolution of the physical quantities involved in the system and in particular the water temperature. Our aim is to show that the proposed model is sufficiently flexible to be adapted to different situations, and that its solution is accurate enough to provide some indication of the time evolution of the temperature of the system. We present the model and its numerical solution with the aid of an extensive example in which the model is applied to the particular case of a district heating system located in Copenhagen, Denmark. The results are then validated through comparison with experimental data. Supported by: GNCS (grant number 2016-2017) - National Group for Scientific Computing, by the project TISU, funded by Regione Piemonte in the framework of the Fondi Fesr 2014-2020, by the European project HeatNet."}
{"original_text": "Based on 46 in-depth interviews with scientists, engineers, and CEOs, thisdocument presents a list of concrete machine research problems, progress onwhich would directly benefit tech ventures in East Africa.", "label": "1", "is_selected": "0", "text": "Based on 46 in-depth interviews with scientists, engineers, and CEOs, thisdocument presents a list of concrete machine research problems, progress onwhich would directly benefit tech ventures in East Africa."}
{"original_text": "Flexibility at hardware level is the main driving force behind adaptive systems whose aim is to realise microarhitecture deconfiguration 'online'. This feature allows the softwarehardware stack to tolerate drastic changes of the workload in data centres. With emerge of FPGA reconfigurablity this technology is becoming a mainstream computing paradigm. Adaptivity is usually accompanied by the high-level tools to facilitate multi-dimensional space exploration. An essential aspect in this space is memory orchestration where on-chip and off-chip memory distribution significantly influences the architecture in coping with the critical spatial and timing constraints, e.g. Place Route. This paper proposes a memory smart technique for a particular class of adaptive systems: Elastic Circuits which enjoy slack elasticity at fine level of granularity. We explore retiming of a set of popular benchmarks via investigating the memory distribution within and among accelerators. The area, performance and power patterns are adopted by our high-level synthesis framework, with respect to the behaviour of the input descriptions, to improve the quality of the synthesised elastic circuits.", "label": "1", "is_selected": "0", "text": "Flexibility at hardware level is the main driving force behind adaptive systems whose aim is to realise microarhitecture deconfiguration 'online'. This feature allows the softwarehardware stack to tolerate drastic changes of the workload in data centres. With emerge of FPGA reconfigurablity this technology is becoming a mainstream computing paradigm. Adaptivity is usually accompanied by the high-level tools to facilitate multi-dimensional space exploration. An essential aspect in this space is memory orchestration where on-chip and off-chip memory distribution significantly influences the architecture in coping with the critical spatial and timing constraints, e.g. Place Route. This paper proposes a memory smart technique for a particular class of adaptive systems: Elastic Circuits which enjoy slack elasticity at fine level of granularity. We explore retiming of a set of popular benchmarks via investigating the memory distribution within and among accelerators. The area, performance and power patterns are adopted by our high-level synthesis framework, with respect to the behaviour of the input descriptions, to improve the quality of the synthesised elastic circuits."}
{"original_text": "This paper proposes an adaptive human pilot model that is able to mimic the crossover model in the presence of uncertainties. The proposed structure is based on the model reference adaptive control, and the adaptive laws are obtained using the Lyapunov-Krasovskii stability criteria. The model can be employed for human-in-the-loop stability and performance analyses incorporating different types of controllers and plant types. For validation purposes, an experimental setup is employed to collect data and a statistical analysis is conducted to measure the predictive power of the pilot model.", "label": "1", "is_selected": "0", "text": "This paper proposes an adaptive human pilot model that is able to mimic the crossover model in the presence of uncertainties. The proposed structure is based on the model reference adaptive control, and the adaptive laws are obtained using the Lyapunov-Krasovskii stability criteria. The model can be employed for human-in-the-loop stability and performance analyses incorporating different types of controllers and plant types. For validation purposes, an experimental setup is employed to collect data and a statistical analysis is conducted to measure the predictive power of the pilot model."}
{"original_text": "We present the Latvian Twitter Eater Corpus - a set of tweets in the narrow domain related to food, drinks, eating and drinking. The corpus has been collected over time-span of over 8 years and includes over 2 million tweets entailed with additional useful data. We also separate two sub-corpora of question and answer tweets and sentiment annotated tweets. We analyse contents of the corpus and demonstrate use-cases for the sub-corpora by training domain-specific question-answering and sentiment-analysis models using data from the corpus.", "label": "1", "is_selected": "0", "text": "We present the Latvian Twitter Eater Corpus - a set of tweets in the narrow domain related to food, drinks, eating and drinking. The corpus has been collected over time-span of over 8 years and includes over 2 million tweets entailed with additional useful data. We also separate two sub-corpora of question and answer tweets and sentiment annotated tweets. We analyse contents of the corpus and demonstrate use-cases for the sub-corpora by training domain-specific question-answering and sentiment-analysis models using data from the corpus."}
{"original_text": "MapReduce is a popular programming paradigm for developing large-scale, data-intensive computation. Many frameworks that implement this paradigm have recently been developed. To leverage these frameworks, however, developers must become familiar with the unique features of MapReduce and adjust their applications to fit the paradigm. In this paper, we propose a technique for automating the translation of a high-level MapReduce application into an optimized implementation. Our technique automatically chooses the number and configuration of reduce tasks and map tasks in the application. The approach also performs a static analysis of the application and generates an optimized map function based on the analysis. We also develop a prototype implementation of our technique. Junfeng Zhao is a Ph.D. student in Computer Science at University of Illinois at Urbana-Champaign. He received his B.S. degree in Computer Science from University of Science and Technology of China, in 2007. His research interest is in the area of database systems. Shao-Wen Wang is an Assistant Professor in the Department of Computer Science at University of Illinois at Urbana-Champaign. He received his Ph.D. in Computer Science from Carnegie Mellon University. His research interests are in data management, including data warehousing and OLAP, query optimization, and data integration.", "label": "0", "is_selected": "1", "text": "In this paper, we present a novel technique for developing high-level MapReduce applications, which can be easily adapted to fit the needs of a wide range of applications."}
{"original_text": "Besides advanced telecommunications techniques, the most prominent evolution of wireless networks is the densification of network deployment. In particular, the increasing access pointsusers density and reduced cell size significantly enhance spatial reuse, thereby improving network capacity. Nevertheless, does network ultra-densification and over-deployment always boost the performance of wireless networks? Since the distance from transmitters to receivers is greatly reduced in dense networks, signal is more likely to be propagated from far- to near-field region. Without considering near-field propagation features, conventional understandings of the impact of network densification become doubtful. With this regard, it is imperative to reconsider the pros and cons brought by network densification. In this article, we first discuss the near-field propagation features in densely deployed network and verify through experimental results the validity of the proposed near-field propagation model. Considering near-field propagation, we further explore how dense is ultra-dense for wireless networks and provide a concrete interpretation of ultra-densification from the spatial throughput perspective. Meanwhile, as near-field propagation makes interference more complicated and difficult to handle, we shed light on the key challenges of applying interference management in ultra-dense wireless networks. Moreover, possible solutions are presented to suggest future directions.", "label": "1", "is_selected": "0", "text": "Besides advanced telecommunications techniques, the most prominent evolution of wireless networks is the densification of network deployment. In particular, the increasing access pointsusers density and reduced cell size significantly enhance spatial reuse, thereby improving network capacity. Nevertheless, does network ultra-densification and over-deployment always boost the performance of wireless networks? Since the distance from transmitters to receivers is greatly reduced in dense networks, signal is more likely to be propagated from far- to near-field region. Without considering near-field propagation features, conventional understandings of the impact of network densification become doubtful. With this regard, it is imperative to reconsider the pros and cons brought by network densification. In this article, we first discuss the near-field propagation features in densely deployed network and verify through experimental results the validity of the proposed near-field propagation model. Considering near-field propagation, we further explore how dense is ultra-dense for wireless networks and provide a concrete interpretation of ultra-densification from the spatial throughput perspective. Meanwhile, as near-field propagation makes interference more complicated and difficult to handle, we shed light on the key challenges of applying interference management in ultra-dense wireless networks. Moreover, possible solutions are presented to suggest future directions."}
{"original_text": "The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sentence along with its syntactic tree, have shown state-of-the-art performance. Recent work shows that a multi-layer LSTM language model outperforms competitive statistical syntactic linearization systems without using syntax. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task.", "label": "1", "is_selected": "0", "text": "The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sentence along with its syntactic tree, have shown state-of-the-art performance. Recent work shows that a multi-layer LSTM language model outperforms competitive statistical syntactic linearization systems without using syntax. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task."}
{"original_text": "We propose a novel biologically-plausible solution to the credit assignment problem, being motivated by observations in the ventral visual pathway and trained deep neural networks. In both, representations of objects in the same category become progressively more similar, while objects belonging to different categories becomes less similar. We use this observation to motivate a layer-specific learning goal in a deep network: each layer aims to learn a representational similarity matrix that interpolates between previous and later layers. We formulate this idea using a supervised deep similarity matching cost function and derive from it deep neural networks with feedforward, lateral and feedback connections, and neurons that exhibit biologically-plausible Hebbian and anti-Hebbian plasticity. Supervised deep similarity matching can be interpreted as an energy-based learning algorithm, but with significant differences from others in how a contrastive function is constructed.", "label": "1", "is_selected": "0", "text": "We propose a novel biologically-plausible solution to the credit assignment problem, being motivated by observations in the ventral visual pathway and trained deep neural networks. In both, representations of objects in the same category become progressively more similar, while objects belonging to different categories becomes less similar. We use this observation to motivate a layer-specific learning goal in a deep network: each layer aims to learn a representational similarity matrix that interpolates between previous and later layers. We formulate this idea using a supervised deep similarity matching cost function and derive from it deep neural networks with feedforward, lateral and feedback connections, and neurons that exhibit biologically-plausible Hebbian and anti-Hebbian plasticity. Supervised deep similarity matching can be interpreted as an energy-based learning algorithm, but with significant differences from others in how a contrastive function is constructed."}
{"original_text": "This paper presents models for transforming standard reversible circuits into Linear Nearest Neighbor (LNN) architecture without inserting SWAP gates. Templates to optimize the transformed LNN circuits are proposed. All minimal reversible circuits can be transformed to LNN architecture without adding extra SWAP gates using the proposed templates. The experimental results show that the transformed LNN circuits are much more area efficient than their reversible counterparts. W. Liu, B. Yu, J. Wang, and X. Zhou, \"Standard Reversible Circuits to Linear Nearest Neighbor Architecture,\" J. Adv. Comput. Intell. Intell. Inform., Vol.15, No.4, pp. 422-430, 2011. E. Fredkin and T. Toffoli, \"Conservative logic,\" Int. J. Theor. Phys., Vol.21, pp. 219-253, 1982. R. Feynman, \"Simulating physics with computers,\" Int. J. Theor. Phys., Vol.21, pp. 467-488, 1982. R. Landauer, \"Irreversibility and heat generation in the computing process,\" IBM J. Res. Develop., Vol.5, pp. 183-191, 1961. C. H. Bennett, \"Logical reversibility of computation,\" IBM J. Res. Develop., Vol.20, pp. 255-260, 1976. B. Y. Hwang, \"Logical reversible gates,\" IEEE Trans. Comput., Vol.30, No.9, pp. 653-660, 1981. B. Y. Hwang and D. Y. Chou, \"Reversible logic circuits,\" IEEE Trans. Comput., Vol.34, No.6, pp. 652-660, 1985. S. Marquardt, D. A. Johnson, and R. H. Borkar, \"Reversible logic and arithmetic circuits,\" IEEE Trans. Comput., Vol.43, No.8, pp. 903-916, 1994. L. Ma and H. Ma, \"Reversible logic synthesis,\" IEEE Trans. Comput., Vol.52, No.1, pp. 54-65, 2003. S. Mendis, J. M. Rubinstein, K. S. Ng, and R. F. Vanderbei, \"Design and synthesis of reversible logic circuits,\" IEEE Trans. Comput., Vol.52, No.1, pp. 4-16, 2003. J. H. Yoo, Y. I. Song, and J. Lee, \"Reversible circuit synthesis using a genetic algorithm,\" IEEE Trans. Comput., Vol.52, No.1, pp. 17-27, 2003. M. P. Das, K. S. Ng, and R. F. Vanderbei, \"A genetic algorithm for reversible logic synthesis,\" Proc. IEEE Int. Symp. Circuits and Systems, Vol.1, pp. 477-480, 2000. M. S. Alam, S. Biswas, D. K. Ray, A. Nayak, and D. K. Pati, \"Genetic programming based reversible logic synthesis,\" Proc. Int. Symp. Circuits and Systems, Vol.4, pp. 474-477, 2005. C. A. Booth, \"Optimal reversible circuit layout: The linear nearest-neighbor architecture,\" Phys. Rev. A, Vol.53, pp. 2534-2547, 1996. G. D. Dodd and T. Y. Siu, \"A new reversible logic synthesis algorithm based on linear nearest-neighbor architecture,\" IEEE Trans. Comput., Vol.56, No.10, pp. , 2007. G. D. Dodd, \"Linear nearest-neighbor reversible circuit synthesis with interleaving,\" Proc. IEEE Int. Symp. Circuits and Systems, Vol.2, pp. 7", "label": "0", "is_selected": "1", "text": "Reversible circuits have been widely used in computer systems for decades, but their design has been complicated by the need to add extra gates at the gate junctions to reduce power consumption and noise."}
{"original_text": "Batch normalization (BN) has become a standard technique for training the modern deep networks. However, its effectiveness diminishes when the batch size becomes smaller, since the batch statistics estimation becomes inaccurate. That hinders batch normalization's usage for 1) training larger model which requires small batches constrained by memory consumption, 2) training on mobile or embedded devices of which the memory resource is limited. In this paper, we propose a simple but effective method, called extended batch normalization (EBN). For NCHW format feature maps, extended batch normalization computes the mean along the (N, H, W) dimensions, as the same as batch normalization, to maintain the advantage of batch normalization. To alleviate the problem caused by small batch size, extended batch normalization computes the standard deviation along the (N, C, H, W) dimensions, thus enlarges the number of samples from which the standard deviation is computed. We compare extended batch normalization with batch normalization and group normalization on the datasets of MNIST, CIFAR-10100, STL-10, and ImageNet, respectively. The experiments show that extended batch normalization alleviates the problem of batch normalization with small batch size while achieving close performances to batch normalization with large batch size.", "label": "1", "is_selected": "0", "text": "Batch normalization (BN) has become a standard technique for training the modern deep networks. However, its effectiveness diminishes when the batch size becomes smaller, since the batch statistics estimation becomes inaccurate. That hinders batch normalization's usage for 1) training larger model which requires small batches constrained by memory consumption, 2) training on mobile or embedded devices of which the memory resource is limited. In this paper, we propose a simple but effective method, called extended batch normalization (EBN). For NCHW format feature maps, extended batch normalization computes the mean along the (N, H, W) dimensions, as the same as batch normalization, to maintain the advantage of batch normalization. To alleviate the problem caused by small batch size, extended batch normalization computes the standard deviation along the (N, C, H, W) dimensions, thus enlarges the number of samples from which the standard deviation is computed. We compare extended batch normalization with batch normalization and group normalization on the datasets of MNIST, CIFAR-10100, STL-10, and ImageNet, respectively. The experiments show that extended batch normalization alleviates the problem of batch normalization with small batch size while achieving close performances to batch normalization with large batch size."}
{"original_text": "Computed tomography (CT) is critical for various clinical applications, e.g., radiotherapy treatment planning and also PET attenuation correction. However, CT exposes radiation during acquisition, which may cause side effects to patients. Compared to CT, magnetic resonance imaging (MRI) is much safer and does not involve any radiations. Therefore, recently, researchers are greatly motivated to estimate CT image from its corresponding MR image of the same subject for the case of radiotherapy planning. In this paper, we propose a data-driven approach to address this challenging problem. Specifically, we train a fully convolutional network to generate CT given an MR image. To better model the nonlinear relationship from MRI to CT and to produce more realistic images, we propose to use the adversarial training strategy and an image gradient difference loss function. We further apply AutoContext Model to implement a context-aware generative adversarial network. Experimental results show that our method is accurate and robust for predicting CT images from MRI images, and also outperforms three state-of-the-art methods under comparison.", "label": "1", "is_selected": "0", "text": "Computed tomography (CT) is critical for various clinical applications, e.g., radiotherapy treatment planning and also PET attenuation correction. However, CT exposes radiation during acquisition, which may cause side effects to patients. Compared to CT, magnetic resonance imaging (MRI) is much safer and does not involve any radiations. Therefore, recently, researchers are greatly motivated to estimate CT image from its corresponding MR image of the same subject for the case of radiotherapy planning. In this paper, we propose a data-driven approach to address this challenging problem. Specifically, we train a fully convolutional network to generate CT given an MR image. To better model the nonlinear relationship from MRI to CT and to produce more realistic images, we propose to use the adversarial training strategy and an image gradient difference loss function. We further apply AutoContext Model to implement a context-aware generative adversarial network. Experimental results show that our method is accurate and robust for predicting CT images from MRI images, and also outperforms three state-of-the-art methods under comparison."}
{"original_text": "Recommender systems (RS) are increasingly present in our daily lives, especially since the advent of Big Data, which allows for storing all kinds of information about users' preferences. Personalized RS are successfully applied in platforms such as Netflix, Amazon or YouTube. However, they are missing in gastronomic platforms such as TripAdvisor, where moreover we can find millions of images tagged with users' tastes. This paper explores the potential of using those images as sources of information for modeling users' tastes and proposes an image-based classification system to obtain personalized recommendations, using a convolutional autoencoder as feature extractor. The proposed architecture will be applied to TripAdvisor data, using users' reviews that can be defined as a triad composed by a user, a restaurant, and an image of it taken by the user. Since the dataset is highly unbalanced, the use of data augmentation on the minority class is also considered in the experimentation. Results on data from three cities of different sizes (Santiago de Compostela, Barcelona and New York) demonstrate the effectiveness of using a convolutional autoencoder as feature extractor, instead of the standard deep features computed with convolutional neural networks.", "label": "1", "is_selected": "0", "text": "Recommender systems (RS) are increasingly present in our daily lives, especially since the advent of Big Data, which allows for storing all kinds of information about users' preferences. Personalized RS are successfully applied in platforms such as Netflix, Amazon or YouTube. However, they are missing in gastronomic platforms such as TripAdvisor, where moreover we can find millions of images tagged with users' tastes. This paper explores the potential of using those images as sources of information for modeling users' tastes and proposes an image-based classification system to obtain personalized recommendations, using a convolutional autoencoder as feature extractor. The proposed architecture will be applied to TripAdvisor data, using users' reviews that can be defined as a triad composed by a user, a restaurant, and an image of it taken by the user. Since the dataset is highly unbalanced, the use of data augmentation on the minority class is also considered in the experimentation. Results on data from three cities of different sizes (Santiago de Compostela, Barcelona and New York) demonstrate the effectiveness of using a convolutional autoencoder as feature extractor, instead of the standard deep features computed with convolutional neural networks."}
{"original_text": "Both feature selection and hyperparameter tuning are key tasks in machine learning. Hyperparameter tuning is often useful to increase model performance, while feature selection is undertaken to attain sparse models. In this paper, we introduce a general sparse optimization framework for both tasks, which is built on the assumption of unimodal and continuous objective functions. This framework includes several popular sparse optimization techniques, such as group lasso, sparse lasso, and orthogonal matching pursuit. In addition, our framework provides a common and unified optimization formulation, which can be solved by using efficient optimization techniques. Moreover, it is also amenable to theoretical analysis. We have implemented our framework, and obtained promising experimental results. 2012 Springer-Verlag Berlin Heidelberg.", "label": "0", "is_selected": "1", "text": "In this paper, we introduce a general sparse optimization framework for two key tasks in machine learning, namely feature tuning and hyperparameter tuning, and we show that it is amenable to theoretical analysis."}
{"original_text": "In this paper, we design and experiment a far-field wireless power transfer (WPT) architecture based on distributed antennas, so-called WPT DAS, that dynamically selects transmit antenna and frequency to increase the output dc power. Uniquely, spatial and frequency diversities are jointly exploited in the proposed WPT DAS with low complexity, low cost, and flexible deployment to combat the wireless fading channel. A numerical experiment is designed to show the benefits using antenna and frequency selections in spatially and frequency selective fading channels for single-user and multi-user cases. Accordingly, the proposed WPT DAS for single-user and two-user cases is prototyped. At the transmitter, we adopt antenna selection to exploit spatial diversity and adopt frequency selection to exploit frequency diversity. A low-complexity over-the-air limited feedback using an IEEE 802.15.4 RF interface is designed for antenna and frequency selections and reporting from the receiver to the transmitter. The proposed WPT DAS prototype is demonstrated in a real indoor environment. The measurements show that WPT DAS can boost the output dc power by up to 30 dB in single-user case and boost the sum of output dc power by up to 21.8 dB in two-user case and broaden the service coverage area in a low cost, low complexity, and flexible manner.", "label": "1", "is_selected": "0", "text": "In this paper, we design and experiment a far-field wireless power transfer (WPT) architecture based on distributed antennas, so-called WPT DAS, that dynamically selects transmit antenna and frequency to increase the output dc power. Uniquely, spatial and frequency diversities are jointly exploited in the proposed WPT DAS with low complexity, low cost, and flexible deployment to combat the wireless fading channel. A numerical experiment is designed to show the benefits using antenna and frequency selections in spatially and frequency selective fading channels for single-user and multi-user cases. Accordingly, the proposed WPT DAS for single-user and two-user cases is prototyped. At the transmitter, we adopt antenna selection to exploit spatial diversity and adopt frequency selection to exploit frequency diversity. A low-complexity over-the-air limited feedback using an IEEE 802.15.4 RF interface is designed for antenna and frequency selections and reporting from the receiver to the transmitter. The proposed WPT DAS prototype is demonstrated in a real indoor environment. The measurements show that WPT DAS can boost the output dc power by up to 30 dB in single-user case and boost the sum of output dc power by up to 21.8 dB in two-user case and broaden the service coverage area in a low cost, low complexity, and flexible manner."}
{"original_text": "A novel method for distributed estimation of the frequency of power systems is introduced based on the cooperation between multiple measurement nodes. The proposed distributed widely linear complex Kalman filter (D-ACKF) and the distributed widely linear extended complex Kalman filter (D-AECKF) employ a widely linear state space and augmented complex statistics to deal with unbalanced system conditions and the generality complex signals, both second order circular (proper) and second order noncircular (improper). It is shown that the current, strictly linear, estimators are inadequate for unbalanced systems, a typical case in smart grids, as they do not account for either the noncircularity of Clarke's a b voltage in unbalanced conditions or the correlated nature of nodal disturbances. We illuminate the relationship between the degree of circularity of Clarke's voltage and system imbalance, and prove that the proposed widely linear estimators are optimal for such conditions, while also accounting for the correlated and noncircular nature of real-world nodal disturbances. Synthetic and real world case studies over a range of power system conditions illustrate the theoretical and practical advantages of the proposed methodology.", "label": "1", "is_selected": "0", "text": "A novel method for distributed estimation of the frequency of power systems is introduced based on the cooperation between multiple measurement nodes. The proposed distributed widely linear complex Kalman filter (D-ACKF) and the distributed widely linear extended complex Kalman filter (D-AECKF) employ a widely linear state space and augmented complex statistics to deal with unbalanced system conditions and the generality complex signals, both second order circular (proper) and second order noncircular (improper). It is shown that the current, strictly linear, estimators are inadequate for unbalanced systems, a typical case in smart grids, as they do not account for either the noncircularity of Clarke's a b voltage in unbalanced conditions or the correlated nature of nodal disturbances. We illuminate the relationship between the degree of circularity of Clarke's voltage and system imbalance, and prove that the proposed widely linear estimators are optimal for such conditions, while also accounting for the correlated and noncircular nature of real-world nodal disturbances. Synthetic and real world case studies over a range of power system conditions illustrate the theoretical and practical advantages of the proposed methodology."}
{"original_text": "Encoding a sequence of observations is an essential task with many applications. The encoding can become highly efficient when the observations are generated by a dynamical system. A dynamical system imposes regularities on the observations that can be leveraged to achieve a more efficient code. We propose a method to encode a given or learned dynamical system. Apart from its application for encoding a sequence of observations, we propose to use the compression achieved by this encoding as a criterion for model selection. Given a dataset, different learning algorithms result in different models. But not all learned models are equally good. We show that the proposed encoding approach can be used to choose the learned model which is closer to the true underlying dynamics. We provide experiments for both encoding and model selection, and theoretical results that shed light on why the approach works.", "label": "1", "is_selected": "0", "text": "Encoding a sequence of observations is an essential task with many applications. The encoding can become highly efficient when the observations are generated by a dynamical system. A dynamical system imposes regularities on the observations that can be leveraged to achieve a more efficient code. We propose a method to encode a given or learned dynamical system. Apart from its application for encoding a sequence of observations, we propose to use the compression achieved by this encoding as a criterion for model selection. Given a dataset, different learning algorithms result in different models. But not all learned models are equally good. We show that the proposed encoding approach can be used to choose the learned model which is closer to the true underlying dynamics. We provide experiments for both encoding and model selection, and theoretical results that shed light on why the approach works."}
{"original_text": "As inertial and visual sensors are becoming ubiquitous, visual-inertial navigation systems (VINS) have prevailed in a wide range of applications from mobile augmented reality to aerial navigation to autonomous driving, in part because of the complementary sensing capabilities and the decreasing costs and size of the sensors. In this paper, we survey thoroughly the research efforts taken in this field and strive to provide a concise but complete review of the related work - which is unfortunately missing in the literature while being greatly demanded by researchers and engineers - in the hope to accelerate the VINS research and beyond in our society as a whole.", "label": "1", "is_selected": "0", "text": "As inertial and visual sensors are becoming ubiquitous, visual-inertial navigation systems (VINS) have prevailed in a wide range of applications from mobile augmented reality to aerial navigation to autonomous driving, in part because of the complementary sensing capabilities and the decreasing costs and size of the sensors. In this paper, we survey thoroughly the research efforts taken in this field and strive to provide a concise but complete review of the related work - which is unfortunately missing in the literature while being greatly demanded by researchers and engineers - in the hope to accelerate the VINS research and beyond in our society as a whole."}
{"original_text": "Sequence set is a widely-used type of data source in a large variety of fields. A typical example is protein structure prediction, which takes an multiple sequence alignment (MSA) as input and returns a protein structure. A problem of interest is to design a learning algorithm which is able to efficiently learn from a set of labeled training sequences, so that the knowledge obtained from the training set can be applied to unseen test sequences. In this paper, we propose a novel learning approach to sequence set classification, which we refer to as sparse log-linear model. Specifically, our sparse log-linear model is a linear model where we assume that the classification score for a query sequence is composed of a sparse set of features. In contrast to the recently proposed sparse log-linear models for traditional data, our model is able to handle the unique challenges posed by sequence set data, where features can be either sequence-level features or sequence-pair-level features. To learn the sparse log-linear model from training data, we present an efficient optimization approach to optimize the objective function in a sequential forward stagewise manner. To evaluate the proposed approach, we conduct experiments on three important bioinformatics applications: protein structure prediction, protein fold recognition, and protein function prediction. Experimental results show that the proposed model outperforms several state-of-the-art models. 2014 Wenliang Chen, Ning Guo, Hai Lu, Jianlin Cheng and Qinglin Chen. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.", "label": "0", "is_selected": "1", "text": "In this paper, we propose a novel learning approach to sequence set classification, where we assume that the classification score for a query sequence is composed of a sparse set of features."}
{"original_text": "The field of automatic image inpainting has progressed rapidly in recent years, but no one has yet proposed a standard method of evaluating algorithms. This absence is due to the difficulties in defining meaningful metrics for evaluation of inpainting algorithms, as well as the lack of consensus regarding how inpainting algorithms should be used in practice. The standard choice for evaluation of an image processing algorithm is to compare its output with that of a human, but this is not the most meaningful choice for inpainting. In this paper we first clarify the concept of inpainting, and then propose a set of metrics for evaluating inpainting algorithms. The proposed metrics include two sets of quantitative measures. The first measures represent the visual quality of an inpainted image; the second measures compare an inpainted image with a reference image in order to detect common properties such as symmetries. We demonstrate the effectiveness of the metrics by evaluating three state-of-the-art inpainting algorithms, which utilize different approaches and produce very different results. We use the measures to quantify the performance of different algorithms and to predict the impact of different parameters on the output of an algorithm. We also use the measures to provide insight into how inpainting algorithms should be used in practice.", "label": "0", "is_selected": "0", "text": "The field of automatic image inpainting has progressed rapidly in recent years, but no one has yet proposed a standard method of evaluating algorithms. This absence is due to the difficulties in defining meaningful metrics for evaluation of inpainting algorithms, as well as the lack of consensus regarding how inpainting algorithms should be used in practice. The standard choice for evaluation of an image processing algorithm is to compare its output with that of a human, but this is not the most meaningful choice for inpainting. In this paper we first clarify the concept of inpainting, and then propose a set of metrics for evaluating inpainting algorithms. The proposed metrics include two sets of quantitative measures. The first measures represent the visual quality of an inpainted image; the second measures compare an inpainted image with a reference image in order to detect common properties such as symmetries. We demonstrate the effectiveness of the metrics by evaluating three state-of-the-art inpainting algorithms, which utilize different approaches and produce very different results. We use the measures to quantify the performance of different algorithms and to predict the impact of different parameters on the output of an algorithm. We also use the measures to provide insight into how inpainting algorithms should be used in practice."}
{"original_text": "We develop a well-balanced central-upwind scheme for rotating shallow water model with horizontal temperature andor density gradients - the thermal rotating shallow water (TRSW). The scheme is designed using the flux globalization approach: first, the source terms are incorporated into the fluxes, which results in a hyperbolic system with global fluxes; second, we apply the Riemann-problem-solver-free central-upwind scheme to the rewritten system. We ensure that the resulting method is well-balanced by switching off the numerical diffusion when the computed solution is near (at) thermo-geostrophic equilibria. The designed scheme is successfully tested on a series of numerical examples. Motivated by future applications to large-scale motions in the ocean and atmosphere, the model is considered on the tangent plane to a rotating planet both in mid-latitudes and at the Equator. The numerical scheme is shown to be capable of quite accurately maintaining the equilibrium states in the presence of nontrivial topography and rotation. Prior to numerical simulations, an analysis of the TRSW model based on the use of Lagrangian variables is presented, allowing one to obtain criteria of existence and uniqueness of the equilibrium state, of the wave-breaking and shock formation, and of instability development out of given initial conditions. The established criteria are confirmed in the conducted numerical experiments.", "label": "1", "is_selected": "0", "text": "We develop a well-balanced central-upwind scheme for rotating shallow water model with horizontal temperature andor density gradients - the thermal rotating shallow water (TRSW). The scheme is designed using the flux globalization approach: first, the source terms are incorporated into the fluxes, which results in a hyperbolic system with global fluxes; second, we apply the Riemann-problem-solver-free central-upwind scheme to the rewritten system. We ensure that the resulting method is well-balanced by switching off the numerical diffusion when the computed solution is near (at) thermo-geostrophic equilibria. The designed scheme is successfully tested on a series of numerical examples. Motivated by future applications to large-scale motions in the ocean and atmosphere, the model is considered on the tangent plane to a rotating planet both in mid-latitudes and at the Equator. The numerical scheme is shown to be capable of quite accurately maintaining the equilibrium states in the presence of nontrivial topography and rotation. Prior to numerical simulations, an analysis of the TRSW model based on the use of Lagrangian variables is presented, allowing one to obtain criteria of existence and uniqueness of the equilibrium state, of the wave-breaking and shock formation, and of instability development out of given initial conditions. The established criteria are confirmed in the conducted numerical experiments."}
{"original_text": "Currently, self-driving cars rely greatly on the Global Positioning System (GPS) infrastructure, albeit there is an increasing demand for alternative methods for GPS-denied environments. One of them is known as place recognition, which associates images of places with their corresponding positions. We previously proposed systems based on Weightless Neural Networks (WNN) to address this problem as a classification task. This encompasses solely one part of the global localization, which is not precise enough for driverless cars. Instead of just recognizing past places and outputting their poses, it is desired that a global localization system estimates the pose of current place images. In this paper, we propose to tackle this problem as follows. Firstly, given a live image, the place recognition system returns the most similar image and its pose. Then, given live and recollected images, a visual localization system outputs the relative camera pose represented by those images. To estimate the relative camera pose between the recollected and the current images, a Convolutional Neural Network (CNN) is trained with the two images as input and a relative pose vector as output. Together, these systems solve the global localization problem using the topological and metric information to approximate the current vehicle pose. The full approach is compared to a Real-Time Kinematic GPS system and a Simultaneous Localization and Mapping (SLAM) system. Experimental results show that the proposed approach correctly localizes a vehicle 90 of the time with a mean error of 1.20m compared to 1.12m of the SLAM system and 0.37m of the GPS, 89 of the time.", "label": "1", "is_selected": "0", "text": "Currently, self-driving cars rely greatly on the Global Positioning System (GPS) infrastructure, albeit there is an increasing demand for alternative methods for GPS-denied environments. One of them is known as place recognition, which associates images of places with their corresponding positions. We previously proposed systems based on Weightless Neural Networks (WNN) to address this problem as a classification task. This encompasses solely one part of the global localization, which is not precise enough for driverless cars. Instead of just recognizing past places and outputting their poses, it is desired that a global localization system estimates the pose of current place images. In this paper, we propose to tackle this problem as follows. Firstly, given a live image, the place recognition system returns the most similar image and its pose. Then, given live and recollected images, a visual localization system outputs the relative camera pose represented by those images. To estimate the relative camera pose between the recollected and the current images, a Convolutional Neural Network (CNN) is trained with the two images as input and a relative pose vector as output. Together, these systems solve the global localization problem using the topological and metric information to approximate the current vehicle pose. The full approach is compared to a Real-Time Kinematic GPS system and a Simultaneous Localization and Mapping (SLAM) system. Experimental results show that the proposed approach correctly localizes a vehicle 90 of the time with a mean error of 1.20m compared to 1.12m of the SLAM system and 0.37m of the GPS, 89 of the time."}
{"original_text": "In unsupervised classification, Hidden Markov Models (HMM) are used to account for a neighborhood structure between observations. The emission distributions are often supposed to belong to some parametric family. In this paper, we propose a nonparametric estimation of the emission densities in the case of Gaussian mixture models. The method is based on a kernel smoothing of the observations. The parameters of the mixture models are estimated using the Expectation-Maximization algorithm. We use a large simulated data set and two real data sets to illustrate the application of this approach. The method shows good results when the number of classes is small. 2009 Elsevier B.V. All rights reserved.", "label": "0", "is_selected": "0", "text": "In unsupervised classification, Hidden Markov Models (HMM) are used to account for a neighborhood structure between observations. The emission distributions are often supposed to belong to some parametric family. In this paper, we propose a nonparametric estimation of the emission densities in the case of Gaussian mixture models. The method is based on a kernel smoothing of the observations. The parameters of the mixture models are estimated using the Expectation-Maximization algorithm. We use a large simulated data set and two real data sets to illustrate the application of this approach. The method shows good results when the number of classes is small. 2009 Elsevier B.V. All rights reserved."}
{"original_text": "National Eating Disorders Association conducts a NEDAwareness week every year, during which it publishes content on social media and news aimed to raise awareness of eating disorders. Measuring the impact of these actions is vital for maximizing the effectiveness of such interventions. This paper is an effort to model the change in behavior of users who engage with NEDAwareness content. We find that, despite popular influencers being involved in the campaign, it is governmental and nonprofit accounts that attract the most retweets. Furthermore, examining the tweeting language of users engaged with this content, we find linguistic categories concerning women, family, and anxiety to be mentioned more within the 15 days after the intervention, and categories concerning affiliation, references to others, and positive emotion mentioned less. We conclude with actionable implications for future campaigns and discussion of the method's limitations.", "label": "1", "is_selected": "0", "text": "National Eating Disorders Association conducts a NEDAwareness week every year, during which it publishes content on social media and news aimed to raise awareness of eating disorders. Measuring the impact of these actions is vital for maximizing the effectiveness of such interventions. This paper is an effort to model the change in behavior of users who engage with NEDAwareness content. We find that, despite popular influencers being involved in the campaign, it is governmental and nonprofit accounts that attract the most retweets. Furthermore, examining the tweeting language of users engaged with this content, we find linguistic categories concerning women, family, and anxiety to be mentioned more within the 15 days after the intervention, and categories concerning affiliation, references to others, and positive emotion mentioned less. We conclude with actionable implications for future campaigns and discussion of the method's limitations."}
{"original_text": "We study the transmission of a set of correlated sources (U 1, , U K) over a Gaussian multiple access relay channel with time asynchronism between the encoders. We assume that the relay decodes the transmissions from the sources and retransmits it to the destination. We consider the case where the sources have no common information and prove that the secrecy capacity region is the closure of the set of all rate pairs (R 1, , R K) such that R 1I (X 1;Y) for all X 1 that are independent of the other transmissions and that satisfy the conditions R iR jI (X i,X j;Y) for all (i,j) {1, , K}.", "label": "0", "is_selected": "1", "text": "The transmission of data over a relay channel with time asynchronism has been investigated in the context of high-speed data networks with high secrecy capacity."}
{"original_text": "In recent times, using small data to train networks has become a hot topic in the field of deep learning. Reusing pre-trained parameters is one of the most important strategies to address the issue of semi-supervised and transfer learning. However, the fundamental reason for the success of these methods is still unclear. In this paper, we propose a solution that can not only judge whether a given network is reusable or not based on the performance of reusing convolution kernels but also judge which layers' parameters of the given network can be reused, based on the performance of reusing corresponding parameters and, ultimately, judge whether those parameters are reusable or not in a target task based on the root mean square error (RMSE) of the corresponding convolution kernels. Specifically, we define that the success of a CNN's parameter reuse depends upon two conditions: first, the network is a reusable network; and second, the RMSE between the convolution kernels from the source domain and target domain is small enough. The experimental results demonstrate that the performance of reused parameters applied to target tasks, when these conditions are met, is significantly improved.", "label": "1", "is_selected": "0", "text": "In recent times, using small data to train networks has become a hot topic in the field of deep learning. Reusing pre-trained parameters is one of the most important strategies to address the issue of semi-supervised and transfer learning. However, the fundamental reason for the success of these methods is still unclear. In this paper, we propose a solution that can not only judge whether a given network is reusable or not based on the performance of reusing convolution kernels but also judge which layers' parameters of the given network can be reused, based on the performance of reusing corresponding parameters and, ultimately, judge whether those parameters are reusable or not in a target task based on the root mean square error (RMSE) of the corresponding convolution kernels. Specifically, we define that the success of a CNN's parameter reuse depends upon two conditions: first, the network is a reusable network; and second, the RMSE between the convolution kernels from the source domain and target domain is small enough. The experimental results demonstrate that the performance of reused parameters applied to target tasks, when these conditions are met, is significantly improved."}
{"original_text": "We seek to learn a representation on a large annotated data source that generalizes to a target domain using limited new supervision. Many prior approaches to this problem have focused on using the labeled information from the target domain to help define an embedding space. However, these approaches often cannot make use of the rich annotations available in the large source domain. We develop a novel framework that can leverage annotations from the source domain to define an embedding space for the target domain, even when the two domains differ in their annotation schemes. This is achieved by jointly learning a transformation between these annotation schemes that is applied to the source embeddings, and an embedding mapping that projects these transformed embeddings to a lower-dimensional space that is specific to the target domain. This framework is designed for unsupervised training on both domains, but it can also use additional weak supervision in the form of target domain labels for a subset of the data. We evaluate the proposed framework on five standard benchmarks, as well as two real-world datasets, showing that it is competitive with, or better than, state-of-the-art methods.", "label": "0", "is_selected": "1", "text": "In this paper, we develop a novel framework for unsupervised training on large annotated data sources, even when the two domains differ in their annotation schemes."}
{"original_text": "As light field images continue to increase in use and application, it becomes necessary to adapt existing image processing methods to this unique form of photography. In this paper we explore methods for applying neural style transfer to light field images. Feed-forward style transfer networks provide fast, high-quality results for monocular images, but no such networks exist for full light field images. Because of the size of these images, current light field data sets are small and are insufficient for training purely feed-forward style-transfer networks from scratch. Thus, it is necessary to adapt existing monocular style transfer networks in a way that allows for the stylization of each view of the light field while maintaining visual consistencies between views. To do this, we first generate disparity maps for each view given a single depth image for the light field. Then in a fashion similar to neural stylization of stereo images, we use disparity maps to enforce a consistency loss between views and to warp feature maps during the feed forward stylization. Unlike previous work, however, light fields have too many views to train a purely feed-forward network that can stylize the entire light field with angular consistency. Instead, the proposed method uses an iterative optimization for each view of a single light field image that backpropagates the consistency loss through the network. Thus, the network architecture allows for the incorporation of pre-trained fast monocular stylization network while avoiding the need for a large light field training set.", "label": "1", "is_selected": "0", "text": "As light field images continue to increase in use and application, it becomes necessary to adapt existing image processing methods to this unique form of photography. In this paper we explore methods for applying neural style transfer to light field images. Feed-forward style transfer networks provide fast, high-quality results for monocular images, but no such networks exist for full light field images. Because of the size of these images, current light field data sets are small and are insufficient for training purely feed-forward style-transfer networks from scratch. Thus, it is necessary to adapt existing monocular style transfer networks in a way that allows for the stylization of each view of the light field while maintaining visual consistencies between views. To do this, we first generate disparity maps for each view given a single depth image for the light field. Then in a fashion similar to neural stylization of stereo images, we use disparity maps to enforce a consistency loss between views and to warp feature maps during the feed forward stylization. Unlike previous work, however, light fields have too many views to train a purely feed-forward network that can stylize the entire light field with angular consistency. Instead, the proposed method uses an iterative optimization for each view of a single light field image that backpropagates the consistency loss through the network. Thus, the network architecture allows for the incorporation of pre-trained fast monocular stylization network while avoiding the need for a large light field training set."}
{"original_text": "We study the power and limits of optimal dynamic pricing in combinatorial markets; i.e., dynamic pricing that leads to optimal social welfare. Previous work by Cohen-Addad et al. [EC'16] demonstrated the existence of optimal dynamic prices for unit-demand buyers, and showed a market with coverage valuations that admits no such prices. However, finding the frontier of markets (i.e., valuation functions) that admit optimal dynamic prices remains an open problem. In this work we establish positive and negative results that narrow the existing gap. On the positive side, we provide tools for handling markets beyond unit-demand valuations. In particular, we characterize all optimal allocations in multi-demand markets. This characterization allows us to partition the items into equivalence classes according to the role they play in achieving optimality. Using these tools, we provide a poly-time optimal dynamic pricing algorithm for up to 3 multi-demand buyers. On the negative side, we establish a maximal domain theorem, showing that for every non-gross substitutes valuation, there exist unit-demand valuations such that adding them yields a market that does not admit an optimal dynamic pricing. This result is reminiscent of the seminal maximal domain theorem by Gul and Stacchetti [JET'99] for Walrasian equilibrium. Yang [JET'17] discovered an error in their original proof, and established a different, incomparable version of their maximal domain theorem. En route to our maximal domain theorem for optimal dynamic pricing, we provide the first complete proof of the original theorem by Gul and Stacchetti.", "label": "1", "is_selected": "0", "text": "We study the power and limits of optimal dynamic pricing in combinatorial markets; i.e., dynamic pricing that leads to optimal social welfare. Previous work by Cohen-Addad et al. [EC'16] demonstrated the existence of optimal dynamic prices for unit-demand buyers, and showed a market with coverage valuations that admits no such prices. However, finding the frontier of markets (i.e., valuation functions) that admit optimal dynamic prices remains an open problem. In this work we establish positive and negative results that narrow the existing gap. On the positive side, we provide tools for handling markets beyond unit-demand valuations. In particular, we characterize all optimal allocations in multi-demand markets. This characterization allows us to partition the items into equivalence classes according to the role they play in achieving optimality. Using these tools, we provide a poly-time optimal dynamic pricing algorithm for up to 3 multi-demand buyers. On the negative side, we establish a maximal domain theorem, showing that for every non-gross substitutes valuation, there exist unit-demand valuations such that adding them yields a market that does not admit an optimal dynamic pricing. This result is reminiscent of the seminal maximal domain theorem by Gul and Stacchetti [JET'99] for Walrasian equilibrium. Yang [JET'17] discovered an error in their original proof, and established a different, incomparable version of their maximal domain theorem. En route to our maximal domain theorem for optimal dynamic pricing, we provide the first complete proof of the original theorem by Gul and Stacchetti."}
{"original_text": "Approximations of loopy belief propagation, including expectation propagation and approximate message passing, have attracted considerable attention for probabilistic inference problems. This paper proposes and analyzes a generalization of Opper and Winther's algorithm for computing the beliefs of loopy graphical models, which offers an alternative to the computation of so-called pseudo-marginals. Our algorithm is based on the moments of the beliefs, rather than the beliefs themselves. The moments are computed by a set of (possibly stochastic) recursive equations. The main result is that the update equations for the moments have the same stationary points as the original beliefs. This property is called consistency. In addition, the mean-square consistency of the moments is analyzed. We show that the consistency and mean-square consistency can be achieved for a wide range of recursive equations, including expectation propagation. The main result is a generalization of the consistency results of Opper and Winther. We also present a new algorithm that can be used for sampling from the posterior distribution of loopy graphical models. Numerical experiments on the one-dimensional Ising model show that the proposed algorithm is faster than Opper and Winther's algorithm. The problem of finding the posterior distribution of a graphical model is fundamental in many machine learning and signal processing applications. Often, the distribution is specified by a Markov network and the data are specified by an observed graphical model. Given a Markov network with graph G, the nodes are partitioned into two sets X and Y. The node sets are connected via the edges in G. The nodes in X are called hidden variables, and the nodes in Y are called observed variables. The goal of the problem is to compute the marginals of the hidden variables, denoted by p (XG). Based on the Markov network, the posterior distribution is given by p (XG) Z1exp (xiTf (xi) yf (y Xexp (xiTf (xi) yf (y, (1) where Z is the partition function, f (xi) is the energy function and f (yi) is the sum of the potentials (including f (xi of the neighboring nodes of yi. The posterior distribution p (XG) is usually intractable, due to the intractability of the partition function Z. In this paper, we assume that the partition function is intractable. 1.1. Loopy Graphical Models The graphical model in Equation (1) is called a chain graphical model if it has no loops (i.e., there is at most one path between any pair of nodes). It is called a loopy graphical model if it has loops. Many real-world problems are naturally modeled as loopy graphical models. For example, in a social network, the connectivity between each pair of nodes is not necessarily known. In such a situation, the graphical model can be represented as a loopy graphical model. In this paper, we focus on the inference of loopy graphical models. The problem of finding the marginal probabilities p (XG) is called the loopy belief propagation (LBP) problem. The LBP problem is NP-hard in general. Hence, the search for approximate algorithms for the LBP problem is of great practical importance. The approximate algorithms are sometimes called loopy belief propagation algorithms. 1.2. Approximate Message Passing A recent line of work in machine learning and signal processing has shown that approximate inference algorithms, such as expectation propagation (EP) and approximate message passing (AMP), are able to give accurate approximate inference results for loopy graphical models. The EP algorithm (Minka, 2001) is an expectation-maximization-based algorithm, which requires an analytic form of the gradient of the log-likelihood function with respect to the parameters of the model. The AMP algorithm (Donoho et al., 2009; Donoho and Maleki, 2010) is a general message-passing algorithm, and can be applied to a wider range of problems. This paper is concerned with the analysis of the approximate inference algorithms for loopy graphical models. As shown in this paper, the EP algorithm is a special case of AMP. Therefore, the analysis of AMP algorithms also applies to the EP algorithm. 1.3. Consistency of Beliefs The approximate inference algorithms mentioned above all rely on the computation of so-called pseudo-marginals, which are the beliefs of the hidden variables. The accuracy of the approximate inference depends on the accuracy of the pseudo-marginals. A natural question is whether the", "label": "0", "is_selected": "0", "text": "Approximations of loopy belief propagation, including expectation propagation and approximate message passing, have attracted considerable attention for probabilistic inference problems. This paper proposes and analyzes a generalization of Opper and Winther's algorithm for computing the beliefs of loopy graphical models, which offers an alternative to the computation of so-called pseudo-marginals. Our algorithm is based on the moments of the beliefs, rather than the beliefs themselves. The moments are computed by a set of (possibly stochastic) recursive equations. The main result is that the update equations for the moments have the same stationary points as the original beliefs. This property is called consistency. In addition, the mean-square consistency of the moments is analyzed. We show that the consistency and mean-square consistency can be achieved for a wide range of recursive equations, including expectation propagation. The main result is a generalization of the consistency results of Opper and Winther. We also present a new algorithm that can be used for sampling from the posterior distribution of loopy graphical models. Numerical experiments on the one-dimensional Ising model show that the proposed algorithm is faster than Opper and Winther's algorithm. The problem of finding the posterior distribution of a graphical model is fundamental in many machine learning and signal processing applications. Often, the distribution is specified by a Markov network and the data are specified by an observed graphical model. Given a Markov network with graph G, the nodes are partitioned into two sets X and Y. The node sets are connected via the edges in G. The nodes in X are called hidden variables, and the nodes in Y are called observed variables. The goal of the problem is to compute the marginals of the hidden variables, denoted by p (XG). Based on the Markov network, the posterior distribution is given by p (XG) Z1exp (xiTf (xi) yf (y Xexp (xiTf (xi) yf (y, (1) where Z is the partition function, f (xi) is the energy function and f (yi) is the sum of the potentials (including f (xi of the neighboring nodes of yi. The posterior distribution p (XG) is usually intractable, due to the intractability of the partition function Z. In this paper, we assume that the partition function is intractable. 1.1. Loopy Graphical Models The graphical model in Equation (1) is called a chain graphical model if it has no loops (i.e., there is at most one path between any pair of nodes). It is called a loopy graphical model if it has loops. Many real-world problems are naturally modeled as loopy graphical models. For example, in a social network, the connectivity between each pair of nodes is not necessarily known. In such a situation, the graphical model can be represented as a loopy graphical model. In this paper, we focus on the inference of loopy graphical models. The problem of finding the marginal probabilities p (XG) is called the loopy belief propagation (LBP) problem. The LBP problem is NP-hard in general. Hence, the search for approximate algorithms for the LBP problem is of great practical importance. The approximate algorithms are sometimes called loopy belief propagation algorithms. 1.2. Approximate Message Passing A recent line of work in machine learning and signal processing has shown that approximate inference algorithms, such as expectation propagation (EP) and approximate message passing (AMP), are able to give accurate approximate inference results for loopy graphical models. The EP algorithm (Minka, 2001) is an expectation-maximization-based algorithm, which requires an analytic form of the gradient of the log-likelihood function with respect to the parameters of the model. The AMP algorithm (Donoho et al., 2009; Donoho and Maleki, 2010) is a general message-passing algorithm, and can be applied to a wider range of problems. This paper is concerned with the analysis of the approximate inference algorithms for loopy graphical models. As shown in this paper, the EP algorithm is a special case of AMP. Therefore, the analysis of AMP algorithms also applies to the EP algorithm. 1.3. Consistency of Beliefs The approximate inference algorithms mentioned above all rely on the computation of so-called pseudo-marginals, which are the beliefs of the hidden variables. The accuracy of the approximate inference depends on the accuracy of the pseudo-marginals. A natural question is whether the"}
{"original_text": "Indoor localization for autonomous micro aerial vehicles (MAVs) requires specific localization techniques, since the Global Positioning System (GPS) is usually not available. We present an efficient onboard computer vision approach for indoor MAV localization that is robust to occlusion and works without prior knowledge of the environment. For online localization, we track the camera poses of the MAV using a novel camera pose estimation method based on super-pixels and extended Kalman filtering. Our method only requires the camera images and ignores the input of additional sensors, such as a depth camera or an inertial measurement unit. For offline localization, we apply a novel robust SIFT algorithm to extract well-matched keypoints from images taken by an omnidirectional camera and a fisheye camera. These keypoints are then matched across the images in the image sequence to compute the camera poses of the MAV. Finally, for both localization tasks, we propose an efficient pose estimation method that iteratively improves the pose accuracy by performing pose refinements in the image sequence. We demonstrate the accuracy and robustness of our approach in real experiments and evaluate the performance in a simulated environment. Our approach significantly improves the performance compared to related approaches. The Computer Vision and Remote Sensing Lab (CVRS) is a research group at the University of Zurich, which was founded in 2011. The lab is headed by Prof. Marc Pollefeys. The CVRS lab aims to bridge the gap between computer vision, robotics, and remote sensing. We work on novel methods for accurate 3D reconstruction of real-world environments and objects, and we are particularly interested in systems that are able to continuously perceive their surroundings and make intelligent decisions based on that information. We work in a wide range of application domains, including autonomous driving, micro aerial vehicles, industrial robotics, and remote sensing. Our work is supported by a number of funding agencies, including the European Union (through H2020, FP7, and ESA), the Swiss National Science Foundation, the Swiss National Center of Competence in Research (NCCR) Robotics, and industry. Computer Vision and Remote Sensing Lab.", "label": "0", "is_selected": "1", "text": "In this paper, we present an efficient computer vision approach for indoor autonomous micro aerial vehicles that is robust to occlusion and works without prior knowledge of the environment."}
{"original_text": "Degree sequence (DS) problems are around for at least hundred twenty years, and with the advent of network science, more and more complicated, structured DS problems were invented. Interestingly enough all those problems so far are computationally easy. It is clear, however, that we will find soon computationally hard DS problems. In this paper we want to find such hard DS problems with relatively simple definition. For a vertex v in the simple graph G denote d I (v) the number of vertices at distance exactly I from v. Then d 1 (v) is the usual degree of vertex v. The vector d 2 (G) d 1 (v 1), d 2 (v 1, ..., (d 1 (v n), d 2 (v n is the second order degree sequence of the graph G. In this note we show that the problem to decide whether a sequence of natural numbers i 1, j 1), ... (i n, j n is a second order degree sequence of a simple undirected graph G is strongly NP -complete. Then we will discuss some further NP -complete DS problems.", "label": "1", "is_selected": "0", "text": "Degree sequence (DS) problems are around for at least hundred twenty years, and with the advent of network science, more and more complicated, structured DS problems were invented. Interestingly enough all those problems so far are computationally easy. It is clear, however, that we will find soon computationally hard DS problems. In this paper we want to find such hard DS problems with relatively simple definition. For a vertex v in the simple graph G denote d I (v) the number of vertices at distance exactly I from v. Then d 1 (v) is the usual degree of vertex v. The vector d 2 (G) d 1 (v 1), d 2 (v 1, ..., (d 1 (v n), d 2 (v n is the second order degree sequence of the graph G. In this note we show that the problem to decide whether a sequence of natural numbers i 1, j 1), ... (i n, j n is a second order degree sequence of a simple undirected graph G is strongly NP -complete. Then we will discuss some further NP -complete DS problems."}
{"original_text": "Quantum memories are a fundamental of any global-scale quantum Internet, high-performance quantum networking and near-term quantum computers. A main problem of quantum memories is the low retrieval efficiency of the stored information, which is the main cause of the loss of stored information in a quantum memory. Here, we report a broadband, high-efficiency quantum memory, which can store information in an ensemble of ytterbium ions and retrieve the stored information with a retrieval efficiency of 96 (31.81.2). This is a 40-fold improvement compared to previously demonstrated quantum memories, and is achieved by an improvement in the control of the atomic interaction and the quantum operation processes. The broadband quantum memory can be used to solve the challenge of quantum information storage in a broadband quantum channel and the problem of the limited frequency of existing quantum memory, which can only store information in a narrowband. The broadband quantum memory can be used to solve the challenge of quantum information storage in a broadband quantum channel and the problem of the limited frequency of existing quantum memory, which can only store information in a narrowband.", "label": "0", "is_selected": "1", "text": "We report a broadband, high-efficiency quantum memory, which can store information in an ensemble of ytterbium ions and retrieve the stored information with a retrieval efficiency of 96."}
{"original_text": "In this work we explore the method of style transfer presented in. We first demonstrate the power of the suggested style space on a few examples. We then vary different parameters of the transfer function and show how the effect of each on the output image can be predicted. The method of [1] was presented in the context of translating the style of one image to another, given that the content of the images are the same. For example, in the image below, the artist added the style of a famous painting to a photograph of his own. The method of [1] achieves this by first extracting the content from one image and the style from the other. These are then used to synthesize the new image. The suggested method is based on the assumption that the style of an image can be captured by a set of basis functions (BFM), and that the amount of style in each of these functions can be determined by their coefficients. Given these coefficients, the method extracts the style of one image by computing the coefficients of its BFM, and the content of the other by multiplying the BFM by their coefficients. The synthesized image is then obtained by combining the two images according to the desired style transfer ratio. Several questions arise from the above description: How does the method of [1] compute the BFM? How does the method determine the coefficients of the style and content images? How is the style transfer ratio controlled? How do we evaluate the quality of the synthesized image? 1. BFM We will start by exploring the basis function method (BFM). Assume that we are given an image I. Given a small region in the image (R), we would like to represent it by a function. Specifically, we are interested in representing the region by a function such that when this function is convolved with a kernel (K), the result is the region itself. The kernel is a small, usually centered, Gaussian function. The resulting convolution is known as the response of the region to the kernel, and it is the basis function. The region R is then represented by the set of basis functions generated for all possible locations of the kernel (K). The image below shows the response of a region to a kernel for different locations. In order to evaluate the quality of the representation, we check the accuracy of the response function for the original region (the first image in the set below). To extract the style of an image, we first find the basis functions that characterize each pixel. The style image is then represented by a vector whose dimension is the number of pixels in the image, and whose entries are the coefficients of the basis functions. These coefficients are found by fitting the basis functions to the image. In our work we used the publicly available code (available here). The coefficients of the BFM were computed using a gradient descent algorithm. The derivation of the algorithm and the formulas used are described in the original paper [1]. 2. Extracting the Content In order to extract the content of an image, we must first find the basis functions that represent the image. Once we have them, we can compute the coefficients of the style image. In the original paper [1] it was suggested to use the PCA of the style image to compute the BFM. However, this is not the only option. In this work we used the BFM that was generated for the content image itself. In other words, for each pixel in the content image we used the basis function that best represented it. 3. Style Transfer Ratio The next question that arises is how does the method of [1] control the style transfer ratio? The style transfer ratio is the weight assigned to the style image. If the style transfer ratio is one, then the content image is replaced by the style image. If it is zero, the content image remains unchanged. If the ratio is between zero and one, then the output image is a mixture of the style and content images. In the original paper [1] the ratio was computed by finding the L1 norm of the BFM coefficients. The method we used is similar. 4. Evaluating the Quality of the Synthesized Image Our last question is how does the method of [1] evaluate the quality of the output image? The evaluation is based on the assumption that the synthesized image is similar to the content image, and it can be easily computed. We first compute the BFM that represents the synthesized image. Then, we compute the coefficients of the BFM for the synthesized image and for the content image. The dissimilarity of the two coefficients is then computed using the L1 norm. 5. Varying Different Parameters of the Transfer Function In this section we present the results of a set of experiments that vary the parameters of the transfer function. In the first experiment", "label": "0", "is_selected": "0", "text": "In this work we explore the method of style transfer presented in. We first demonstrate the power of the suggested style space on a few examples. We then vary different parameters of the transfer function and show how the effect of each on the output image can be predicted. The method of [1] was presented in the context of translating the style of one image to another, given that the content of the images are the same. For example, in the image below, the artist added the style of a famous painting to a photograph of his own. The method of [1] achieves this by first extracting the content from one image and the style from the other. These are then used to synthesize the new image. The suggested method is based on the assumption that the style of an image can be captured by a set of basis functions (BFM), and that the amount of style in each of these functions can be determined by their coefficients. Given these coefficients, the method extracts the style of one image by computing the coefficients of its BFM, and the content of the other by multiplying the BFM by their coefficients. The synthesized image is then obtained by combining the two images according to the desired style transfer ratio. Several questions arise from the above description: How does the method of [1] compute the BFM? How does the method determine the coefficients of the style and content images? How is the style transfer ratio controlled? How do we evaluate the quality of the synthesized image? 1. BFM We will start by exploring the basis function method (BFM). Assume that we are given an image I. Given a small region in the image (R), we would like to represent it by a function. Specifically, we are interested in representing the region by a function such that when this function is convolved with a kernel (K), the result is the region itself. The kernel is a small, usually centered, Gaussian function. The resulting convolution is known as the response of the region to the kernel, and it is the basis function. The region R is then represented by the set of basis functions generated for all possible locations of the kernel (K). The image below shows the response of a region to a kernel for different locations. In order to evaluate the quality of the representation, we check the accuracy of the response function for the original region (the first image in the set below). To extract the style of an image, we first find the basis functions that characterize each pixel. The style image is then represented by a vector whose dimension is the number of pixels in the image, and whose entries are the coefficients of the basis functions. These coefficients are found by fitting the basis functions to the image. In our work we used the publicly available code (available here). The coefficients of the BFM were computed using a gradient descent algorithm. The derivation of the algorithm and the formulas used are described in the original paper [1]. 2. Extracting the Content In order to extract the content of an image, we must first find the basis functions that represent the image. Once we have them, we can compute the coefficients of the style image. In the original paper [1] it was suggested to use the PCA of the style image to compute the BFM. However, this is not the only option. In this work we used the BFM that was generated for the content image itself. In other words, for each pixel in the content image we used the basis function that best represented it. 3. Style Transfer Ratio The next question that arises is how does the method of [1] control the style transfer ratio? The style transfer ratio is the weight assigned to the style image. If the style transfer ratio is one, then the content image is replaced by the style image. If it is zero, the content image remains unchanged. If the ratio is between zero and one, then the output image is a mixture of the style and content images. In the original paper [1] the ratio was computed by finding the L1 norm of the BFM coefficients. The method we used is similar. 4. Evaluating the Quality of the Synthesized Image Our last question is how does the method of [1] evaluate the quality of the output image? The evaluation is based on the assumption that the synthesized image is similar to the content image, and it can be easily computed. We first compute the BFM that represents the synthesized image. Then, we compute the coefficients of the BFM for the synthesized image and for the content image. The dissimilarity of the two coefficients is then computed using the L1 norm. 5. Varying Different Parameters of the Transfer Function In this section we present the results of a set of experiments that vary the parameters of the transfer function. In the first experiment"}
{"original_text": "This paper describes the Amobee sentiment analysis system, adapted to compete in SemEval 2017 task 4. The system consists of two parts: a supervised training of RNN models based on word embeddings, and a word representation module that uses contextual information to enrich a word's representation. Our system is based on a transfer learning approach, where we use an ensemble of deep learning models to initialize the new models, and then tune their parameters for the new task. This approach enables us to achieve a high accuracy with relatively few training examples. In the SemEval competition, our system ranked second out of 47 participants, and we also won the second place in the SemEval 2017 ongoing task.", "label": "0", "is_selected": "0", "text": "This paper describes the Amobee sentiment analysis system, adapted to compete in SemEval 2017 task 4. The system consists of two parts: a supervised training of RNN models based on word embeddings, and a word representation module that uses contextual information to enrich a word's representation. Our system is based on a transfer learning approach, where we use an ensemble of deep learning models to initialize the new models, and then tune their parameters for the new task. This approach enables us to achieve a high accuracy with relatively few training examples. In the SemEval competition, our system ranked second out of 47 participants, and we also won the second place in the SemEval 2017 ongoing task."}
{"original_text": "3D point cloud semantic and instance segmentation is crucial and fundamental for 3D scene understanding. Due to the complex structure, point sets are distributed off balance and diversely, which appears as both category imbalance and pattern imbalance. As a result, deep networks can easily forget the non-dominant cases during the learning process, resulting in unsatisfactory performance. Although re-weighting can reduce the influence of the well-classified examples, they cannot handle the non-dominant patterns during the dynamic training. In this paper, we propose a memory-augmented network to learn and memorize the representative prototypes that cover diverse samples universally. Specifically, a memory module is introduced to alleviate the forgetting issue by recording the patterns seen in mini-batch training. The learned memory items consistently reflect the interpretable and meaningful information for both dominant and non-dominant categories and cases. The distorted observations and rare cases can thus be augmented by retrieving the stored prototypes, leading to better performances and generalization. Exhaustive experiments on the benchmarks, i.e. S3DIS and ScanNetV2, reflect the superiority of our method on both effectiveness and efficiency. Not only the overall accuracy but also non-dominant classes have improved substantially.", "label": "1", "is_selected": "0", "text": "3D point cloud semantic and instance segmentation is crucial and fundamental for 3D scene understanding. Due to the complex structure, point sets are distributed off balance and diversely, which appears as both category imbalance and pattern imbalance. As a result, deep networks can easily forget the non-dominant cases during the learning process, resulting in unsatisfactory performance. Although re-weighting can reduce the influence of the well-classified examples, they cannot handle the non-dominant patterns during the dynamic training. In this paper, we propose a memory-augmented network to learn and memorize the representative prototypes that cover diverse samples universally. Specifically, a memory module is introduced to alleviate the forgetting issue by recording the patterns seen in mini-batch training. The learned memory items consistently reflect the interpretable and meaningful information for both dominant and non-dominant categories and cases. The distorted observations and rare cases can thus be augmented by retrieving the stored prototypes, leading to better performances and generalization. Exhaustive experiments on the benchmarks, i.e. S3DIS and ScanNetV2, reflect the superiority of our method on both effectiveness and efficiency. Not only the overall accuracy but also non-dominant classes have improved substantially."}
{"original_text": "Shannon's theory of information was built on the assumption that the information carriers were classical systems. Its quantum counterpart, quantum Shannon theory, explores the new possibilities arising when the information carriers are quantum systems. The present article reviews the construction of quantum Shannon theory, with an emphasis on the differences between it and its classical counterpart. The information-processing tasks considered are compression and communication with and without entanglement. The new information-processing tasks are cloning and deleting quantum states. The compression of pure quantum states and the communication of classical and quantum information are treated in detail. Mathematical concepts that are the foundation of quantum Shannon theory are the entropy and the entropy power. Entropy was originally conceived as a measure of the average information content of a message in classical information theory, but the quantum version of the entropy is a measure of the average information content of a quantum system. The entropy power is a measure of the average power of a quantum system. The quantum entropy and entropy power are analogous to the energy and energy density of a classical system. The quantum analog of the energy-entropy relation is the entropy-entropy power relation. The quantum analog of the equipartition theorem is the quantum entropy power inequality. Another important relation in quantum Shannon theory is the quantum data processing inequality. This inequality is analogous to the classical data processing inequality. It states that if the quantum system has a noiseless channel connecting it to another quantum system, then the entropy of the composite system is not less than the sum of the entropies of the individual quantum systems. In the present article, we review the quantum versions of the classical information-processing tasks of compression and communication. The generalizations of the classical tasks of cloning and deleting are also discussed. Shannon's information theory is the mathematical theory of information processing, and it has found application in many fields such as communication, computer science, and biology. Shannon's theory of information was built on the assumption that the information carriers were classical systems. The theory was extended to quantum systems in the 1960s by the physicists Manin [1] and Gnedenko [2] and the information theorist Ahlswede [3]. The present article reviews the construction of quantum Shannon theory, with an emphasis on the differences between it and its classical counterpart. We begin with an introduction to quantum information theory and then review the quantum versions of the classical information-processing tasks of compression and communication. In the last section, we discuss quantum cloning and deleting, which are the generalizations of the classical tasks of cloning and deleting. The information-processing tasks considered here are compression and communication. Compression is a method of storing information more efficiently by converting it into a more compact form. In communication, information is transmitted over a channel, for example, a telephone line or a wireless link, from a sender to a receiver. The sender and receiver are called Alice and Bob in quantum Shannon theory, and we will assume that they are located at two different points in space. Communication has two parts: encoding and decoding. The encoding transforms the message to be transmitted into a string of symbols, and the decoding reverses the encoding. The information-processing tasks considered here are communication and compression. Compression is a method of storing information more efficiently by converting it into a more compact form. In communication, information is transmitted over a channel, for example, a telephone line or a wireless link, from a sender to a receiver. The sender and receiver are called Alice and Bob in quantum Shannon theory, and we will assume that they are located at two different points in space. Communication has two parts: encoding and decoding. The encoding transforms the message to be transmitted into a string of symbols, and the decoding reverses the encoding. The information-processing tasks considered here are communication and compression. Compression is a method of storing information more efficiently by converting it into a more compact form. In communication, information is transmitted over a channel, for example, a telephone line or a wireless link, from a sender to a receiver. The sender and receiver are called Alice and Bob in quantum Shannon theory, and we will assume that they are located at two different points in space. Communication has two parts: encoding and decoding. The encoding transforms the message to be transmitted into a string of symbols, and the decoding reverses the encoding. In the present article, we focus on the encoding part of communication. In classical information theory, the information is assumed to be classical, that is, the information carriers are classical systems. The classical information carrier has a finite number of states, and the state is given by a point in a finite-dimensional phase space. The set of states forms a closed set. The closedness of the state space in classical information", "label": "0", "is_selected": "1", "text": "The theory of information and the theory of quantum information are closely related, but their constructions are separated by only a few hundredths of a second."}
{"original_text": "This paper provides a new way to improve the efficiency of the REINFORCE training process. We apply it to the task of instance selection in distant supervision. Modeling the instance selection problem as a reinforcement learning problem, we select the training instances in an iterative process, where each iteration is modeled as a Markov decision process. In each iteration, the Markov decision process receives a reward that measures the effectiveness of the instance selection. The REINFORCE algorithm is used to learn the optimal instance selection policy. Our experimental results show that the proposed method significantly outperforms previous work on distant supervision using the same neural network structure. Natural language processing, Information retrieval, Supervised learning, Supervised learning, Supervised learning, Reinforcement learning, Supervised learning, Natural language processing, Machine learning, Machine learning, Machine learning, Machine learning, Machine learning, Reinforcement learning, Information retrieval, Information retrieval, Information retrieval, Information retrieval, Information retrieval, Information retrieval, Markov decision processes, Natural language processing, Distant supervision, Reinforcement learning, Markov decision processes, Natural language processing, Reinforcement learning, Information retrieval, Machine learning, Reinforcement learning X. Lu, S. Das, J. Li, X. Yan, K. Liu, \"Reinforcement Learning for Instance Selection in Distant Supervision,\" IEEE Transactions on Knowledge and Data Engineering, vol. 28, no. 11, pp. 2933-2948, Nov. 2016. Articles by Xiaofeng Lu Articles by Satyabrata Das Articles by Jiebo Luo Articles by Xiaodong Yan", "label": "0", "is_selected": "1", "text": "Reinforcement learning is a promising method for learning natural language, but its performance has been poorly understood. Natural language processing, Information retrieval, Supervised learning, supervised learning,"}
{"original_text": "Many sensors, such as range, sonar, radar, GPS and visual devices, produce measurements which are contaminated by outliers. This problem can be addressed by using fat-tailed sensor models, which account for the possibility of outliers. Unfortunately, all estimation algorithms belonging to the family of Gaussian filters (such as the widely-used extended Kalman filter and unscented Kalman filter) are inherently incompatible with such fat-tailed sensor models. The contribution of this paper is to show that any Gaussian filter can be made compatible with fat-tailed sensor models by applying one simple change: Instead of filtering with the physical measurement, we propose to filter with a pseudo measurement obtained by applying a feature function to the physical measurement. We derive such a feature function which is optimal under some conditions. Simulation results show that the proposed method can effectively handle measurement outliers and allows for robust filtering in both linear and nonlinear systems.", "label": "1", "is_selected": "0", "text": "Many sensors, such as range, sonar, radar, GPS and visual devices, produce measurements which are contaminated by outliers. This problem can be addressed by using fat-tailed sensor models, which account for the possibility of outliers. Unfortunately, all estimation algorithms belonging to the family of Gaussian filters (such as the widely-used extended Kalman filter and unscented Kalman filter) are inherently incompatible with such fat-tailed sensor models. The contribution of this paper is to show that any Gaussian filter can be made compatible with fat-tailed sensor models by applying one simple change: Instead of filtering with the physical measurement, we propose to filter with a pseudo measurement obtained by applying a feature function to the physical measurement. We derive such a feature function which is optimal under some conditions. Simulation results show that the proposed method can effectively handle measurement outliers and allows for robust filtering in both linear and nonlinear systems."}
{"original_text": "In the last decade, social media has evolved as one of the leading platform to create, share, or exchange information; it is commonly used as a way for individuals to maintain social connections. In this online digital world, people use to post texts or pictures to express their views socially and create user-user engagement through discussions and conversations. Thus, social media has established itself to bear signals relating to human behavior. One can easily design user characteristic network by scraping through someone's social media profiles. In this paper, we investigate the potential of social media in characterizing and understanding predominant drunk texters from the perspective of their social, psychological and linguistic behavior as evident from the content generated by them. Our research aims to analyze the behavior of drunk texters on social media and to contrast this with non-drunk texters. We use Twitter social media to obtain the set of drunk texters and non-drunk texters and show that we can classify users into these two respective sets using various psycholinguistic features with an overall average accuracy of 96.78 with very high precision and recall. Note that such an automatic classification can have far-reaching impact - (i) on health research related to addiction prevention and control, and (ii) in eliminating abusive and vulgar contents from Twitter, borne by the tweets of drunk texters.", "label": "1", "is_selected": "0", "text": "In the last decade, social media has evolved as one of the leading platform to create, share, or exchange information; it is commonly used as a way for individuals to maintain social connections. In this online digital world, people use to post texts or pictures to express their views socially and create user-user engagement through discussions and conversations. Thus, social media has established itself to bear signals relating to human behavior. One can easily design user characteristic network by scraping through someone's social media profiles. In this paper, we investigate the potential of social media in characterizing and understanding predominant drunk texters from the perspective of their social, psychological and linguistic behavior as evident from the content generated by them. Our research aims to analyze the behavior of drunk texters on social media and to contrast this with non-drunk texters. We use Twitter social media to obtain the set of drunk texters and non-drunk texters and show that we can classify users into these two respective sets using various psycholinguistic features with an overall average accuracy of 96.78 with very high precision and recall. Note that such an automatic classification can have far-reaching impact - (i) on health research related to addiction prevention and control, and (ii) in eliminating abusive and vulgar contents from Twitter, borne by the tweets of drunk texters."}
{"original_text": "Cyber is the newest domain of war, and the topic of cyber warfare is one that is receiving increasing attention. Research efforts into cyber warfare are extensive, covering a range of issues from technological and legal aspects to more conceptual and historical topics. As a part of the research into cyber warfare, and as a part of the MA program Cyber Security at The Royal Danish Defence College, we have created an overview of some of the most important publications and experts in the field. This overview is not an exhaustive list, but rather a collection of books, articles and experts that we have found to be of particular interest and value. This page is dedicated to publications and experts from 2010 to the present. For publications from 2000-2009, click here. The purpose of this collection is to serve as a bibliography on cyber warfare, and to give new researchers in the field a starting point and inspiration for further reading. In this way, we hope that the field will continue to grow and develop. We encourage our readers to suggest additions to this bibliography. Please send suggestions to the following address: Kevin M. Benson and Michael A. Innes, eds. Cyber Warfare: The next Digital Battlefield. London: Routledge, 2012. Contributors include: Martin Libicki, Rolf Tamnes, Thomas Wingfield, Lt. Gen. Charles Croom, S.J. Czupinski, and Joel Brenner Topics covered include: the strategic role of cyberspace, cyber warfare and strategy, and cyber security, ethics, and law. Sebastian Bruns and Steven Myers \"Cyber Warfare: The State of the Art and a Way Forward.\" Information Security: An International Journal, vol. 29, no. 1 (2012): 19-28. The authors argue that cyber warfare is currently at the level of tactics and that there is a need to develop more strategic cyber weapons. The authors also provide a definition of cyber warfare: \"the use of cyber weapons to incapacitate critical information infrastructure components with the intent to create physical effects that would result in casualties or other significant consequences.\" Thomas Rid and Peter McBurney \"The Fifth Domain? The War Over Cyberspace.\" Foreign Policy, (2012). Rid and McBurney argue that cyber warfare has been overhyped, and that cyber attacks are best understood as a form of espionage or sabotage. Richard A. Clarke and Robert K. Knake New York: Harper Collins, 2010. The authors argue that the U.S. has been complacent about the emerging threats in cyberspace. Both the U.S. and China are mentioned as strategic cyber threats. Clarke and Knake furthermore offer a framework for analyzing cyber threats. Gregory J. Rattray Strategic Warfare in Cyberspace. Cambridge: MIT Press, 2011. Rattray argues that we need to understand cyberspace in terms of its military potential. He suggests a framework for understanding and analyzing cyber warfare, and gives a historical overview of the military potential of the different technologies that have been developed for information and communications. \"A Cyber Arms Race?\" In his speech, Blair argues that the cyber threat is growing and that a cyber arms race is underway. He furthermore argues that the private sector is not willing to spend the necessary resources on cyber security. NATO's cyber defense center of excellence is mentioned as an important institution in the fight against cyber threats. Cyber War: The Next Threat to National Security and What to Do About It. New York: HarperCollins, 2010. Clarke argues that cyber warfare is already a reality, and that the U.S. government has not taken the necessary steps to protect the country from cyber threats. Clarke argues that the U.S. is vulnerable to cyber attacks from foreign governments and terrorist organizations. A Declaration of Cyber-War. The New York Times, (2010). This is a response to a cyber attack on the Pentagon. The Pentagon argues that the", "label": "0", "is_selected": "1", "text": "The Royal Danish Defence College has created an overview of some of the most important publications and experts in the field of cyber warfare, as a part of the MA program Cyber Security at the college."}
{"original_text": "Adversarial sample attacks perturb benign inputs to induce DNN misbehaviors. Recent research has demonstrated the widespread presence and the devastating consequences of such attacks. Existing defense techniques either assume prior knowledge of specific attacks or may not work well on complex models due to their underlying assumptions. We argue that adversarial sample attacks are deeply entangled with interpretability of DNN models: while classification results on benign inputs can be reasoned based on the human perceptible featuresattributes, results on adversarial samples can hardly be explained. Therefore, we propose a novel adversarial sample detection technique for face recognition models, based on interpretability. It features a novel bi-directional correspondence inference between attributes and internal neurons to identify neurons critical for individual attributes. The activation values of critical neurons are enhanced to amplify the reasoning part of the computation and the values of other neurons are weakened to suppress the uninterpretable part. The classification results after such transformation are compared with those of the original model to detect adversaries. Results show that our technique can achieve 94 detection accuracy for 7 different kinds of attacks with 9.91 false positives on benign inputs. In contrast, a state-of-the-art feature squeezing technique can only achieve 55 accuracy with 23.3 false positives.", "label": "1", "is_selected": "0", "text": "Adversarial sample attacks perturb benign inputs to induce DNN misbehaviors. Recent research has demonstrated the widespread presence and the devastating consequences of such attacks. Existing defense techniques either assume prior knowledge of specific attacks or may not work well on complex models due to their underlying assumptions. We argue that adversarial sample attacks are deeply entangled with interpretability of DNN models: while classification results on benign inputs can be reasoned based on the human perceptible featuresattributes, results on adversarial samples can hardly be explained. Therefore, we propose a novel adversarial sample detection technique for face recognition models, based on interpretability. It features a novel bi-directional correspondence inference between attributes and internal neurons to identify neurons critical for individual attributes. The activation values of critical neurons are enhanced to amplify the reasoning part of the computation and the values of other neurons are weakened to suppress the uninterpretable part. The classification results after such transformation are compared with those of the original model to detect adversaries. Results show that our technique can achieve 94 detection accuracy for 7 different kinds of attacks with 9.91 false positives on benign inputs. In contrast, a state-of-the-art feature squeezing technique can only achieve 55 accuracy with 23.3 false positives."}
{"original_text": "Great successes of deep neural networks have been witnessed in various real applications. Many algorithmic and implementation techniques have been developed; however, theoretical understanding of many aspects of deep neural networks is far from clear. A particular interesting issue is the usefulness of dropout, which was motivated from the intuition of preventing complex co-adaptation of feature detectors. In this paper, we study the Rademacher complexity of different types of dropout, and our theoretical results disclose that for shallow neural networks (with one or none hidden layer) dropout is able to reduce the Rademacher complexity in polynomial, whereas for deep neural networks it can amazingly lead to an exponential reduction.", "label": "1", "is_selected": "0", "text": "Great successes of deep neural networks have been witnessed in various real applications. Many algorithmic and implementation techniques have been developed; however, theoretical understanding of many aspects of deep neural networks is far from clear. A particular interesting issue is the usefulness of dropout, which was motivated from the intuition of preventing complex co-adaptation of feature detectors. In this paper, we study the Rademacher complexity of different types of dropout, and our theoretical results disclose that for shallow neural networks (with one or none hidden layer) dropout is able to reduce the Rademacher complexity in polynomial, whereas for deep neural networks it can amazingly lead to an exponential reduction."}
{"original_text": "One of the roadmap plans for quantum computers is an integration within HPC ecosystems assigning them a role of accelerators for a variety of computationally hard tasks. However, in the near term, quantum hardware will be in a constant state of change. Heading towards solving real-world problems, we advocate development of portable, architecture-agnostic hybrid quantum-classical frameworks and demonstrate one for the community detection problem evaluated using quantum annealing and gate-based universal quantum computation paradigms.", "label": "1", "is_selected": "0", "text": "One of the roadmap plans for quantum computers is an integration within HPC ecosystems assigning them a role of accelerators for a variety of computationally hard tasks. However, in the near term, quantum hardware will be in a constant state of change. Heading towards solving real-world problems, we advocate development of portable, architecture-agnostic hybrid quantum-classical frameworks and demonstrate one for the community detection problem evaluated using quantum annealing and gate-based universal quantum computation paradigms."}
{"original_text": "Object detection and instance segmentation are dominated by region-based methods such as Mask RCNN. However, there is a growing interest in reducing these problems to pixel labeling tasks, as the latter could be more efficient, could be integrated seamlessly in image-to-image network architectures as used in many other tasks, and could be more accurate for objects that are not well approximated by bounding boxes. In this paper we show theoretically and empirically that constructing dense pixel embeddings that can separate object instances cannot be easily achieved using convolutional operators. At the same time, we show that simple modifications, which we call semi-convolutional, have a much better chance of succeeding at this task. We use the latter to show a connection to Hough voting as well as to a variant of the bilateral kernel that is spatially steered by a convolutional network. We demonstrate that these operators can also be used to improve approaches such as Mask RCNN, demonstrating better segmentation of complex biological shapes and PASCAL VOC categories than achievable by Mask RCNN alone.", "label": "1", "is_selected": "0", "text": "Object detection and instance segmentation are dominated by region-based methods such as Mask RCNN. However, there is a growing interest in reducing these problems to pixel labeling tasks, as the latter could be more efficient, could be integrated seamlessly in image-to-image network architectures as used in many other tasks, and could be more accurate for objects that are not well approximated by bounding boxes. In this paper we show theoretically and empirically that constructing dense pixel embeddings that can separate object instances cannot be easily achieved using convolutional operators. At the same time, we show that simple modifications, which we call semi-convolutional, have a much better chance of succeeding at this task. We use the latter to show a connection to Hough voting as well as to a variant of the bilateral kernel that is spatially steered by a convolutional network. We demonstrate that these operators can also be used to improve approaches such as Mask RCNN, demonstrating better segmentation of complex biological shapes and PASCAL VOC categories than achievable by Mask RCNN alone."}
{"original_text": "This paper considers a network of stochastic evidence accumulators, each represented by a drift-diffusion model accruing evidence towards a decision in continuous time by observing a noisy signal and by receiving a time-varying external input. In a mean-field scenario, the influence of one agent on the other is effectively captured by the mean field associated with the distribution of the decision variables. We present an approximate mean-field model to describe the population dynamics of the network. We derive approximate mean-field equations of the decision variables in terms of the distribution of the decision variables, and obtain a closed-form expression of the expected time to reach a decision in the asymptotic limit. Moreover, we show that when the decision problem can be linearly decomposed into subproblems, the mean-field model can be reduced to a set of independent subproblems. These results provide a rigorous mean-field approximation of the population dynamics of stochastic evidence accumulators in continuous time. We discuss extensions of the mean-field model and provide a numerical comparison of the mean-field model with the original system for a specific example.", "label": "0", "is_selected": "1", "text": "The population dynamics of a network of evidence accumulators are studied in terms of a mean-field model for the distribution of the decision variables and their expected time to reach a decision."}
{"original_text": "Real-world audio recordings are often degraded by factors such as noise, reverberation, and equalization distortion. This paper introduces HiFi-GAN, a deep learning method to transform recorded speech to sound as if it had been recorded in a different acoustic environment. HiFi-GAN is trained in a fully supervised fashion, using pairs of matched multi-channel recordings that are free from additive noise. Matching recordings are obtained by recording real-world sounds in a number of acoustic environments, and using a non-linear filter to match the frequency and time responses of the acoustic environments. The resulting speech is free from additive noise, and can therefore be used to train a GAN to map any input signal to the corresponding signal from the matched recording. In contrast to previous methods, HiFi-GAN can be trained to produce realistic-sounding speech, while also being robust to mismatches between the training and test acoustic environments. HiFi-GAN is shown to produce better speech quality than the state of the art on both clean and noisy speech. In a speaker recognition task, HiFi-GAN is shown to provide significant performance improvements over state-of-the-art audio enhancement methods. Previous ArticleDual Discriminator GANs for Text-to-Image Synthesis.", "label": "0", "is_selected": "1", "text": "A novel deep learning method for improving speech quality has been developed.<n> The resulting speech is free from additive noise, and can therefore be used to train a GAN to map any input signal to the corresponding signal from the matched recording."}
{"original_text": "Compromised social media accounts are legitimate user accounts that have been hijacked by a malicious party and can cause various kinds of damage, which makes the detection of these accounts crucial. In this work we propose a novel general framework for discovering compromised accounts by utilizing statistical text analysis. The framework is built on the observation that users will use language that is measurably different from the language that an attacker would use, when the account is compromised. We use the framework to develop specific algorithms based on language modeling and use the similarity of language models of users and attackers as features in a supervised learning setup to identify compromised accounts. Evaluation results on a large Twitter corpus of over 129 million tweets show promising results of the proposed approach.", "label": "1", "is_selected": "0", "text": "Compromised social media accounts are legitimate user accounts that have been hijacked by a malicious party and can cause various kinds of damage, which makes the detection of these accounts crucial. In this work we propose a novel general framework for discovering compromised accounts by utilizing statistical text analysis. The framework is built on the observation that users will use language that is measurably different from the language that an attacker would use, when the account is compromised. We use the framework to develop specific algorithms based on language modeling and use the similarity of language models of users and attackers as features in a supervised learning setup to identify compromised accounts. Evaluation results on a large Twitter corpus of over 129 million tweets show promising results of the proposed approach."}
{"original_text": "In tensor completion, the latent nuclear norm is commonly used to induce low-rank structure, while substantially failing to capture the global information due to the utilization of unbalanced unfolding scheme. In this paper, we propose to enhance the global information by adaptively selecting the unfolding scheme to create balanced unfoldings. We also extend the conventional singular value thresholding (SVT) approach by using the total variation (TV) to exploit the sparsity structure, which is particularly useful when the underlying tensor is sparse. We then analyze the proposed TV-SVT method and establish its recovery guarantees. Specifically, under the setting that the error tensor is bounded in its Frobenius norm, we show that by employing TV-SVT method with adaptive unfolding scheme, we can recover the underlying tensor exactly, as long as the nuclear norm is less than 12. Furthermore, extensive simulations demonstrate the superiority of the proposed method over the state-of-the-art tensor completion approaches. The work was supported in part by National Natural Science Foundation of China under Grant 61472037, the 973 Program of China under Grant 2012CB720904, in part by the Natural Science Foundation of Shandong Province of China under Grant ZR2015AQ006, in part by the Applied Basic Research Project of Yantai under Grant 2015YTSYGJJ0010, and in part by the Key Laboratory of Computer Network and Information Integration of Ministry of Education, Southeast University, Nanjing, China.", "label": "0", "is_selected": "1", "text": "In this paper, we propose a new method for tensor completion, which is particularly useful when the underlying tensor is sparse and the nuclear norm is less than 12."}
{"original_text": "GANs can generate photo-realistic images from the domain of their training data. However, those wanting to use them for creative purposes often want to generate imagery from a truly novel image, not just one that is a member of a class of images. In this paper, we propose to leverage the underlying semantics of the data to enable generation from a truly novel image. To this end, we propose a novel approach, called VQ-VAE, that converts a semantically meaningful representation into a compressed representation. We show that this approach is competitive to state-of-the-art approaches on visual classification and reconstruction tasks. We further show that, by combining VQ-VAE with a generative model, we can generate photo-realistic images from truly novel images. We demonstrate that the same generator can be used to synthesize images from different domains. We show results on images from the domain of birds, and the style domain of Van Gogh. We show that our approach is capable of generating novel images for the domains of birds and Van Gogh. We show that our approach is able to generate novel images of cats by conditioning on a text description of a cat. We show that our approach can generate images of cats from a novel image of a cat. We show that our approach is able to generate images of birds by conditioning on a text description of a bird. We show that our approach can generate images of birds from a novel image of a bird.", "label": "0", "is_selected": "1", "text": "generative adversarial networks (GANs) have been used to train computer systems on a wide range of visual tasks, from image recognition to learning new languages."}
{"original_text": "Computed tomography (CT) is critical for various clinical applications, e.g., radiotherapy treatment planning and also PET attenuation correction. However, CT exposes radiation during acquisition, which may cause side effects to patients. In this paper, we propose a deep convolutional neural network (DCNN) to reconstruct CT images from the photon-counting CT (PCCT) measurements acquired with a new X-ray CT system. The proposed DCNN is trained with the MNIST dataset, and we use the center region of the MNIST images as the training dataset. We also propose a new data augmentation method to improve the generalization ability of the proposed DCNN. We have conducted experiments on both simulated and real clinical CT images. Experimental results show that the proposed DCNN can effectively reconstruct the images from the PCCT measurements. The online version of this chapter contains supplementary material, which is available to authorized users.", "label": "0", "is_selected": "0", "text": "Computed tomography (CT) is critical for various clinical applications, e.g., radiotherapy treatment planning and also PET attenuation correction. However, CT exposes radiation during acquisition, which may cause side effects to patients. In this paper, we propose a deep convolutional neural network (DCNN) to reconstruct CT images from the photon-counting CT (PCCT) measurements acquired with a new X-ray CT system. The proposed DCNN is trained with the MNIST dataset, and we use the center region of the MNIST images as the training dataset. We also propose a new data augmentation method to improve the generalization ability of the proposed DCNN. We have conducted experiments on both simulated and real clinical CT images. Experimental results show that the proposed DCNN can effectively reconstruct the images from the PCCT measurements. The online version of this chapter contains supplementary material, which is available to authorized users."}
{"original_text": "A picture is worth a thousand words. Albeit a cliche, for the fashion industry, an image of a clothing piece allows one to perceive its category (e.g., dress), sub-category (e.g., day dress) and properties (e.g., white colour with floral patterns). The seasonal nature of the fashion industry creates a highly dynamic and creative domain with evermore data, making it unpractical to manually describe a large set of images (of products). In this paper, we explore the concept of visual recognition for fashion images through an end-to-end architecture embedding the hierarchical nature of the annotations directly into the model. Towards that goal, and inspired by the work of (,), we have modified and adapted the original architecture proposal. Namely, we have removed the message passing layer symmetry to cope with Farfetch category tree, added extra layers for hierarchy level specificity, and moved the message passing layer into an enriched latent space. We compare the proposed unified architecture against state-of-the-art models and demonstrate the performance advantage of our model for structured multi-level categorization on a dataset of about 350k fashion product images.", "label": "1", "is_selected": "0", "text": "A picture is worth a thousand words. Albeit a cliche, for the fashion industry, an image of a clothing piece allows one to perceive its category (e.g., dress), sub-category (e.g., day dress) and properties (e.g., white colour with floral patterns). The seasonal nature of the fashion industry creates a highly dynamic and creative domain with evermore data, making it unpractical to manually describe a large set of images (of products). In this paper, we explore the concept of visual recognition for fashion images through an end-to-end architecture embedding the hierarchical nature of the annotations directly into the model. Towards that goal, and inspired by the work of (,), we have modified and adapted the original architecture proposal. Namely, we have removed the message passing layer symmetry to cope with Farfetch category tree, added extra layers for hierarchy level specificity, and moved the message passing layer into an enriched latent space. We compare the proposed unified architecture against state-of-the-art models and demonstrate the performance advantage of our model for structured multi-level categorization on a dataset of about 350k fashion product images."}
{"original_text": "The residual network is now one of the most effective structures in deep learning, which utilizes the skip connections to \"guarantee\" the performance will not get worse. However, the non-convexity of this method makes it hard to provide a theoretical proof for the convergence of the training process. In this paper, we propose a new approach to get rid of the non-convexity of the residual network. In this way, the training process is transformed into a convex problem. Moreover, the objective function is piecewise linear, which makes it easier to get the global optimal. Finally, we evaluate our method on CIFAR-10 dataset and reach a new state-of-art result of 3.53, which also prove the effectiveness of our approach.", "label": "0", "is_selected": "1", "text": "This paper presents a new approach for the convergence of the residual network and the non-ity of this network., as well as a new method for the training of the state-of-AR3, which proves the effectiveness of this approach."}
{"original_text": "Recent work by (,) looks at semantic parser induction and question answering in the domain of source code libraries and APIs. In this brief note, we formalize the representations being learned in these studies and introduce a simple domain specific language and a systematic translation from this language to first-order logic. By recasting the target representations in terms of classical logic, we aim to broaden the applicability of existing code datasets for investigating more complex natural language understanding and reasoning problems in the software domain.", "label": "1", "is_selected": "0", "text": "Recent work by (,) looks at semantic parser induction and question answering in the domain of source code libraries and APIs. In this brief note, we formalize the representations being learned in these studies and introduce a simple domain specific language and a systematic translation from this language to first-order logic. By recasting the target representations in terms of classical logic, we aim to broaden the applicability of existing code datasets for investigating more complex natural language understanding and reasoning problems in the software domain."}
{"original_text": "Existing source-code-generating tools such as Lex and Yacc suffer from practical inconveniences because they use disembodied code to implement actions. To prevent this problem, such tools could generate closed functors rather than disembodied code. Unfortunately, the generated functor would then have to be parsed by a general-purpose parser, resulting in quadratic time complexity. We show that a system that generates closed functors can be designed to avoid this problem, and as a result can be made to run in constant time.", "label": "0", "is_selected": "1", "text": "In this paper, we develop a source-code-generating system that can be made to run in time, without the need for disembodied code to implement actions."}
{"original_text": "At the core of any inference procedure in deep neural networks are dot product operations, which are the component that require the highest computational resources. For instance, deep neural networks are only becoming more popular in real-time computer vision tasks due to recent developments in more efficient dot product operations. In this paper, we compare different methods for computing dot products in deep neural networks. We show that the popular cuDNN implementation is not the best choice for all neural network architectures, and we propose a novel implementation that uses SIMD vector instructions. Our implementation achieves higher performance and higher energy efficiency than both cuDNN and other SIMD-based approaches.", "label": "0", "is_selected": "0", "text": "At the core of any inference procedure in deep neural networks are dot product operations, which are the component that require the highest computational resources. For instance, deep neural networks are only becoming more popular in real-time computer vision tasks due to recent developments in more efficient dot product operations. In this paper, we compare different methods for computing dot products in deep neural networks. We show that the popular cuDNN implementation is not the best choice for all neural network architectures, and we propose a novel implementation that uses SIMD vector instructions. Our implementation achieves higher performance and higher energy efficiency than both cuDNN and other SIMD-based approaches."}
{"original_text": "This document serves to complement our website which was developed with the aim of exposing the students to Gaussian Processes (GPs). GPs are non-parametric bayesian regression models that are largely used by statisticians and geospatial data scientists for modeling spatial data. Several open source libraries spanning from Matlab , Python , R etc. are already available for simple plug-and-use. The objective of this handout and in turn the website was to allow the users to develop stand-alone GPs in Python by relying on minimal external dependencies. To this end, we only use the default python modules and assist the users in developing their own GPs from scratch giving them an in-depth knowledge of what goes on under the hood. The module covers GP inference using maximum likelihood estimation (MLE) and gives examples for 1D (dummy) spatial data.", "label": "1", "is_selected": "0", "text": "This document serves to complement our website which was developed with the aim of exposing the students to Gaussian Processes (GPs). GPs are non-parametric bayesian regression models that are largely used by statisticians and geospatial data scientists for modeling spatial data. Several open source libraries spanning from Matlab , Python , R etc. are already available for simple plug-and-use. The objective of this handout and in turn the website was to allow the users to develop stand-alone GPs in Python by relying on minimal external dependencies. To this end, we only use the default python modules and assist the users in developing their own GPs from scratch giving them an in-depth knowledge of what goes on under the hood. The module covers GP inference using maximum likelihood estimation (MLE) and gives examples for 1D (dummy) spatial data."}
{"original_text": "The increase in the world's population and rising standards of living is leading to an ever-increasing number of vehicles on the roads, and with it ever-increasing difficulties in traffic management. More and more cities are making the decision to invest in the latest technologies in order to make the traffic flow more smoothly. The main challenge for municipal authorities is to make optimum use of the available space. In the past, the main goal was to increase the capacities of the streets and highways. With the increasing traffic volumes, it is becoming more and more difficult to achieve this with only this approach. A complex challenge that calls for a complex solution! For this reason, the current trend is to use dynamic traffic management systems. They are used to optimise traffic flows on the roads and to improve the information offered to motorists. A traffic management system consists of various subsystems that communicate with each other. Among these are traffic control centres, mobile applications and variable traffic signs. Variable traffic signs play an essential role in traffic management. They provide information on traffic situations, enabling motorists to adapt their behaviour in order to prevent traffic congestion. The signs also provide a connection between the infrastructure and the motorist, providing dynamic information on the current traffic situation. A traffic control centre is an important hub in the traffic management network. The control centre receives the traffic and travel time data collected from the traffic network and the traffic signs, and processes the information so that the traffic situation can be evaluated and optimised. We use the information and image data collected from the road infrastructure and the signs to calculate traffic flow forecasts. This enables us to detect traffic jams early and to develop effective measures to avoid them. We use the information to evaluate the road network, to find out which components are causing the bottlenecks. With our experience and knowledge of traffic management systems, we offer the municipal authorities the optimum solution for their traffic situation. We use this information to develop a tailored concept for the traffic management system for a particular city. Our traffic management system is designed so that it can be adapted to the local conditions. We offer a range of options for transmitting information and image data to the traffic control centre. Our control centres can be adapted to meet the specific requirements of each customer. The information from the traffic signs is integrated with the information from the road infrastructure, enabling traffic to be optimally managed. The system can be extended at any time, enabling further subsystems to be integrated. We support our customers with the operation of the traffic management system and the infrastructure, ensuring the smooth operation of the system. We offer our customers a complete range of services for the traffic control centre. We operate the traffic management system 24 hours a day, 7 days a week, from our central control centre. We offer our customers extensive support with the operation of the control centre. We offer our customers the option of monitoring and maintaining the control centre, the infrastructure and the signs. We are the only company to offer an integrated service package. Our employees have been trained to the latest standards.", "label": "0", "is_selected": "1", "text": "Traffic management systems play an important role in managing the flow of traffic on the roads and motorways of the world's major cities. They provide information on traffic situations, enabling motorists to adapt their behaviour in order to prevent traffic congestion."}
{"original_text": "Fruit tree pruning and fruit thinning require a powerful vision system that can provide high resolution segmentation of the fruit trees and their branches. However, recent works only consider the detection of single trees or the localized counting of the fruits per tree. In this work, we introduce a new fully convolutional architecture for the detection and counting of the fruit trees, as well as for the delineation of their branches and fruits. We train our model end-to-end, thus making it possible to count fruits with fewer false negatives and false positives compared to the state-of-the-art. We also show that a more precise pruning and thinning can be achieved if the spatial relations between the detected fruit trees and their branches are exploited. Our results, on an annotated and a publicly available dataset, show that our method achieves a much higher precision and recall than the state-of-the-art. IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019", "label": "0", "is_selected": "0", "text": "Fruit tree pruning and fruit thinning require a powerful vision system that can provide high resolution segmentation of the fruit trees and their branches. However, recent works only consider the detection of single trees or the localized counting of the fruits per tree. In this work, we introduce a new fully convolutional architecture for the detection and counting of the fruit trees, as well as for the delineation of their branches and fruits. We train our model end-to-end, thus making it possible to count fruits with fewer false negatives and false positives compared to the state-of-the-art. We also show that a more precise pruning and thinning can be achieved if the spatial relations between the detected fruit trees and their branches are exploited. Our results, on an annotated and a publicly available dataset, show that our method achieves a much higher precision and recall than the state-of-the-art. IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019"}
{"original_text": "We live and cooperate in networks. However, links in networks only allow forpairwise interactions, thus making the framework suitable for dyadic games, butnot for games that are played in groups. We here introduce a model fornetwork games with groups that facilitates a deeper understanding of thecomplementarity in group interactions. This model is based on anagency-theoretic setting that includes the option of group interactions, aseparate choice of group size, and a method for the assessment of groupcooperations. We illustrate the potentials of this new model by means ofan experiment that allows for direct comparisons between network gameswith groups and dyadic games, both with and without a punishmentoption. Our results show that this group structure significantly improvesthe cooperation level, even in the absence of a punishment mechanism. Thisenhancement is not due to the specific group formation, but results from theincreased number of links in the game network.", "label": "0", "is_selected": "0", "text": "We live and cooperate in networks. However, links in networks only allow forpairwise interactions, thus making the framework suitable for dyadic games, butnot for games that are played in groups. We here introduce a model fornetwork games with groups that facilitates a deeper understanding of thecomplementarity in group interactions. This model is based on anagency-theoretic setting that includes the option of group interactions, aseparate choice of group size, and a method for the assessment of groupcooperations. We illustrate the potentials of this new model by means ofan experiment that allows for direct comparisons between network gameswith groups and dyadic games, both with and without a punishmentoption. Our results show that this group structure significantly improvesthe cooperation level, even in the absence of a punishment mechanism. Thisenhancement is not due to the specific group formation, but results from theincreased number of links in the game network."}
{"original_text": "Context: In C, low-level errors, such as buffer overflow and use-after-free, are a major problem, as they cause security vulnerabilities and hard-to-find bugs. C lacks automatic checks, and programmers cannot afford to perform manual checks on every memory access due to the overhead. Researchers have proposed several solutions to check for buffer overflows and use-after-frees, such as shadow memory and static taint analysis, but they are not as efficient as context-aware tracking of pointers. Existing approaches of context-aware pointer tracking are inefficient, because they keep track of pointers by using a shadow memory, which leads to a large memory overhead and the inability to track pointers that are returned from function calls. Objectives: In this paper, we propose a context-aware pointer tracking approach for C called context-aware shadow memory (CASM). Our approach introduces a novel shadow memory that tracks pointers with a reduced memory overhead, and a novel static pointer analysis that exploits the reduced shadow memory size to track pointers that are returned from function calls. The main contributions of our approach are: (1) the novel shadow memory that tracks pointers with a reduced memory overhead; and (2) the novel static pointer analysis that uses the reduced memory overhead of our novel shadow memory to track pointers that are returned from function calls. Methods: To track pointers, we use shadow memory, which is a memory that is logically separate from the real memory, and stores all the memory blocks that are currently in use. We maintain shadow memory entries for every memory block. Each shadow memory entry consists of a pointer to the corresponding real memory block, and a contextual description of the pointer. The contextual description is used to check whether a pointer access is valid. If a pointer access is invalid, then the corresponding real memory block is marked as invalid. To reduce the memory overhead of shadow memory, we propose a novel shadow memory called compressed shadow memory (CSM), which is a multi-dimensional array of pointers to the corresponding real memory blocks. The dimensions of CSM represent different contexts of the pointers. The dimensions of CSM are predefined, and the number of pointers in each dimension is determined at compile time. We also propose a novel static pointer analysis that uses the reduced memory overhead of our novel shadow memory to track pointers that are returned from function calls. The main idea of this static pointer analysis is to track the pointers that are returned from function calls in the function call stack. We propose a novel function call stack that represents the function call stack with a multi-dimensional array of pointers to the corresponding function calls. The dimensions of the function call stack represent the functions and the contexts of the pointers. The number of pointers in each dimension of the function call stack is determined at compile time, and it depends on the number of different contexts of the pointers in the function calls. We also propose a novel pointer transfer analysis that uses the pointer tracking information of CASM to determine the locations of the shadow memory entries for the pointers that are returned from function calls. Results: We have implemented CASM in a prototype tool that is based on LLVM. We evaluated CASM on a set of benchmarks. Our evaluation results show that CASM is more efficient than the state-of-the-art in terms of time and memory overhead.", "label": "0", "is_selected": "1", "text": "In this paper, we propose a context-aware pointer tracking approach for the C programming language to check for buffer overflows and use-after-frees, and introduce a novel shadow memory that reduces memory overhead."}
{"original_text": "Shannon's theory of information was built on the assumption that the information carriers were classical systems. Its quantum counterpart, quantum Shannon theory, explores the new possibilities arising when the information carriers are quantum systems. Traditionally, quantum Shannon theory has focussed on scenarios where the internal state of the information carriers is quantum, while their trajectory is classical. Here we propose a second level of quantisation where both the information and its propagation in spacetime is treated quantum mechanically. The framework is illustrated with a number of examples, showcasing some of the counterintuitive phenomena taking place when information travels simultaneously through multiple transmission lines.", "label": "1", "is_selected": "0", "text": "Shannon's theory of information was built on the assumption that the information carriers were classical systems. Its quantum counterpart, quantum Shannon theory, explores the new possibilities arising when the information carriers are quantum systems. Traditionally, quantum Shannon theory has focussed on scenarios where the internal state of the information carriers is quantum, while their trajectory is classical. Here we propose a second level of quantisation where both the information and its propagation in spacetime is treated quantum mechanically. The framework is illustrated with a number of examples, showcasing some of the counterintuitive phenomena taking place when information travels simultaneously through multiple transmission lines."}
{"original_text": "In this paper, a novel experienced deep reinforcement learning (deep-RL) framework is proposed to provide model-free resource allocation for in the downlink of a wireless network. The goal is to allocate power and time slots to users in a downlink such that the sum rate of all users is maximized. The key idea is to transform the sum rate maximization problem into a Markov decision process (MDP) and use a deep-RL method to solve it. This framework is a model-free learning approach because no state-action value function is required to be trained or stored. Exploration and exploitation are implemented in a manner that takes advantage of the Markov property of the network. In this way, the learning is more efficient, and the method is more general. Deep-RL algorithms have demonstrated a great success in many applications, such as image recognition, machine translation, and game playing, but they have never been used to solve resource allocation problems in wireless networks. Our numerical results show that our proposed method can converge to a near optimal solution for the resource allocation problem within 1000 time slots, and the performance is near to the theoretical limit for the system.", "label": "0", "is_selected": "0", "text": "In this paper, a novel experienced deep reinforcement learning (deep-RL) framework is proposed to provide model-free resource allocation for in the downlink of a wireless network. The goal is to allocate power and time slots to users in a downlink such that the sum rate of all users is maximized. The key idea is to transform the sum rate maximization problem into a Markov decision process (MDP) and use a deep-RL method to solve it. This framework is a model-free learning approach because no state-action value function is required to be trained or stored. Exploration and exploitation are implemented in a manner that takes advantage of the Markov property of the network. In this way, the learning is more efficient, and the method is more general. Deep-RL algorithms have demonstrated a great success in many applications, such as image recognition, machine translation, and game playing, but they have never been used to solve resource allocation problems in wireless networks. Our numerical results show that our proposed method can converge to a near optimal solution for the resource allocation problem within 1000 time slots, and the performance is near to the theoretical limit for the system."}
{"original_text": "Signal estimation problems with smoothness and sparsity priors can be naturally modeled as quadratic optimization with l 0 norm\" constraints. Since such problems are non-convex and hard-to-solve, the standard approach is to relax the original problem to its convex relaxation, where the l 0 norm is approximated by an l 1 norm. While the solution of the relaxed problem is not guaranteed to be an optimal solution of the original problem, it has been observed in practice that the relaxed solution performs satisfactorily when the underlying signal is sparse or compressible in some known transform domain. However, the relaxation may fail to produce satisfactory results if the signal is not sufficiently sparse or compressible. The authors present a framework to improve the solution of the relaxed problem by exploiting the fact that the solution of the relaxed problem can be obtained by minimizing a strictly convex function (or a non-negative function) over the simplex. They propose to find a stationary point (or a local optimum) of the original non-convex problem by using a local search algorithm based on a Newton-type method for strictly convex functions. They present some theoretical results that justify the use of the Newton-type method. The proposed method is applied to several signal estimation problems and compared with existing algorithms. Experimental results show that the proposed method is able to provide improved solutions over the relaxed problem.", "label": "0", "is_selected": "0", "text": "Signal estimation problems with smoothness and sparsity priors can be naturally modeled as quadratic optimization with l 0 norm\" constraints. Since such problems are non-convex and hard-to-solve, the standard approach is to relax the original problem to its convex relaxation, where the l 0 norm is approximated by an l 1 norm. While the solution of the relaxed problem is not guaranteed to be an optimal solution of the original problem, it has been observed in practice that the relaxed solution performs satisfactorily when the underlying signal is sparse or compressible in some known transform domain. However, the relaxation may fail to produce satisfactory results if the signal is not sufficiently sparse or compressible. The authors present a framework to improve the solution of the relaxed problem by exploiting the fact that the solution of the relaxed problem can be obtained by minimizing a strictly convex function (or a non-negative function) over the simplex. They propose to find a stationary point (or a local optimum) of the original non-convex problem by using a local search algorithm based on a Newton-type method for strictly convex functions. They present some theoretical results that justify the use of the Newton-type method. The proposed method is applied to several signal estimation problems and compared with existing algorithms. Experimental results show that the proposed method is able to provide improved solutions over the relaxed problem."}
{"original_text": "Centrality measures such as the degree, k-shell, or eigenvalue centrality can identify a network's most influential nodes, but are rarely usefully accurate in quantifying the spreading power of the vast majority of nodes which are not highly influential. The spreading power of all network nodes is better explained by considering, from a continuous-time epidemiological perspective, the distribution of the force of infection each node generates. The resulting metric, the Expected Force (ExF), accurately quantifies node spreading power under all primary epidemiological models across a wide range of archetypical human contact networks. When node power is low, influence is a function of neighbor degree. As power increases, a node's own degree becomes more important. The strength of this relationship is modulated by network structure, being more pronounced in narrow, dense networks typical of social networking and weakening in broader, looser association networks such as Internet webpages. The ExF can be computed independently for individual nodes, making it applicable for networks whose adjacency matrix is dynamic, not well specified, or overwhelmingly large.", "label": "1", "is_selected": "0", "text": "Centrality measures such as the degree, k-shell, or eigenvalue centrality can identify a network's most influential nodes, but are rarely usefully accurate in quantifying the spreading power of the vast majority of nodes which are not highly influential. The spreading power of all network nodes is better explained by considering, from a continuous-time epidemiological perspective, the distribution of the force of infection each node generates. The resulting metric, the Expected Force (ExF), accurately quantifies node spreading power under all primary epidemiological models across a wide range of archetypical human contact networks. When node power is low, influence is a function of neighbor degree. As power increases, a node's own degree becomes more important. The strength of this relationship is modulated by network structure, being more pronounced in narrow, dense networks typical of social networking and weakening in broader, looser association networks such as Internet webpages. The ExF can be computed independently for individual nodes, making it applicable for networks whose adjacency matrix is dynamic, not well specified, or overwhelmingly large."}
{"original_text": "Recent transient-execution attacks, such as RIDL, Fallout, and ZombieLoad, demonstrated that attackers can leak information while it transits through microarchitectural buffers. Named Microarchitectural Data Sampling (MDS) by Intel, these attacks are likened to \"drinking from the firehose,\" as the attacker has little control over what data is observed and from what origin. Unable to prevent the buffers from leaking, Intel issued countermeasures via microcode updates that overwrite the buffers when the CPU changes security domains. In this work we present CacheOut, a new microarchitectural attack that is capable of bypassing Intel's buffer overwrite countermeasures. We observe that as data is being evicted from the CPU's L1 cache, it is often transferred back to the leaky CPU buffers where it can be recovered by the attacker. CacheOut improves over previous MDS attacks by allowing the attacker to choose which data to leak from the CPU's L1 cache, as well as which part of a cache line to leak. We demonstrate that CacheOut can leak information across multiple security boundaries, including those between processes, virtual machines, user and kernel space, and from SGX enclaves.", "label": "1", "is_selected": "0", "text": "Recent transient-execution attacks, such as RIDL, Fallout, and ZombieLoad, demonstrated that attackers can leak information while it transits through microarchitectural buffers. Named Microarchitectural Data Sampling (MDS) by Intel, these attacks are likened to \"drinking from the firehose,\" as the attacker has little control over what data is observed and from what origin. Unable to prevent the buffers from leaking, Intel issued countermeasures via microcode updates that overwrite the buffers when the CPU changes security domains. In this work we present CacheOut, a new microarchitectural attack that is capable of bypassing Intel's buffer overwrite countermeasures. We observe that as data is being evicted from the CPU's L1 cache, it is often transferred back to the leaky CPU buffers where it can be recovered by the attacker. CacheOut improves over previous MDS attacks by allowing the attacker to choose which data to leak from the CPU's L1 cache, as well as which part of a cache line to leak. We demonstrate that CacheOut can leak information across multiple security boundaries, including those between processes, virtual machines, user and kernel space, and from SGX enclaves."}
{"original_text": "Head pose estimation is an important pre-processing step in many pattern recognition and computer vision systems such as face recognition. Since the performance of the face recognition systems is greatly affected by the pose of the face, how to estimate the accurate pose of the face in the face image is still a challenging problem. In this paper, we present a novel method for head pose estimation. To enhance the efficiency of the estimation we first use contourlet transform for feature extraction. Contourlet transform is a multi-resolution, multi-direction transform. Finally, in order to reduce the feature space dimension and obtain appropriate features, we use LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) to remove inefficient features. Then, we apply k-nearest neighborhood (k-NN) and minimum distance classifiers to classify the pose of head. We use the public available FERET database to evaluate the performance of the proposed method. Simulation results indicate the efficiency of the proposed method in comparison with previous method.", "label": "1", "is_selected": "0", "text": "Head pose estimation is an important pre-processing step in many pattern recognition and computer vision systems such as face recognition. Since the performance of the face recognition systems is greatly affected by the pose of the face, how to estimate the accurate pose of the face in the face image is still a challenging problem. In this paper, we present a novel method for head pose estimation. To enhance the efficiency of the estimation we first use contourlet transform for feature extraction. Contourlet transform is a multi-resolution, multi-direction transform. Finally, in order to reduce the feature space dimension and obtain appropriate features, we use LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) to remove inefficient features. Then, we apply k-nearest neighborhood (k-NN) and minimum distance classifiers to classify the pose of head. We use the public available FERET database to evaluate the performance of the proposed method. Simulation results indicate the efficiency of the proposed method in comparison with previous method."}
{"original_text": "In this paper, a novel experienced deep reinforcement learning (deep-RL) framework is proposed to provide model-free resource allocation for in the downlink of a wireless network. The goal is to guarantee high end-to-end reliability and low end-to-end latency, under explicit data rate constraints, for each wireless user without any models of or assumptions on the users' traffic. In particular, in order to enable the deep-RL framework to account for extreme network conditions and operate in highly reliable systems, a new approach based on generative adversarial networks (GANs) is proposed. This GAN approach is used to pre-train the deep-RL framework using a mix of real and synthetic data, thus creating an experienced deep-RL framework that has been exposed to a broad range of network conditions. The proposed deep-RL framework is particularly applied to a multi-user orthogonal frequency division multiple access (OFDMA) resource allocation system. Formally, this resource allocation problem in OFDMA systems is posed as a power minimization problem under reliability, latency, and rate constraints. To solve this problem using experienced deep-RL, first, the rate of each user is determined. Then, these rates are mapped to the resource block and power allocation vectors of the studied wireless system. Finally, the end-to-end reliability and latency of each user are used as feedback to the deep-RL framework. It is then shown that at the fixed-point of the deep-RL algorithm, the reliability and latency of the users are near-optimal. Moreover, for the proposed GAN approach, a theoretical limit for the generator output is analytically derived. Simulation results show how the proposed approach can achieve near-optimal performance within the rate-reliability-latency region, depending on the network and service requirements. The results also show that the proposed experienced deep-RL framework is able to remove the transient training time that makes conventional deep-RL methods unsuitable for. Moreover, during extreme conditions, it is shown that the proposed, experienced deep-RL agent can recover instantly while a conventional deep-RL agent takes several epochs to adapt to new extreme conditions.", "label": "1", "is_selected": "0", "text": "In this paper, a novel experienced deep reinforcement learning (deep-RL) framework is proposed to provide model-free resource allocation for in the downlink of a wireless network. The goal is to guarantee high end-to-end reliability and low end-to-end latency, under explicit data rate constraints, for each wireless user without any models of or assumptions on the users' traffic. In particular, in order to enable the deep-RL framework to account for extreme network conditions and operate in highly reliable systems, a new approach based on generative adversarial networks (GANs) is proposed. This GAN approach is used to pre-train the deep-RL framework using a mix of real and synthetic data, thus creating an experienced deep-RL framework that has been exposed to a broad range of network conditions. The proposed deep-RL framework is particularly applied to a multi-user orthogonal frequency division multiple access (OFDMA) resource allocation system. Formally, this resource allocation problem in OFDMA systems is posed as a power minimization problem under reliability, latency, and rate constraints. To solve this problem using experienced deep-RL, first, the rate of each user is determined. Then, these rates are mapped to the resource block and power allocation vectors of the studied wireless system. Finally, the end-to-end reliability and latency of each user are used as feedback to the deep-RL framework. It is then shown that at the fixed-point of the deep-RL algorithm, the reliability and latency of the users are near-optimal. Moreover, for the proposed GAN approach, a theoretical limit for the generator output is analytically derived. Simulation results show how the proposed approach can achieve near-optimal performance within the rate-reliability-latency region, depending on the network and service requirements. The results also show that the proposed experienced deep-RL framework is able to remove the transient training time that makes conventional deep-RL methods unsuitable for. Moreover, during extreme conditions, it is shown that the proposed, experienced deep-RL agent can recover instantly while a conventional deep-RL agent takes several epochs to adapt to new extreme conditions."}
{"original_text": "We consider the problem of robustly recovering a k -sparse coefficient vector from the Fourier series that it generates, restricted to the interval [ - O, O ]. The difficulty of this problem is linked to the superresolution factor SRF, equal to the ratio of the Rayleigh length (inverse of O) by the spacing of the grid supporting the sparse vector. In the presence of additive deterministic noise of norm s, we show upper and lower bounds on the minimax error rate that both scale like (S R F) - 2 k 1 s, providing a partial answer to a question posed by Donoho in 1992. The scaling arises from comparing the noise level to a restricted isometry constant at sparsity 2 k, or equivalently from comparing 2 k to the so-called s -spark of the Fourier system. The proof involves new bounds on the singular values of restricted Fourier matrices, obtained in part from old techniques in complex analysis.", "label": "1", "is_selected": "0", "text": "We consider the problem of robustly recovering a k -sparse coefficient vector from the Fourier series that it generates, restricted to the interval [ - O, O ]. The difficulty of this problem is linked to the superresolution factor SRF, equal to the ratio of the Rayleigh length (inverse of O) by the spacing of the grid supporting the sparse vector. In the presence of additive deterministic noise of norm s, we show upper and lower bounds on the minimax error rate that both scale like (S R F) - 2 k 1 s, providing a partial answer to a question posed by Donoho in 1992. The scaling arises from comparing the noise level to a restricted isometry constant at sparsity 2 k, or equivalently from comparing 2 k to the so-called s -spark of the Fourier system. The proof involves new bounds on the singular values of restricted Fourier matrices, obtained in part from old techniques in complex analysis."}
{"original_text": "The motivation for this paper is to apply Bayesian structure learning using Model Averaging in large-scale networks. Currently, Bayesian model averaging algorithm is applicable to networks with only tens of variables, restrained by its super-exponential complexity. We present a novel framework, called LSBN (Large-Scale Bayesian Network), making it possible to handle networks with infinite size by following the principle of divide-and-conquer. The method of LSBN comprises three steps. In general, LSBN first performs the partition by using a second-order partition strategy, which achieves more robust results. LSBN conducts sampling and structure learning within each overlapping community after the community is isolated from other variables by Markov Blanket. Finally LSBN employs an efficient algorithm, to merge structures of overlapping communities into a whole. In comparison with other four state-of-art large-scale network structure learning algorithms such as ARACNE, PC, Greedy Search and MMHC, LSBN shows comparable results in five common benchmark datasets, evaluated by precision, recall and f-score. What's more, LSBN makes it possible to learn large-scale Bayesian structure by Model Averaging which used to be intractable. In summary, LSBN provides an scalable and parallel framework for the reconstruction of network structures. Besides, the complete information of overlapping communities serves as the byproduct, which could be used to mine meaningful clusters in biological networks, such as protein-protein-interaction network or gene regulatory network, as well as in social network.", "label": "1", "is_selected": "0", "text": "The motivation for this paper is to apply Bayesian structure learning using Model Averaging in large-scale networks. Currently, Bayesian model averaging algorithm is applicable to networks with only tens of variables, restrained by its super-exponential complexity. We present a novel framework, called LSBN (Large-Scale Bayesian Network), making it possible to handle networks with infinite size by following the principle of divide-and-conquer. The method of LSBN comprises three steps. In general, LSBN first performs the partition by using a second-order partition strategy, which achieves more robust results. LSBN conducts sampling and structure learning within each overlapping community after the community is isolated from other variables by Markov Blanket. Finally LSBN employs an efficient algorithm, to merge structures of overlapping communities into a whole. In comparison with other four state-of-art large-scale network structure learning algorithms such as ARACNE, PC, Greedy Search and MMHC, LSBN shows comparable results in five common benchmark datasets, evaluated by precision, recall and f-score. What's more, LSBN makes it possible to learn large-scale Bayesian structure by Model Averaging which used to be intractable. In summary, LSBN provides an scalable and parallel framework for the reconstruction of network structures. Besides, the complete information of overlapping communities serves as the byproduct, which could be used to mine meaningful clusters in biological networks, such as protein-protein-interaction network or gene regulatory network, as well as in social network."}
{"original_text": "For mobile robots navigating on sidewalks, it is essential to be able to safely cross street intersections. Most existing approaches rely on the recognition of the traffic light signal to cross a street. However, the traffic light recognition is prone to various failures such as the recognition of a traffic light at a place where it is not located. In this paper, we present a novel approach to detect the traffic light signal and extract the traffic light phase by a small monocular camera. In contrast to most previous approaches that detect the traffic light based on the signal itself, our approach first detects the location of the signal and then estimates the traffic light phase based on the location. The proposed approach can significantly improve the reliability of the traffic light detection and is also robust to the variable traffic light design as well as the impact of environmental conditions. We evaluate the performance of the proposed approach using a monocular camera on real-world data from two cities in Germany and Austria. The evaluation results show that the proposed approach works reliably under various scenarios and outperforms the state-of-the-art traffic light detection approach. This work was supported by the Austrian Research Promotion Agency (FFG) under the contracts KIRAS 844908 and ICT of the Future (NESSI).", "label": "0", "is_selected": "1", "text": "In this paper, we present a novel approach to detect the traffic light signal to cross a street by a small monocular camera and evaluate its performance on real-world data from two cities in Germany and Austria."}
{"original_text": "It is a challenge to specify unambiguous distance (UD) in a phase-based ranging system with hopping frequencies (PRSHF). In this letter, we propose to characterize the UD in a PRSHF by the probability that it takes on its maximum value. We obtain a very simple and elegant expression of the probability with growth estimation techniques from analytic number theory. It is revealed that the UD in a PRSHF usually takes on the maximum value with as few as 10 frequencies in measurement, almost independent of the specific distribution of available bandwidth.", "label": "1", "is_selected": "0", "text": "It is a challenge to specify unambiguous distance (UD) in a phase-based ranging system with hopping frequencies (PRSHF). In this letter, we propose to characterize the UD in a PRSHF by the probability that it takes on its maximum value. We obtain a very simple and elegant expression of the probability with growth estimation techniques from analytic number theory. It is revealed that the UD in a PRSHF usually takes on the maximum value with as few as 10 frequencies in measurement, almost independent of the specific distribution of available bandwidth."}
{"original_text": "The kinetic battery model is a popular model of the dynamic behavior of a conventional battery, useful to predict or optimize the time until battery depletion. The model however lacks certain obvious aspects of batteries in-the-wild, especially with respect to (i) the effects of random influences and (ii) the behavior when charging up to capacity bounds. This paper considers the kinetic battery model with bounded capacity in the context of piecewise constant yet random charging and discharging. The resulting model enables the time-dependent evaluation of the risk of battery depletion. This is exemplified in a power dependability study of a nano satellite mission.", "label": "1", "is_selected": "0", "text": "The kinetic battery model is a popular model of the dynamic behavior of a conventional battery, useful to predict or optimize the time until battery depletion. The model however lacks certain obvious aspects of batteries in-the-wild, especially with respect to (i) the effects of random influences and (ii) the behavior when charging up to capacity bounds. This paper considers the kinetic battery model with bounded capacity in the context of piecewise constant yet random charging and discharging. The resulting model enables the time-dependent evaluation of the risk of battery depletion. This is exemplified in a power dependability study of a nano satellite mission."}
{"original_text": "Partially answering a question of Paul Seymour, we obtain a sufficient eigenvalue condition for the existence of k edge-disjoint spanning trees in a regular graph, when k {2, 3 }. More precisely, we show that if the second largest eigenvalue of a d -regular graph G is less than - d - 2 k 1 d 1, then G contains at least k edge-disjoint spanning trees, when k {2, 3 }. We construct examples of graphs that show our bounds are essentially best possible. We conjecture that the above statement is true for any k d 2.", "label": "1", "is_selected": "0", "text": "Partially answering a question of Paul Seymour, we obtain a sufficient eigenvalue condition for the existence of k edge-disjoint spanning trees in a regular graph, when k {2, 3 }. More precisely, we show that if the second largest eigenvalue of a d -regular graph G is less than - d - 2 k 1 d 1, then G contains at least k edge-disjoint spanning trees, when k {2, 3 }. We construct examples of graphs that show our bounds are essentially best possible. We conjecture that the above statement is true for any k d 2."}
{"original_text": "This paper presents an adaptive randomized algorithm for computing the butterfly factorization of a x m n matrix with m n provided that both the matrix and its transpose can be rapidly applied to arbitrary vectors. The resulting factorization is composed of O (log n) sparse factors, each containing O (n) nonzero entries. The factorization can be attained using O (n 3 2 log n) computation and O (n log n) memory resources. The proposed algorithm applies to matrices with strong and weak admissibility conditions arising from surface integral equation solvers with a rigorous error bound, and is implemented in parallel.", "label": "1", "is_selected": "0", "text": "This paper presents an adaptive randomized algorithm for computing the butterfly factorization of a x m n matrix with m n provided that both the matrix and its transpose can be rapidly applied to arbitrary vectors. The resulting factorization is composed of O (log n) sparse factors, each containing O (n) nonzero entries. The factorization can be attained using O (n 3 2 log n) computation and O (n log n) memory resources. The proposed algorithm applies to matrices with strong and weak admissibility conditions arising from surface integral equation solvers with a rigorous error bound, and is implemented in parallel."}
{"original_text": "Most studies on optical wireless communications (OWCs) have neglected the effect of random orientation in their performance analysis due to the lack of a proper model for the random orientation. Our recent empirical-based research illustrates that the random orientation follows a Laplace distribution for static user equipment (UE). In this paper, we analyze the device orientation and assess its importance on system performance. The probability of establishing a line-of-sight link is investigated and the probability density function (PDF) of signal-to-noise ratio (SNR) for a randomly-oriented device is derived. By means of the PDF of SNR, the bit-error ratio (BER) of DC biased optical orthogonal frequency division multiplexing (DCO-OFDM) in additive white Gaussian noise (AWGN) channels is evaluated. A closed form approximation for the BER of UE with random orientation is presented which shows a good match with Monte-Carlo simulation results.", "label": "1", "is_selected": "0", "text": "Most studies on optical wireless communications (OWCs) have neglected the effect of random orientation in their performance analysis due to the lack of a proper model for the random orientation. Our recent empirical-based research illustrates that the random orientation follows a Laplace distribution for static user equipment (UE). In this paper, we analyze the device orientation and assess its importance on system performance. The probability of establishing a line-of-sight link is investigated and the probability density function (PDF) of signal-to-noise ratio (SNR) for a randomly-oriented device is derived. By means of the PDF of SNR, the bit-error ratio (BER) of DC biased optical orthogonal frequency division multiplexing (DCO-OFDM) in additive white Gaussian noise (AWGN) channels is evaluated. A closed form approximation for the BER of UE with random orientation is presented which shows a good match with Monte-Carlo simulation results."}
{"original_text": "We present an evaluation of several representative sampling-based and optimization-based motion planners, and then introduce an integrated motion planning system which incorporates recent advances in trajectory optimization into a sparse roadmap framework. Through experiments in 4 common application scenarios with 5000 test cases each, we show that optimization-based or sampling-based planners alone are not effective for realistic problems where fast planning times are required. To the best of our knowledge, this is the first work that presents such a systematic and comprehensive evaluation of state-of-the-art motion planners, which are based on a significant amount of experiments. We then combine different stand-alone planners with trajectory optimization. The results show that the combination of our sparse roadmap and trajectory optimization provides superior performance over other standard sampling-based planners' combinations. By using a multi-query roadmap instead of generating completely new trajectories for each planning problem, our approach allows for extensions such as persistent control policy information associated with a trajectory across planning problems. Also, the sub-optimality resulting from the sparsity of roadmap, as well as the unexpected disturbances from the environment, can both be overcome by the real-time trajectory optimization process.", "label": "1", "is_selected": "0", "text": "We present an evaluation of several representative sampling-based and optimization-based motion planners, and then introduce an integrated motion planning system which incorporates recent advances in trajectory optimization into a sparse roadmap framework. Through experiments in 4 common application scenarios with 5000 test cases each, we show that optimization-based or sampling-based planners alone are not effective for realistic problems where fast planning times are required. To the best of our knowledge, this is the first work that presents such a systematic and comprehensive evaluation of state-of-the-art motion planners, which are based on a significant amount of experiments. We then combine different stand-alone planners with trajectory optimization. The results show that the combination of our sparse roadmap and trajectory optimization provides superior performance over other standard sampling-based planners' combinations. By using a multi-query roadmap instead of generating completely new trajectories for each planning problem, our approach allows for extensions such as persistent control policy information associated with a trajectory across planning problems. Also, the sub-optimality resulting from the sparsity of roadmap, as well as the unexpected disturbances from the environment, can both be overcome by the real-time trajectory optimization process."}
{"original_text": "High-performance implementations of graph algorithms are challenging toimplement on new parallel hardware such as GPUs because of three challenges: (1) the difficulty of coming up with graph building blocks, (2) the need to carefully tune these graph building blocks to the new hardware, and (3) the difficulty of finding the right combination of graph building blocks to implement the graph algorithms. We present GraphR, a new graph framework that addresses these challenges by defining a graph library that captures the common graph abstractions, designing a set of graph building blocks, and implementing them to run efficiently on modern parallel hardware, including GPUs, multicore processors and distributed systems. We evaluate our graph library on several GPUs and show that it is significantly faster and more scalable than the current state-of-the-art libraries. We also show that it is easy to express graph algorithms using our graph library and that these algorithms are more efficient than their CPU counterparts.", "label": "0", "is_selected": "1", "text": "Graph algorithms are used in a wide range of scientific and engineering applications, including high-performance computing, machine learning, graph annealing, and many other fields."}
{"original_text": "Current neural network-based conversational models lack diversity and generate boring responses to open-ended utterances. Priors such as persona, emotion, or topic provide additional information to dialog models to aid response generation. In this paper, we develop a neural network model, which we call a topic-aware conversational model (TAC), that incorporates such prior information from a topic model for a more engaging dialog. We evaluate TAC on a public dataset containing social media conversations. Our results show that TAC outperforms baseline models in two aspects: (1) it generates responses that are more diverse and (2) it has a lower perplexity score.", "label": "0", "is_selected": "0", "text": "Current neural network-based conversational models lack diversity and generate boring responses to open-ended utterances. Priors such as persona, emotion, or topic provide additional information to dialog models to aid response generation. In this paper, we develop a neural network model, which we call a topic-aware conversational model (TAC), that incorporates such prior information from a topic model for a more engaging dialog. We evaluate TAC on a public dataset containing social media conversations. Our results show that TAC outperforms baseline models in two aspects: (1) it generates responses that are more diverse and (2) it has a lower perplexity score."}
{"original_text": "Head pose estimation is an important pre-processing step in many pattern recognition and computer vision systems such as face recognition. Since the performance of the face recognition systems is greatly affected by the head pose variations, the pose estimation plays a vital role in those systems. A head pose estimation algorithm is presented in this paper, which is an extension of the modified discrete cosine transform (MDCT) based head pose estimation algorithm. The method is based on the three-dimensional (3-D) model of head, which is built from 2-D images. The algorithm locates the head position and orientation in 3-D space, using the normalized moments of the 3-D head model. The proposed method is evaluated on the Head Poses in Real World Images (HPRW) dataset, which consists of 2000 images, and the accuracy of the algorithm is compared with other state-of-the-art methods. The proposed algorithm is found to be more robust and accurate than the existing algorithms.", "label": "0", "is_selected": "1", "text": "A new head pose estimation algorithm is presented in this paper, which is an extension of the modified discrete cosine transform (MDCT) based head poses estimation algorithm."}
{"original_text": "Retrieving videos of a particular person with face image as query via hashing technique has many important applications. While face images are typically represented as vectors in Euclidean space, characterizing the similarity between two face images in this space requires a lot of computation. Hashing technique has been proposed to efficiently retrieve videos based on person identity and it is the core of this paper. The generality of the hashing methods which can be applied in retrieving videos of a particular person with face image as query is discussed in this paper. A face image is represented as a set of local features which are extracted via two feature extraction methods: geometric features, and Local Binary Pattern (LBP). In the proposed method, these features are used as a query to retrieve the videos which contain the same person. To make the face features more discriminative, a random projection is proposed. In this method, a similarity matrix is constructed which compares the feature of the query with the features of the videos in the database. The proposed hashing method is tested on three different databases and the results are presented. The results of this work are encouraging and give some suggestions for the future work in this domain.", "label": "0", "is_selected": "0", "text": "Retrieving videos of a particular person with face image as query via hashing technique has many important applications. While face images are typically represented as vectors in Euclidean space, characterizing the similarity between two face images in this space requires a lot of computation. Hashing technique has been proposed to efficiently retrieve videos based on person identity and it is the core of this paper. The generality of the hashing methods which can be applied in retrieving videos of a particular person with face image as query is discussed in this paper. A face image is represented as a set of local features which are extracted via two feature extraction methods: geometric features, and Local Binary Pattern (LBP). In the proposed method, these features are used as a query to retrieve the videos which contain the same person. To make the face features more discriminative, a random projection is proposed. In this method, a similarity matrix is constructed which compares the feature of the query with the features of the videos in the database. The proposed hashing method is tested on three different databases and the results are presented. The results of this work are encouraging and give some suggestions for the future work in this domain."}
{"original_text": "In this contribution we generalize the classical Fourier Mellin transform, which transforms functions f representing, e.g., a gray level image defined over a compact set of R 2. The quaternionic Fourier Mellin transform (QFMT) applies to functions: f - R 2 H, for which f is summable over x R S 1 under the measure d th d r r. R is the multiplicative group of positive and non-zero real numbers. We investigate the properties of the QFMT similar to the investigation of the quaternionic Fourier Transform (QFT) in.", "label": "1", "is_selected": "0", "text": "In this contribution we generalize the classical Fourier Mellin transform, which transforms functions f representing, e.g., a gray level image defined over a compact set of R 2. The quaternionic Fourier Mellin transform (QFMT) applies to functions: f - R 2 H, for which f is summable over x R S 1 under the measure d th d r r. R is the multiplicative group of positive and non-zero real numbers. We investigate the properties of the QFMT similar to the investigation of the quaternionic Fourier Transform (QFT) in."}
{"original_text": "Distinction among nearby poses and among symmetries of an object is challenging. In this paper, we propose a unified, group-theoretic approach to tackle both. Different from existing works which directly optimize a pose-symmetry set, we compute an optimal pose-symmetry set for the first time. We decompose the problem into two subproblems, pose set recovery and symmetry set recovery, by exploiting the group structure of poses and symmetries. Our formulation is simple, yet can provide optimal solutions in polynomial time. We show that the objective function is convex, and thus the optimal solutions can be computed by an iterative method. We demonstrate the effectiveness and efficiency of the proposed method on synthetic and real-world datasets. T. Zhou, K. Kedem, and S. Sclaroff. Optimal pose-symmetry sets for rigid objects. In Proc. of 3DV Conference, Orlando, FL, 2015.", "label": "0", "is_selected": "1", "text": "The optimal pose-symmetry set for a rigid object is one of the most important problems in the field of finite element theory and computer science, as well as in many other areas of mathematics."}
{"original_text": "Although artificial neural networks have shown great promise in applications including computer vision and speech recognition, there remains considerable practical and theoretical difficulty in optimizing their parameters. The seemingly unreasonable success of gradient descent methods in minimizing these non-convex functions remains poorly understood. In this work we offer some theoretical guarantees for networks with piecewise affine activation functions, which have in recent years become the norm. We prove three main results. Firstly, that the network is piecewise convex as a function of the input data. Secondly, that the network, considered as a function of the parameters in a single layer, all others held constant, is again piecewise convex. Finally, that the network as a function of all its parameters is piecewise multi-convex, a generalization of biconvexity. From here we characterize the local minima and stationary points of the training objective, showing that they minimize certain subsets of the parameter space. We then analyze the performance of two optimization algorithms on multi-convex problems: gradient descent, and a method which repeatedly solves a number of convex sub-problems. We prove necessary convergence conditions for the first algorithm and both necessary and sufficient conditions for the second, after introducing regularization to the objective. Finally, we remark on the remaining difficulty of the global optimization problem. Under the squared error objective, we show that by varying the training data, a single rectifier neuron admits local minima arbitrarily far apart, both in objective value and parameter space.", "label": "1", "is_selected": "0", "text": "Although artificial neural networks have shown great promise in applications including computer vision and speech recognition, there remains considerable practical and theoretical difficulty in optimizing their parameters. The seemingly unreasonable success of gradient descent methods in minimizing these non-convex functions remains poorly understood. In this work we offer some theoretical guarantees for networks with piecewise affine activation functions, which have in recent years become the norm. We prove three main results. Firstly, that the network is piecewise convex as a function of the input data. Secondly, that the network, considered as a function of the parameters in a single layer, all others held constant, is again piecewise convex. Finally, that the network as a function of all its parameters is piecewise multi-convex, a generalization of biconvexity. From here we characterize the local minima and stationary points of the training objective, showing that they minimize certain subsets of the parameter space. We then analyze the performance of two optimization algorithms on multi-convex problems: gradient descent, and a method which repeatedly solves a number of convex sub-problems. We prove necessary convergence conditions for the first algorithm and both necessary and sufficient conditions for the second, after introducing regularization to the objective. Finally, we remark on the remaining difficulty of the global optimization problem. Under the squared error objective, we show that by varying the training data, a single rectifier neuron admits local minima arbitrarily far apart, both in objective value and parameter space."}
{"original_text": "Why and why-not provenance have been studied extensively in recent years. However, why-not provenance and - to a lesser degree - why provenance, can be very large resulting in severe scalability problems. This paper presents a compact representation of why-not provenance for large and nested why-not provenance graphs and a method of efficient retrieval of why-not provenance. This representation is based on the concept of a goal tree. To support efficient retrieval, a novel hashing method is presented. Results from our experiments show that the proposed method is effective in decreasing the size of why-not provenance, and can significantly reduce the retrieval time. 2012 Springer-Verlag.", "label": "0", "is_selected": "1", "text": "A novel method for efficient retrieval of why-not provenance has been proposed and tested in the context of largeProvenance research at the University of California, Berkeley."}
{"original_text": "We present a bandit algorithm, SAO (Stochastic and Adversarial Optimal), whose regret is, essentially, optimal both for adversarial rewards and for stochastic rewards. Specifically, SAO combines the O (n) worst-case regret of Exp3 [, ] for adversarial rewards and the (poly) logarithmic regret of UCB1 [, ] for stochastic rewards. Adversarial rewards and stochastic rewards are the two main settings in the literature on (non-Bayesian) multi-armed bandits. Prior work on multi-armed bandits treats them separately, and does not attempt to jointly optimize for both. Our result falls into a general theme of achieving good worst-case performance while also taking advantage of \"nice\" problem instances, an important issue in the design of algorithms with partially known inputs.", "label": "1", "is_selected": "0", "text": "We present a bandit algorithm, SAO (Stochastic and Adversarial Optimal), whose regret is, essentially, optimal both for adversarial rewards and for stochastic rewards. Specifically, SAO combines the O (n) worst-case regret of Exp3 [, ] for adversarial rewards and the (poly) logarithmic regret of UCB1 [, ] for stochastic rewards. Adversarial rewards and stochastic rewards are the two main settings in the literature on (non-Bayesian) multi-armed bandits. Prior work on multi-armed bandits treats them separately, and does not attempt to jointly optimize for both. Our result falls into a general theme of achieving good worst-case performance while also taking advantage of \"nice\" problem instances, an important issue in the design of algorithms with partially known inputs."}
{"original_text": "Disjunctive Answer Set Programming (ASP) is a powerful declarative programming paradigm whose main decision problems are located on the second level of the polynomial hierarchy. Identifying tractable fragments and developing efficient algorithms for such fragments are thus important objectives in order to complement the sophisticated ASP systems available to date. Hard problems can become tractable if some problem parameter is bounded by a fixed constant; such problems are then called fixed-parameter tractable (FPT). While several FPT results for ASP exist, parameters that relate to directed or signed graphs representing the program at hand have been neglected so far. In this paper, we first give some negative observations showing that directed width measures on the dependency graph of a program do not lead to FPT results. We then consider the graph parameter of signed clique-width and present a novel dynamic programming algorithm that is FPT w.r.t. this parameter. Clique-width is more general than the well-known treewidth, and, to the best of our knowledge, ours is the first FPT algorithm for bounded clique-width for reasoning problems beyond SAT.", "label": "1", "is_selected": "0", "text": "Disjunctive Answer Set Programming (ASP) is a powerful declarative programming paradigm whose main decision problems are located on the second level of the polynomial hierarchy. Identifying tractable fragments and developing efficient algorithms for such fragments are thus important objectives in order to complement the sophisticated ASP systems available to date. Hard problems can become tractable if some problem parameter is bounded by a fixed constant; such problems are then called fixed-parameter tractable (FPT). While several FPT results for ASP exist, parameters that relate to directed or signed graphs representing the program at hand have been neglected so far. In this paper, we first give some negative observations showing that directed width measures on the dependency graph of a program do not lead to FPT results. We then consider the graph parameter of signed clique-width and present a novel dynamic programming algorithm that is FPT w.r.t. this parameter. Clique-width is more general than the well-known treewidth, and, to the best of our knowledge, ours is the first FPT algorithm for bounded clique-width for reasoning problems beyond SAT."}
{"original_text": "The concept of nestedness, in particular for ecological and economical networks, has been introduced as a structural characteristic of real interacting systems. We suggest that the nestedness is in fact another way to express a mesoscale network property called the core-periphery structure. With real ecological mutualistic networks and synthetic model networks, we reveal the strong correlation between the nestedness and core-periphery-ness (likeness to the core-periphery structure), by defining the network-level measures for nestedness and core-periphery-ness in the case of weighted and bipartite networks. However, at the same time, via more sophisticated null-model analysis, we also discover that the degree (the number of connected neighbors of a node) distribution poses quite severe restrictions on the possible nestedness and core-periphery-ness parameter space. Therefore, there must exist structurally interwoven properties in more fundamental levels of network formation, behind this seemingly obvious relation between nestedness and core-periphery structures.", "label": "1", "is_selected": "0", "text": "The concept of nestedness, in particular for ecological and economical networks, has been introduced as a structural characteristic of real interacting systems. We suggest that the nestedness is in fact another way to express a mesoscale network property called the core-periphery structure. With real ecological mutualistic networks and synthetic model networks, we reveal the strong correlation between the nestedness and core-periphery-ness (likeness to the core-periphery structure), by defining the network-level measures for nestedness and core-periphery-ness in the case of weighted and bipartite networks. However, at the same time, via more sophisticated null-model analysis, we also discover that the degree (the number of connected neighbors of a node) distribution poses quite severe restrictions on the possible nestedness and core-periphery-ness parameter space. Therefore, there must exist structurally interwoven properties in more fundamental levels of network formation, behind this seemingly obvious relation between nestedness and core-periphery structures."}
{"original_text": "We consider the problem of decomposing a higher-order tensor with binary entries. Such data problems arise frequently in applications such as neuroimaging, recommendation system, topic modeling, and sensor network localization. We develop an algorithm that decomposes a binary tensor into a product of binary tensors in a low-rank sense, and we present a theoretical analysis that guarantees the recovery of the tensor product under several common sampling models. We also discuss a variety of applications and present results on real and synthetic datasets. Our algorithm is computationally efficient, and scales to large tensors with dimension as high as 10. This work was supported by the NSF under grants DMS-09-45331 and DMS-12-16431, the Simons Foundation under grant 352214, and the Office of Naval Research under grant N00014-14-1-0165.", "label": "0", "is_selected": "0", "text": "We consider the problem of decomposing a higher-order tensor with binary entries. Such data problems arise frequently in applications such as neuroimaging, recommendation system, topic modeling, and sensor network localization. We develop an algorithm that decomposes a binary tensor into a product of binary tensors in a low-rank sense, and we present a theoretical analysis that guarantees the recovery of the tensor product under several common sampling models. We also discuss a variety of applications and present results on real and synthetic datasets. Our algorithm is computationally efficient, and scales to large tensors with dimension as high as 10. This work was supported by the NSF under grants DMS-09-45331 and DMS-12-16431, the Simons Foundation under grant 352214, and the Office of Naval Research under grant N00014-14-1-0165."}
{"original_text": "Most theoretical frameworks that focus on data errors and inconsistencies follow logic-based reasoning. Yet, practical data cleaning tools need to incorporate statistical reasoning to be effective in real-world data cleaning tasks. For example, if one knows the average value for a data field in a database, this can be used to identify spurious outliers. We provide a statistical-based framework for identifying data errors and inconsistencies, called the IMES framework. The IMES framework can be applied to any data field. First, we compute a data field's Invariant Measure (IM). The IM is a measure of the data field's invariance under some given conditions, such as the age of a patient in a hospital. The IM can be any statistical measure, such as a mean, median, variance, etc. Second, we compute the Expected Measure (EM) of the data field. The EM is an expected value of the data field given the underlying distribution of the data field. Third, we compute the Suspicious Measure (SM) of the data field. The SM is a measure that can be used to identify suspicious data values. The SM can be any statistical measure, such as the z-score, standard deviation from the mean, etc. We apply the IMES framework to real-world data sets to show its effectiveness. The full version of this paper is available in the proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2010).", "label": "0", "is_selected": "1", "text": "In this paper, we present a statistical-based framework for identifying data errors and inconsistencies in a database, which can be used to clean data in real-world databases."}
{"original_text": "This paper proposes a robust adversarial reinforcement learning (RARL) -based multi-access point (AP) coordination method that is robust even against unexpected decentralized operations of uncoordinated APs. Multi-AP coordination is a promising technique towards IEEE 802.11be, and there are studies that use RL for multi-AP coordination. Indeed, a simple RL-based multi-AP coordination method diminishes the collision probability among the APs; therefore, the method is a promising approach to improve time-resource efficiency. However, this method is vulnerable to frame transmissions of uncoordinated APs that are less aware of frame transmissions of other coordinated APs. To help the central agent experience even such unexpected frame transmissions, in addition to the central agent, the proposed method also competitively trains an adversarial AP that disturbs coordinated APs by causing frame collisions intensively. Besides, we propose to exploit a history of frame losses of a coordinated AP to promote reasonable competition between the central agent and adversarial AP. The simulation results indicate that the proposed method can avoid uncoordinated interference and thereby improve the minimum sum of the throughputs in the system compared to not considering the uncoordinated AP.", "label": "1", "is_selected": "0", "text": "This paper proposes a robust adversarial reinforcement learning (RARL) -based multi-access point (AP) coordination method that is robust even against unexpected decentralized operations of uncoordinated APs. Multi-AP coordination is a promising technique towards IEEE 802.11be, and there are studies that use RL for multi-AP coordination. Indeed, a simple RL-based multi-AP coordination method diminishes the collision probability among the APs; therefore, the method is a promising approach to improve time-resource efficiency. However, this method is vulnerable to frame transmissions of uncoordinated APs that are less aware of frame transmissions of other coordinated APs. To help the central agent experience even such unexpected frame transmissions, in addition to the central agent, the proposed method also competitively trains an adversarial AP that disturbs coordinated APs by causing frame collisions intensively. Besides, we propose to exploit a history of frame losses of a coordinated AP to promote reasonable competition between the central agent and adversarial AP. The simulation results indicate that the proposed method can avoid uncoordinated interference and thereby improve the minimum sum of the throughputs in the system compared to not considering the uncoordinated AP."}
{"original_text": "Accurate rainfall forecasting is critical because it has a great impact on people's social and economic activities. Recent trends on various literatures shows that Deep Learning (Neural Network) is a powerful method to deal with rainfall forecasting problem. However, deep learning requires large dataset. In this paper, we present a simple, yet effective and efficient method to transform rainfall intensity to rainfall image data. The transformed data are then used to train Deep Convolutional Neural Network (CNN) in forecasting rainfall data. Our proposed method achieves significantly better results compared to benchmarking methods. Our proposed method is particularly suited for short-term rainfall forecasting and achieves the best performance for different prediction time-steps. Rainfall forecasting is a difficult problem because it has a great impact on people's social and economic activities. Many countries still experience difficulties with inaccurate rainfall forecasting. Inaccurate rainfall forecasting may lead to flood and destruction in the area. Inaccurate rainfall forecasting may also lead to damage to crops and other agricultural activities. Inaccurate rainfall forecasting may also affect the activities of daily life in a region. For example, if a place is forecasted to experience heavy rainfall, then the local government may decide to delay the opening of schools or may decide to take other precautionary measures. This may affect the daily activities of the people in the area. Inaccurate rainfall forecasting may also affect the activities of the airport and other infrastructures. Inaccurate rainfall forecasting may also affect the tourism industry. For example, tourists may not visit a place that is forecasted to experience heavy rainfall. This will affect the tourism industry and economy of the area. Thus, it is important to have accurate rainfall forecasting models that can be used to predict rainfall events. In this paper, we propose a method to transform rainfall intensity data to rainfall image data. We then use the transformed data to train deep convolutional neural network (CNN) in forecasting rainfall data. Our proposed method is particularly suited for short-term rainfall forecasting. Our experiments show that our proposed method outperforms benchmarking methods.", "label": "0", "is_selected": "0", "text": "Accurate rainfall forecasting is critical because it has a great impact on people's social and economic activities. Recent trends on various literatures shows that Deep Learning (Neural Network) is a powerful method to deal with rainfall forecasting problem. However, deep learning requires large dataset. In this paper, we present a simple, yet effective and efficient method to transform rainfall intensity to rainfall image data. The transformed data are then used to train Deep Convolutional Neural Network (CNN) in forecasting rainfall data. Our proposed method achieves significantly better results compared to benchmarking methods. Our proposed method is particularly suited for short-term rainfall forecasting and achieves the best performance for different prediction time-steps. Rainfall forecasting is a difficult problem because it has a great impact on people's social and economic activities. Many countries still experience difficulties with inaccurate rainfall forecasting. Inaccurate rainfall forecasting may lead to flood and destruction in the area. Inaccurate rainfall forecasting may also lead to damage to crops and other agricultural activities. Inaccurate rainfall forecasting may also affect the activities of daily life in a region. For example, if a place is forecasted to experience heavy rainfall, then the local government may decide to delay the opening of schools or may decide to take other precautionary measures. This may affect the daily activities of the people in the area. Inaccurate rainfall forecasting may also affect the activities of the airport and other infrastructures. Inaccurate rainfall forecasting may also affect the tourism industry. For example, tourists may not visit a place that is forecasted to experience heavy rainfall. This will affect the tourism industry and economy of the area. Thus, it is important to have accurate rainfall forecasting models that can be used to predict rainfall events. In this paper, we propose a method to transform rainfall intensity data to rainfall image data. We then use the transformed data to train deep convolutional neural network (CNN) in forecasting rainfall data. Our proposed method is particularly suited for short-term rainfall forecasting. Our experiments show that our proposed method outperforms benchmarking methods."}
{"original_text": "The rapidly growing field of network analytics requires data sets for use in evaluation. Real world data often lack truth and simulated data lack narrative fidelity or statistical generality. This work presents a toolkit for creating realistic synthetic network data. We developed a model for generating synthetic social networks and analyzed its properties. The results indicate that the model can generate networks with similar statistical properties as real world networks. We also developed a tool for generating realistic synthetic network data. Finally, we applied the methodology to real world data by simulating the attack behavior of a network intrusion attack. Synthetic Social Network Data: An Evaluation of the Small-world Network Model The rapidly growing field of network analytics requires data sets for use in evaluation. Real world data often lack truth and simulated data lack narrative fidelity or statistical generality. This work presents a toolkit for creating realistic synthetic network data. We developed a model for generating synthetic social networks and analyzed its properties. The results indicate that the model can generate networks with similar statistical properties as real world networks. We also developed a tool for generating realistic synthetic network data. Finally, we applied the methodology to real world data by simulating the attack behavior of a network intrusion attack. 1 ...igh-level network models and do not attempt to reproduce the complexities of a real network. The small-world model is a graph generator that produces networks with a small-world property. The model 3 is based on a Barabasi-Albert preferential attachment graph generator [4]. In the small-world model, edges are added in a preferential attachment fashion but the degree of the node that receives ... G. C. Brown, T. A. M. Mapp, and A. L. Barabasi by G. C. Brown, T. A. M. Mapp, A. L. Barabasi - Proceedings of the National Academy of Sciences , 2004 ...\" Identifying the organizational principles underlying the topology of complex networks is a fundamental problem in the study of complex systems. In this paper, we examine the organizational properties of the protein-protein interaction network of yeast. We show that this network possesses a n ...\" Identifying the organizational principles underlying the topology of complex networks is a fundamental problem in the study of complex systems. In this paper, we examine the organizational properties of the protein-protein interaction network of yeast. We show that this network possesses a nearly optimal degree distribution, which leads to the most efficient flow of information among its nodes. We also show that this network possesses a high clustering coefficient, indicating that related nodes are clustered together. Both these features suggest that the protein-protein interaction network of yeast has evolved to facilitate efficient communication between its constituents. 1. INTRODUCTION Most complex systems are made up of many distinct parts that interact with one another to form a network of connections. The evolution of these networks is therefore fundamental to our understanding of complex systems. The development of a comprehensive theory for network evolution, however, requires the identification of the organizational principles underlying network topologies. This problem has been studied most extensively in random and scale-free networks. A random network, such as a Bernoulli random graph, contains no correlations between the nodes. A scale-free network, on the other hand, is characterized by a power-law degree distribution, which has been identified in a wide variety of systems, ranging from metabolic networks to computer networks [1-12]. More recently, ...sults in a number of distinct properties, including a power-law degree distribution, a high clustering coefficient, and small average path lengths [13-15]. A number of models, such as the Barabasi-Albert 16, 17 model, have been proposed to capture the growth and evolution of scale-free networks. These models focus on the relationship between the degree of a node and the number of edges it is attached to. I... by Yang-yu Liu, Jon Kleinberg - In Proc. of ACM-SIAM Symposium on Discrete Algorithms , 2004 ...\" The barbell graph is a bipartite graph with vertices partitioned into two equal-sized sets, and consisting of two long chains of edges connecting the two sets together, with the chains being otherwise disjoint. The barbell graph has been used in models of the World Wide Web and other networked sy ...\" The barbell graph is a bipartite graph with vertices partitioned into two equal-sized sets, and consisting of two long chains of edges connecting the two sets together, with the chains being", "label": "0", "is_selected": "0", "text": "The rapidly growing field of network analytics requires data sets for use in evaluation. Real world data often lack truth and simulated data lack narrative fidelity or statistical generality. This work presents a toolkit for creating realistic synthetic network data. We developed a model for generating synthetic social networks and analyzed its properties. The results indicate that the model can generate networks with similar statistical properties as real world networks. We also developed a tool for generating realistic synthetic network data. Finally, we applied the methodology to real world data by simulating the attack behavior of a network intrusion attack. Synthetic Social Network Data: An Evaluation of the Small-world Network Model The rapidly growing field of network analytics requires data sets for use in evaluation. Real world data often lack truth and simulated data lack narrative fidelity or statistical generality. This work presents a toolkit for creating realistic synthetic network data. We developed a model for generating synthetic social networks and analyzed its properties. The results indicate that the model can generate networks with similar statistical properties as real world networks. We also developed a tool for generating realistic synthetic network data. Finally, we applied the methodology to real world data by simulating the attack behavior of a network intrusion attack. 1 ...igh-level network models and do not attempt to reproduce the complexities of a real network. The small-world model is a graph generator that produces networks with a small-world property. The model 3 is based on a Barabasi-Albert preferential attachment graph generator [4]. In the small-world model, edges are added in a preferential attachment fashion but the degree of the node that receives ... G. C. Brown, T. A. M. Mapp, and A. L. Barabasi by G. C. Brown, T. A. M. Mapp, A. L. Barabasi - Proceedings of the National Academy of Sciences , 2004 ...\" Identifying the organizational principles underlying the topology of complex networks is a fundamental problem in the study of complex systems. In this paper, we examine the organizational properties of the protein-protein interaction network of yeast. We show that this network possesses a n ...\" Identifying the organizational principles underlying the topology of complex networks is a fundamental problem in the study of complex systems. In this paper, we examine the organizational properties of the protein-protein interaction network of yeast. We show that this network possesses a nearly optimal degree distribution, which leads to the most efficient flow of information among its nodes. We also show that this network possesses a high clustering coefficient, indicating that related nodes are clustered together. Both these features suggest that the protein-protein interaction network of yeast has evolved to facilitate efficient communication between its constituents. 1. INTRODUCTION Most complex systems are made up of many distinct parts that interact with one another to form a network of connections. The evolution of these networks is therefore fundamental to our understanding of complex systems. The development of a comprehensive theory for network evolution, however, requires the identification of the organizational principles underlying network topologies. This problem has been studied most extensively in random and scale-free networks. A random network, such as a Bernoulli random graph, contains no correlations between the nodes. A scale-free network, on the other hand, is characterized by a power-law degree distribution, which has been identified in a wide variety of systems, ranging from metabolic networks to computer networks [1-12]. More recently, ...sults in a number of distinct properties, including a power-law degree distribution, a high clustering coefficient, and small average path lengths [13-15]. A number of models, such as the Barabasi-Albert 16, 17 model, have been proposed to capture the growth and evolution of scale-free networks. These models focus on the relationship between the degree of a node and the number of edges it is attached to. I... by Yang-yu Liu, Jon Kleinberg - In Proc. of ACM-SIAM Symposium on Discrete Algorithms , 2004 ...\" The barbell graph is a bipartite graph with vertices partitioned into two equal-sized sets, and consisting of two long chains of edges connecting the two sets together, with the chains being otherwise disjoint. The barbell graph has been used in models of the World Wide Web and other networked sy ...\" The barbell graph is a bipartite graph with vertices partitioned into two equal-sized sets, and consisting of two long chains of edges connecting the two sets together, with the chains being"}
{"original_text": "Many modern applications produce massive streams of data series that need to be analyzed, requiring efficient similarity search operations. However, the state-of-the-art data series indexes that are used for this purpose do not scale well for massive datasets in terms of performance, or storage costs. We pinpoint the problem to the fact that existing summarizations of data series used for indexing cannot be sorted while keeping similar data series close to each other in the sorted order. To address this problem, we present Coconut, the first data series index based on sortable summarizations, and the first efficient solution for indexing and querying streaming series. The first innovation in Coconut is an inverted, sortable data series summarization that organizes data series based on a z-order curve, keeping similar series close to each other in the sorted order. As a result, Coconut is able to use bulk-loading and updating techniques that rely on sorting to quickly build and maintain a contiguous index using large sequential disk IOs. We then explore prefix-based and median-based splitting policies for bottom-up bulk-loading, showing that median-based splitting outperforms the state of the art, ensuring that all nodes are densely populated. Finally, we explore the impact of sortable summarizations on variable size window queries, showing that they can be supported in the presence of updates through efficient merging of temporal partitions. Overall, we show analytically and empirically that Coconut dominates the state-of-the-art data series indexes in terms of construction speed, query speed, and storage costs.", "label": "1", "is_selected": "0", "text": "Many modern applications produce massive streams of data series that need to be analyzed, requiring efficient similarity search operations. However, the state-of-the-art data series indexes that are used for this purpose do not scale well for massive datasets in terms of performance, or storage costs. We pinpoint the problem to the fact that existing summarizations of data series used for indexing cannot be sorted while keeping similar data series close to each other in the sorted order. To address this problem, we present Coconut, the first data series index based on sortable summarizations, and the first efficient solution for indexing and querying streaming series. The first innovation in Coconut is an inverted, sortable data series summarization that organizes data series based on a z-order curve, keeping similar series close to each other in the sorted order. As a result, Coconut is able to use bulk-loading and updating techniques that rely on sorting to quickly build and maintain a contiguous index using large sequential disk IOs. We then explore prefix-based and median-based splitting policies for bottom-up bulk-loading, showing that median-based splitting outperforms the state of the art, ensuring that all nodes are densely populated. Finally, we explore the impact of sortable summarizations on variable size window queries, showing that they can be supported in the presence of updates through efficient merging of temporal partitions. Overall, we show analytically and empirically that Coconut dominates the state-of-the-art data series indexes in terms of construction speed, query speed, and storage costs."}
{"original_text": "In this paper, the authors aim to combine the latest state of the art models in image recognition with the best publicly available satellite images to create a system for landslide risk mitigation. We focus first on landslide detection and further propose a similar system to be used for prediction. Such models are valuable as they could easily be scaled up to provide data for hazard evaluation, as satellite imagery becomes increasingly available. The goal is to use satellite images and correlated data to enrich the public repository of data and guide disaster relief efforts for locating precise areas where landslides have occurred. Different image augmentation methods are used to increase diversity in the chosen dataset and create more robust classification. The resulting outputs are then fed into variants of 3-D convolutional neural networks. A review of the current literature indicates there is no research using CNNs (Convolutional Neural Networks) and freely available satellite imagery for classifying landslide risk. The model has shown to be ultimately able to achieve a significantly better than baseline accuracy.", "label": "1", "is_selected": "0", "text": "In this paper, the authors aim to combine the latest state of the art models in image recognition with the best publicly available satellite images to create a system for landslide risk mitigation. We focus first on landslide detection and further propose a similar system to be used for prediction. Such models are valuable as they could easily be scaled up to provide data for hazard evaluation, as satellite imagery becomes increasingly available. The goal is to use satellite images and correlated data to enrich the public repository of data and guide disaster relief efforts for locating precise areas where landslides have occurred. Different image augmentation methods are used to increase diversity in the chosen dataset and create more robust classification. The resulting outputs are then fed into variants of 3-D convolutional neural networks. A review of the current literature indicates there is no research using CNNs (Convolutional Neural Networks) and freely available satellite imagery for classifying landslide risk. The model has shown to be ultimately able to achieve a significantly better than baseline accuracy."}
{"original_text": "Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose a general method for bounding the risk of exploration by learning a predictive model of the environment and using this model to reason about safety. We demonstrate our approach on MuJoCo continuous control tasks, reaching the state of the art on Walker2d-v1 while having more than 10x faster learning than prior methods.", "label": "0", "is_selected": "0", "text": "Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose a general method for bounding the risk of exploration by learning a predictive model of the environment and using this model to reason about safety. We demonstrate our approach on MuJoCo continuous control tasks, reaching the state of the art on Walker2d-v1 while having more than 10x faster learning than prior methods."}
{"original_text": "Hex is a complex game with a high branching factor. For the first time Hex is being attempted to be solved without the use of game tree structures and associated methods of pruning. We also are abstaining from any heuristic information about Virtual Connections or Semi Virtual Connections which were previously used in all previous known computer versions of the game. The H-search algorithm which was the basis of finding such connections and had been used with success in previous Hex playing agents has been forgone. Instead what we use is reinforcement learning through self play and approximations through neural networks to by pass the problem of high branching factor and maintaining large tables for state-action evaluations. Our code is based primarily on NeuroHex. The inspiration is drawn from the recent success of AlphaGo Zero.", "label": "1", "is_selected": "0", "text": "Hex is a complex game with a high branching factor. For the first time Hex is being attempted to be solved without the use of game tree structures and associated methods of pruning. We also are abstaining from any heuristic information about Virtual Connections or Semi Virtual Connections which were previously used in all previous known computer versions of the game. The H-search algorithm which was the basis of finding such connections and had been used with success in previous Hex playing agents has been forgone. Instead what we use is reinforcement learning through self play and approximations through neural networks to by pass the problem of high branching factor and maintaining large tables for state-action evaluations. Our code is based primarily on NeuroHex. The inspiration is drawn from the recent success of AlphaGo Zero."}
{"original_text": "The design of software systems inevitably enacts normative boundaries aroundthe site of intervention. These boundaries are, in part, a reflection of thevalues, ethics, power, and politics of the situation and the designer. In thispaper, we examine these normative boundaries in the design of one particularurban environmental monitoring system. We draw on design ethnography toexplicate the social and political contexts that shaped the design of the system,and to illuminate the ideological, ethical, and political work of designingwith sensors in a socially complex and contested urban environment.", "label": "0", "is_selected": "1", "text": "The design of software systems affects the way we interact with the world around us, and can have a profound impact on how we live, work, and play."}
{"original_text": "We initiate the study of indivisible chore allocation for agents with asymmetric shares. The fairness concept we focus on is the weighted natural generalization of maxmin share: WMMS fairness and we show that it is impossible to allocate chores fairly in a coalition-free way, unless PNP. We then study the more general class of monotone allocations, which are allocations that split each chore into as many pieces as there are agents. We show that it is impossible to allocate chores in a way that is fair and monotone in a coalition-free way. We also show that it is impossible to allocate chores in a way that is fair and coalition-free in general.", "label": "0", "is_selected": "0", "text": "We initiate the study of indivisible chore allocation for agents with asymmetric shares. The fairness concept we focus on is the weighted natural generalization of maxmin share: WMMS fairness and we show that it is impossible to allocate chores fairly in a coalition-free way, unless PNP. We then study the more general class of monotone allocations, which are allocations that split each chore into as many pieces as there are agents. We show that it is impossible to allocate chores in a way that is fair and monotone in a coalition-free way. We also show that it is impossible to allocate chores in a way that is fair and coalition-free in general."}
{"original_text": "In this paper we systematically study the importance, i.e., the influence on performance, of the main design elements that differentiate scalarizing functions-based multiobjective evolutionary algorithms (MOEAs). This class of MOEAs has been successfully applied to many different optimization problems in the literature. We analyze three important scalarizing functions: the Weighted-Sum, the Tchebycheff, and the Penalty. We focus on the influence of four main MOEA design elements: (1) selection operator, (2) crossover operator, (3) mutation operator, and (4) diversity preserving mechanism. We conduct an exhaustive set of experiments involving 16 possible combinations of the above mentioned design elements, using three different test problems. For each of the combinations, we compute the obtained spread and the hypervolume improvement. Our findings reveal that the Tchebycheff scalarizing function is the most successful in terms of both performance indicators, and it also shows that the main element that affects the performance of scalarizing functions-based MOEAs is the mutation operator. This work was supported by FEDER through COMPETE Programme and by National Funds through FCT - Fundac{c}ao para a Ciencia e Tecnologia under project FCOMP-01-0124-FEDER-016078.", "label": "0", "is_selected": "1", "text": "The influence of the main design elements on the performance of an evolutionary algorithm is one of the most important issues in the field of computer science, and has been the subject of many studies."}
{"original_text": "Crowd flow prediction has been increasingly investigated in intelligent urban computing field as a fundamental component of urban management system. The most challenging part of predicting crowd flow is to accurately model human mobility patterns. Recently, with the development of social media and crowdsourcing platforms, users' location data is increasingly available in various social media platforms. In this paper, we propose a crowd flow prediction method based on geo-tagged tweets which can provide more accurate crowd flow prediction. The proposed method consists of two phases: 1) tweets are collected by the region-based query and then analyzed by N-gram modeling, 2) crowd flow is predicted based on the results of the N-gram modeling and crowdsourced data. We present an empirical study for predicting crowd flow based on the proposed method using real geo-tagged tweets and crowdsourced data. The results show that the proposed method can provide more accurate crowd flow prediction than the existing methods. The work is supported by the National Natural Science Foundation of China (61300042), National High Technology Research and Development Program of China (2012AA011700), and the Natural Science Foundation of Shandong Province of China (ZR2013DQ018).", "label": "0", "is_selected": "0", "text": "Crowd flow prediction has been increasingly investigated in intelligent urban computing field as a fundamental component of urban management system. The most challenging part of predicting crowd flow is to accurately model human mobility patterns. Recently, with the development of social media and crowdsourcing platforms, users' location data is increasingly available in various social media platforms. In this paper, we propose a crowd flow prediction method based on geo-tagged tweets which can provide more accurate crowd flow prediction. The proposed method consists of two phases: 1) tweets are collected by the region-based query and then analyzed by N-gram modeling, 2) crowd flow is predicted based on the results of the N-gram modeling and crowdsourced data. We present an empirical study for predicting crowd flow based on the proposed method using real geo-tagged tweets and crowdsourced data. The results show that the proposed method can provide more accurate crowd flow prediction than the existing methods. The work is supported by the National Natural Science Foundation of China (61300042), National High Technology Research and Development Program of China (2012AA011700), and the Natural Science Foundation of Shandong Province of China (ZR2013DQ018)."}
{"original_text": "We present Animo, a smartwatch app that enables people to share and view each other's biosignals. We designed and engineered Animo to explore new ground for smartwatch-based biosignals social computing systems: identifying opportunities where these systems can support lightweight and mood-centric interactions. In our work we develop, explore, and evaluate several innovative features designed for dyadic communication of heart rate. We discuss the results of a two-week study (N34), including new communication patterns participants engaged in, and outline the design landscape for communicating with biosignals on smartwatches.", "label": "1", "is_selected": "0", "text": "We present Animo, a smartwatch app that enables people to share and view each other's biosignals. We designed and engineered Animo to explore new ground for smartwatch-based biosignals social computing systems: identifying opportunities where these systems can support lightweight and mood-centric interactions. In our work we develop, explore, and evaluate several innovative features designed for dyadic communication of heart rate. We discuss the results of a two-week study (N34), including new communication patterns participants engaged in, and outline the design landscape for communicating with biosignals on smartwatches."}
{"original_text": "We study the power and limits of optimal dynamic pricing in combinatorial markets; i.e., dynamic pricing that leads to optimal social welfare. Previous work by Cohen-Addad et al. [EC'16] demonstrated that, in combinatorial markets, dynamic pricing can be strictly better than static pricing. However, their algorithm has a run-time that is exponential in the number of items in the market, and so cannot be applied to large markets. Motivated by this limitation, we show that, in fact, no approximation algorithm can do better than static pricing, even if we relax the assumption that the seller knows the agents' valuations exactly. Our lower bound also applies to the online setting, where the valuations of the agents are revealed one at a time, and the seller has to determine the price of each item as soon as it is queried. We complement our negative result by exhibiting a simple algorithm for static pricing that achieves an approximation ratio of 1 - 1e and a run-time that is polynomial in the number of items in the market.", "label": "0", "is_selected": "1", "text": "In this paper, we show that no approximation algorithm can do better than static pricing in a combinatorial market, even if the seller knows the agents' valuations exactly."}
{"original_text": "Compromised social media accounts are legitimate user accounts that have been hijacked by a malicious party and can cause various kinds of damage, which makes the detection of these accounts very important. In this paper, we propose a framework that uses the social context of users to identify the compromised accounts on Twitter. We investigate the compromised accounts by analyzing their social connections and their communication behavior, and then we apply machine learning techniques to detect the malicious users. By using the proposed framework, we are able to detect 71 of the compromised accounts.", "label": "0", "is_selected": "1", "text": "In this paper, we propose a framework that uses the social context of users to identify the compromised accounts on Twitter, and then we apply machine learning techniques to detect the malicious users."}
{"original_text": "This work proposes an improved reversible data hiding scheme in encrypted images using parametric binary tree labeling (IPBTL-RDHEI), which takes advantage of the spatial correlation in the entire original image but not in small image blocks to reserve room for hiding data. Then the original image is encrypted with an encryption key and the parametric binary tree is used to label encrypted pixels into two different categories. Finally, one of the two categories of encrypted pixels can embed secret information by bit replacement. According to the experimental results, compared with several state-of-the-art methods, the proposed IPBTL-RDHEI method achieves higher embedding rate and outperforms the competitors. Due to the reversibility of IPBTL-RDHEI, the original plaintext image and the secret information can be restored and extracted losslessly and separately.", "label": "1", "is_selected": "0", "text": "This work proposes an improved reversible data hiding scheme in encrypted images using parametric binary tree labeling (IPBTL-RDHEI), which takes advantage of the spatial correlation in the entire original image but not in small image blocks to reserve room for hiding data. Then the original image is encrypted with an encryption key and the parametric binary tree is used to label encrypted pixels into two different categories. Finally, one of the two categories of encrypted pixels can embed secret information by bit replacement. According to the experimental results, compared with several state-of-the-art methods, the proposed IPBTL-RDHEI method achieves higher embedding rate and outperforms the competitors. Due to the reversibility of IPBTL-RDHEI, the original plaintext image and the secret information can be restored and extracted losslessly and separately."}
{"original_text": "We investigate the automatic classification of patient discharge notes into standard disease labels. We find that Convolutional Neural Networks with Attention outperform previous algorithms used in this task, and suggest further areas for improvement.", "label": "1", "is_selected": "0", "text": "We investigate the automatic classification of patient discharge notes into standard disease labels. We find that Convolutional Neural Networks with Attention outperform previous algorithms used in this task, and suggest further areas for improvement."}
{"original_text": "Since its renaissance, deep learning has been widely used in various medical imaging tasks and has achieved remarkable success in many medical imaging applications, thereby propelling us into the so-called artificial intelligence (AI) era. It is known that the success of AI is mostly attributed to the availability of big data with annotations for a single task and the advances in high performance computing. However, medical imaging presents unique challenges that confront deep learning approaches. In this survey paper, we first highlight both clinical needs and technical challenges in medical imaging and describe how emerging trends in deep learning are addressing these issues. We cover the topics of network architecture, sparse and noisy labels, federating learning, interpretability, uncertainty quantification, etc. Then, we present several case studies that are commonly found in clinical practice, including digital pathology and chest, brain, cardiovascular, and abdominal imaging. Rather than presenting an exhaustive literature survey, we instead describe some prominent research highlights related to these case study applications. We conclude with a discussion and presentation of promising future directions.", "label": "1", "is_selected": "0", "text": "Since its renaissance, deep learning has been widely used in various medical imaging tasks and has achieved remarkable success in many medical imaging applications, thereby propelling us into the so-called artificial intelligence (AI) era. It is known that the success of AI is mostly attributed to the availability of big data with annotations for a single task and the advances in high performance computing. However, medical imaging presents unique challenges that confront deep learning approaches. In this survey paper, we first highlight both clinical needs and technical challenges in medical imaging and describe how emerging trends in deep learning are addressing these issues. We cover the topics of network architecture, sparse and noisy labels, federating learning, interpretability, uncertainty quantification, etc. Then, we present several case studies that are commonly found in clinical practice, including digital pathology and chest, brain, cardiovascular, and abdominal imaging. Rather than presenting an exhaustive literature survey, we instead describe some prominent research highlights related to these case study applications. We conclude with a discussion and presentation of promising future directions."}
{"original_text": "This paper explores feedback systems using incremental redundancy (IR) with noiseless transmitter confirmation (NTC). For IR-NTC systems based on finite-length codes (with blocklength N) and decoding attempts only at certain specified decoding times, this paper presents the asymptotic expansion achieved by random coding, provides rate-compatible sphere-packing (RCSP) performance approximations, and presents simulation results of tail-biting convolutional codes. The information-theoretic analysis shows that values of N relatively close to the expected latency yield the same random-coding achievability expansion as with N . However, the penalty introduced in the expansion by limiting decoding times is linear in the interval between decoding times. For binary symmetric channels, the RCSP approximation provides an efficiently-computed approximation of performance that shows excellent agreement with a family of rate-compatible, tail-biting convolutional codes in the short-latency regime. For the additive white Gaussian noise channel, bounded-distance decoding simplifies the computation of the marginal RCSP approximation and produces similar results as analysis based on maximum-likelihood decoding for latencies greater than 200. The efficiency of the marginal RCSP approximation facilitates optimization of the lengths of incremental transmissions when the number of incremental transmissions is constrained to be small or the length of the incremental transmissions is constrained to be uniform after the first transmission. Finally, an RCSP-based decoding error trajectory is introduced that provides target error rates for the design of rate-compatible code families for use in feedback communication systems.", "label": "1", "is_selected": "0", "text": "This paper explores feedback systems using incremental redundancy (IR) with noiseless transmitter confirmation (NTC). For IR-NTC systems based on finite-length codes (with blocklength N) and decoding attempts only at certain specified decoding times, this paper presents the asymptotic expansion achieved by random coding, provides rate-compatible sphere-packing (RCSP) performance approximations, and presents simulation results of tail-biting convolutional codes. The information-theoretic analysis shows that values of N relatively close to the expected latency yield the same random-coding achievability expansion as with N . However, the penalty introduced in the expansion by limiting decoding times is linear in the interval between decoding times. For binary symmetric channels, the RCSP approximation provides an efficiently-computed approximation of performance that shows excellent agreement with a family of rate-compatible, tail-biting convolutional codes in the short-latency regime. For the additive white Gaussian noise channel, bounded-distance decoding simplifies the computation of the marginal RCSP approximation and produces similar results as analysis based on maximum-likelihood decoding for latencies greater than 200. The efficiency of the marginal RCSP approximation facilitates optimization of the lengths of incremental transmissions when the number of incremental transmissions is constrained to be small or the length of the incremental transmissions is constrained to be uniform after the first transmission. Finally, an RCSP-based decoding error trajectory is introduced that provides target error rates for the design of rate-compatible code families for use in feedback communication systems."}
{"original_text": "Frequent pattern mining is a key area of study that gives insights into the structure and dynamics of evolving networks, such as social or road networks. However, not only does it require a very large amount of storage space to hold the evolving network, but also the computational cost of finding frequent patterns is extremely high. In this paper, we propose a novel pattern mining approach to capture frequent patterns in evolving networks. We exploit the information-theoretic concept of entropy and use it in a new definition of the support of a pattern. To increase the efficiency of our approach, we develop a new entropy-based pattern count algorithm that is able to obtain an exact count of all support values in polynomial time. Finally, we conduct a series of experiments on a wide range of real-world datasets. We empirically demonstrate the efficiency of our approach, as well as the fact that the patterns captured by our entropy-based approach capture structures that are not captured by existing approaches, without the need to tune any parameters.", "label": "0", "is_selected": "0", "text": "Frequent pattern mining is a key area of study that gives insights into the structure and dynamics of evolving networks, such as social or road networks. However, not only does it require a very large amount of storage space to hold the evolving network, but also the computational cost of finding frequent patterns is extremely high. In this paper, we propose a novel pattern mining approach to capture frequent patterns in evolving networks. We exploit the information-theoretic concept of entropy and use it in a new definition of the support of a pattern. To increase the efficiency of our approach, we develop a new entropy-based pattern count algorithm that is able to obtain an exact count of all support values in polynomial time. Finally, we conduct a series of experiments on a wide range of real-world datasets. We empirically demonstrate the efficiency of our approach, as well as the fact that the patterns captured by our entropy-based approach capture structures that are not captured by existing approaches, without the need to tune any parameters."}
{"original_text": "The task of event detection and classification is central to most information retrieval applications. We show that a Transformer based architecture can effectively model event extraction as a sequence labeling task. We propose a combination of sentence level and token level training objectives that significantly boosts the performance of a BERT based event extraction model. Our approach achieves a new state-of-the-art performance on ACE 2005 data for English and Chinese. We also test our model on ERE Spanish, achieving an average gain of 2 absolute F 1 points over prior best performing models.", "label": "1", "is_selected": "0", "text": "The task of event detection and classification is central to most information retrieval applications. We show that a Transformer based architecture can effectively model event extraction as a sequence labeling task. We propose a combination of sentence level and token level training objectives that significantly boosts the performance of a BERT based event extraction model. Our approach achieves a new state-of-the-art performance on ACE 2005 data for English and Chinese. We also test our model on ERE Spanish, achieving an average gain of 2 absolute F 1 points over prior best performing models."}
{"original_text": "This study mainly investigates two decoding problems in neural keyphrase generation: sequence length bias and beam diversity. We introduce an extension of beam search inference based on word-level and n-gram-level LSTM language models, which we name beam search with contextualized recurrent language models (CSL-Beam). Experimental results show that the proposed approach can reduce the effect of sequence length bias and improve the performance of neural keyphrase generation in terms of both precision and recall. Y. Zhang, Q. Pan, and J. Wang, \"CSL-Beam: Contextualized Sequence Length Bias Reduction for Neural Keyphrase Generation,\" J. Adv. Comput. Intell. Intell. Inform., Vol.21, No.4, pp. 556-565, 2017. D. Zhang and P. Liu, \"A Survey of Keyphrase Extraction,\" IEEE Trans. on Knowledge and Data Engineering, Vol.28, No.1, pp. 167-182, 2016. T. Lin, J. Yih, R. C. Moore, et al., \"Keyphrase Generation with Neural Networks,\" Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pp. , 2013. X. Ding and D. Zhang, \"KGRAF: Keyphrase Generation with Graph-based Recurrent Neural Networks,\" Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. , 2016. Y. Zhang, Q. Pan, and J. Wang, \"A Study on Word-Level and n-gram-Level LSTM Language Models,\" J. of the China Computer Federation, Vol.33, No.2, pp. 218-230, 2016 (in Chinese). T. Mikolov, K. Karafiat, L. Burget, et al., \"Recurrent Neural Network based Language Model,\" Advances in Neural Information Processing Systems (NIPS), pp. , 2010. J. Chung, K. Lee, Y. Kwak, et al., \"A Recurrent Neural Network Language Model for Speech Recognition,\" Proc. of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 863-867, 2014. S. Graves, J. Schmidhuber, and F. Cummins, \"Framewise phoneme classification with bidirectional LSTM and other neural network architectures,\" Proc. of the International Conference on Neural Information Processing Systems (NIPS), pp. , 2005. C. Kim, J. P. Lim, and T. Kim, \"Character-level Neural Language Models for Next-word Prediction,\" Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. , 2016. N. C. J. de Waal, R. L. Uhr, T. K. Nguyen, et al., \"Word-Level Neural Language Models with Subword Input,\" Proc. of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pp. , 2015. A. J. R. Dinh, T. Le, M. S. Sekhavat, et al., \"Improving Neural Machine Translation Models with Domain Knowledge,\" Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. , 2015. Y. Zhang, Q. Pan, and J. Wang, \"An Empirical Study on Beam Search Decoding for Neural Keyphrase Generation,\" Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 271-276,", "label": "0", "is_selected": "0", "text": "This study mainly investigates two decoding problems in neural keyphrase generation: sequence length bias and beam diversity. We introduce an extension of beam search inference based on word-level and n-gram-level LSTM language models, which we name beam search with contextualized recurrent language models (CSL-Beam). Experimental results show that the proposed approach can reduce the effect of sequence length bias and improve the performance of neural keyphrase generation in terms of both precision and recall. Y. Zhang, Q. Pan, and J. Wang, \"CSL-Beam: Contextualized Sequence Length Bias Reduction for Neural Keyphrase Generation,\" J. Adv. Comput. Intell. Intell. Inform., Vol.21, No.4, pp. 556-565, 2017. D. Zhang and P. Liu, \"A Survey of Keyphrase Extraction,\" IEEE Trans. on Knowledge and Data Engineering, Vol.28, No.1, pp. 167-182, 2016. T. Lin, J. Yih, R. C. Moore, et al., \"Keyphrase Generation with Neural Networks,\" Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pp. , 2013. X. Ding and D. Zhang, \"KGRAF: Keyphrase Generation with Graph-based Recurrent Neural Networks,\" Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. , 2016. Y. Zhang, Q. Pan, and J. Wang, \"A Study on Word-Level and n-gram-Level LSTM Language Models,\" J. of the China Computer Federation, Vol.33, No.2, pp. 218-230, 2016 (in Chinese). T. Mikolov, K. Karafiat, L. Burget, et al., \"Recurrent Neural Network based Language Model,\" Advances in Neural Information Processing Systems (NIPS), pp. , 2010. J. Chung, K. Lee, Y. Kwak, et al., \"A Recurrent Neural Network Language Model for Speech Recognition,\" Proc. of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 863-867, 2014. S. Graves, J. Schmidhuber, and F. Cummins, \"Framewise phoneme classification with bidirectional LSTM and other neural network architectures,\" Proc. of the International Conference on Neural Information Processing Systems (NIPS), pp. , 2005. C. Kim, J. P. Lim, and T. Kim, \"Character-level Neural Language Models for Next-word Prediction,\" Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. , 2016. N. C. J. de Waal, R. L. Uhr, T. K. Nguyen, et al., \"Word-Level Neural Language Models with Subword Input,\" Proc. of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pp. , 2015. A. J. R. Dinh, T. Le, M. S. Sekhavat, et al., \"Improving Neural Machine Translation Models with Domain Knowledge,\" Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. , 2015. Y. Zhang, Q. Pan, and J. Wang, \"An Empirical Study on Beam Search Decoding for Neural Keyphrase Generation,\" Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 271-276,"}
{"original_text": "Knowledge graphs capture entities and relations from long documents and can facilitate reasoning in many downstream applications. Extracting compact knowledge graphs containing only salient entities and relations is important but challenging. We propose a novel knowledge graph extraction system that can extract compact knowledge graphs from long documents. Our system is built on top of a large-scale knowledge base and a novel entity-relation graph model. It jointly optimizes entities and relations using a reinforcement learning framework. We also propose a novel hybrid entity-relation model that captures different relations between entities. We have developed a scalable and efficient knowledge graph extraction system using a reinforcement learning framework. To optimize compact knowledge graphs, we model both entities and relations using reinforcement learning. To achieve good performance on long documents, we use a hybrid entity-relation model that can capture different relations between entities. Our system is trained on a large-scale knowledge base and achieves the best performance on several challenging benchmark datasets. Knowledge graph, knowledge graph extraction, long documents, reinforcement learning, entity-relation model Zheng Chen, Lingxiao Jing, and Jianfeng Gao. Towards Compact Knowledge Graph Extraction from Long Documents. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 2019 Conference on Computational Natural Language Learning, 2019.", "label": "0", "is_selected": "0", "text": "Knowledge graphs capture entities and relations from long documents and can facilitate reasoning in many downstream applications. Extracting compact knowledge graphs containing only salient entities and relations is important but challenging. We propose a novel knowledge graph extraction system that can extract compact knowledge graphs from long documents. Our system is built on top of a large-scale knowledge base and a novel entity-relation graph model. It jointly optimizes entities and relations using a reinforcement learning framework. We also propose a novel hybrid entity-relation model that captures different relations between entities. We have developed a scalable and efficient knowledge graph extraction system using a reinforcement learning framework. To optimize compact knowledge graphs, we model both entities and relations using reinforcement learning. To achieve good performance on long documents, we use a hybrid entity-relation model that can capture different relations between entities. Our system is trained on a large-scale knowledge base and achieves the best performance on several challenging benchmark datasets. Knowledge graph, knowledge graph extraction, long documents, reinforcement learning, entity-relation model Zheng Chen, Lingxiao Jing, and Jianfeng Gao. Towards Compact Knowledge Graph Extraction from Long Documents. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 2019 Conference on Computational Natural Language Learning, 2019."}
{"original_text": "In optimization, the negative gradient of a function denotes the direction of steepest descent. Furthermore, traveling in any direction orthogonal to the gradient maintains the value of the function. In this work, we show that these orthogonal directions that are ignored by gradient descent can be critical in equilibrium problems. Equilibrium problems have drawn heightened attention in machine learning due to the emergence of the Generative Adversarial Network (GAN). We use the framework of Variational Inequalities to analyze popular training algorithms for a fundamental GAN variant: the Wasserstein Linear-Quadratic GAN. We show that the steepest descent direction causes divergence from the equilibrium, and convergence to the equilibrium is achieved through following a particular orthogonal direction. We call this successful technique Crossing-the-Curl, named for its mathematical derivation as well as its intuition: identify the game's axis of rotation and move \"across\" space in the direction towards smaller \"curling.\"", "label": "1", "is_selected": "0", "text": "In optimization, the negative gradient of a function denotes the direction of steepest descent. Furthermore, traveling in any direction orthogonal to the gradient maintains the value of the function. In this work, we show that these orthogonal directions that are ignored by gradient descent can be critical in equilibrium problems. Equilibrium problems have drawn heightened attention in machine learning due to the emergence of the Generative Adversarial Network (GAN). We use the framework of Variational Inequalities to analyze popular training algorithms for a fundamental GAN variant: the Wasserstein Linear-Quadratic GAN. We show that the steepest descent direction causes divergence from the equilibrium, and convergence to the equilibrium is achieved through following a particular orthogonal direction. We call this successful technique Crossing-the-Curl, named for its mathematical derivation as well as its intuition: identify the game's axis of rotation and move \"across\" space in the direction towards smaller \"curling.\""}
{"original_text": "In Reasoning about Action and Planning, one synthesizes the agent plan by taking advantage of the assumption on how the environment works (that is, one exploits the environment's effects, its model, or its laws). In contrast, a plan synthesis mechanism that exploits the environment's model is not part of the reasoning mechanism in Action and Planning. In this paper we extend such a reasoning mechanism to Action and Planning. This extended reasoning mechanism, called Partially Observable Action and Planning (POAP) is built upon POMDPs, and thus, it is a probabilistic reasoning mechanism that exploits the environment's model (POMDPs) to synthesize plans. In the POAP mechanism, we augment the traditional POMDP formulation with the cost of each action. The cost of an action reflects the contribution of the action to the goal. We show that the POAP planning can be performed by a POMDP algorithm that is modified for POAP. Our experiments show that POAP can synthesize plans that are better than the plans synthesized by a POMDP algorithm for POAP. X In Reasoning about Action and Planning, one synthesizes the agent plan by taking advantage of the assumption on how the environment works (that is, one exploits the environment's effects, its model, or its laws). In contrast, a plan synthesis mechanism that exploits the environment's model is not part of the reasoning mechanism in Action and Planning. In this paper we extend such a reasoning mechanism to Action and Planning. This extended reasoning mechanism, called Partially Observable Action and Planning (POAP) is built upon POMDPs, and thus, it is a probabilistic reasoning mechanism that exploits the environment's model (POMDPs) to synthesize plans. In the POAP mechanism, we augment the traditional POMDP formulation with the cost of each action. The cost of an action reflects the contribution of the action to the goal. We show that the POAP planning can be performed by a POMDP algorithm that is modified for POAP. Our experiments show that POAP can synthesize plans that are better than the plans synthesized by a POMDP algorithm for POAP. AB - In Reasoning about Action and Planning, one synthesizes the agent plan by taking advantage of the assumption on how the environment works (that is, one exploits the environment's effects, its model, or its laws). In contrast, a plan synthesis mechanism that exploits the environment's model is not part of the reasoning mechanism in Action and Planning. In this paper we extend such a reasoning mechanism to Action and Planning. This extended reasoning mechanism, called Partially Observable Action and Planning (POAP) is built upon POMDPs, and thus, it is a probabilistic reasoning mechanism that exploits the environment's model (POMDPs) to synthesize plans. In the POAP mechanism, we augment the traditional POMDP formulation with the cost of each action. The cost of an action reflects the contribution of the action to the goal. We show that the POAP planning can be performed by a POMDP algorithm that is modified for POAP. Our experiments show that POAP can synthesize plans that are better than the plans synthesized by a POMDP algorithm for POAP.", "label": "0", "is_selected": "1", "text": "In the paper Action and Planning, we show that a plan synthesis mechanism that exploits the environment's model is not part of the reasoning mechanism in action and planning."}
{"original_text": "The logic FO (ID) uses ideas from the field of logic programming to extend first order logic with non-monotone inductive definitions. Such logic formally extends logic programming, abductive logic programming, and (non-monotonic) inductive definitions. In particular, FO (ID) allows non-monotonic least fixpoints over infinite sets. Furthermore, the user can define his own non-monotonic inductive definitions in terms of FO (ID) definable inductive definitions. Thus, we provide a simple and flexible framework to reason with non-monotonic least fixpoints.", "label": "0", "is_selected": "0", "text": "The logic FO (ID) uses ideas from the field of logic programming to extend first order logic with non-monotone inductive definitions. Such logic formally extends logic programming, abductive logic programming, and (non-monotonic) inductive definitions. In particular, FO (ID) allows non-monotonic least fixpoints over infinite sets. Furthermore, the user can define his own non-monotonic inductive definitions in terms of FO (ID) definable inductive definitions. Thus, we provide a simple and flexible framework to reason with non-monotonic least fixpoints."}
{"original_text": "Various statistical analysis methods are studied for years to extract accurate trends of network traffic and predict the future load mainly to allocate required resources. Besides, many stochastic modeling techniques are offered to represent fundamental characteristics of different types of network traffic. In this study, we analyze autoregressive traffic forecasting techniques considering their popularity and wide-use in the domain. In comparison to similar works, we present important traffic characteristics and discussions from the literature to create a self-consistent guidance along with the survey. Then, we approach to techniques in the literature revealing which network characteristics they can capture offering a characteristic-based framework. Most importantly, we aim to fill the gap between the statistical analysis of those methods and their relevance with networking by dicussing significant aspects and requirements for accurate forecasting from a network-telemetric perspective.", "label": "1", "is_selected": "0", "text": "Various statistical analysis methods are studied for years to extract accurate trends of network traffic and predict the future load mainly to allocate required resources. Besides, many stochastic modeling techniques are offered to represent fundamental characteristics of different types of network traffic. In this study, we analyze autoregressive traffic forecasting techniques considering their popularity and wide-use in the domain. In comparison to similar works, we present important traffic characteristics and discussions from the literature to create a self-consistent guidance along with the survey. Then, we approach to techniques in the literature revealing which network characteristics they can capture offering a characteristic-based framework. Most importantly, we aim to fill the gap between the statistical analysis of those methods and their relevance with networking by dicussing significant aspects and requirements for accurate forecasting from a network-telemetric perspective."}
{"original_text": "Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.", "label": "1", "is_selected": "0", "text": "Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability."}
{"original_text": "As of September 2020, the COVID-19 pandemic continues to devastate the health and well-being of the global population. With more than 33 million confirmed cases and over a million deaths, global health organizations are still a long way from fully containing the pandemic. This pandemic has raised serious questions about the emergency preparedness of health agencies, not only in terms of treatment of an unseen disease, but also in identifying its early symptoms. In the particular case of COVID-19, several studies have indicated that chest radiography images of the infected patients show characteristic abnormalities. However, at the onset of a given pandemic, such as COVID-19, there may not be sufficient data for the affected cases to train models for their robust detection. Hence, supervised classification is ill-posed for this problem because the time spent in collecting large amounts of infected peoples' data could lead to the loss of human lives and delays in preventive interventions. Therefore, we formulate this problem within a one-class classification framework, in which the data for healthy patients is abundantly available, whereas no training data is present for the class of interest (COVID-19 in our case). To solve this problem, we present COVIDomaly, a convolutional autoencoder framework to detect unseen COVID-19 cases from the chest radiographs. We tested two settings on a publicly available dataset (COVIDx) by training the model on chest X-rays from (i) only healthy adults, and (ii) healthy and other non-COVID-19 pneumonia, and detected COVID-19 as an anomaly. After performing 3-fold cross validation, we obtain a pooled ROC-AUC of 0.7652 and 0.6902 in the two settings respectively. These results are very encouraging and pave the way towards research for ensuring emergency preparedness in future pandemics, especially the ones that could be detected from chest X-rays.", "label": "1", "is_selected": "0", "text": "As of September 2020, the COVID-19 pandemic continues to devastate the health and well-being of the global population. With more than 33 million confirmed cases and over a million deaths, global health organizations are still a long way from fully containing the pandemic. This pandemic has raised serious questions about the emergency preparedness of health agencies, not only in terms of treatment of an unseen disease, but also in identifying its early symptoms. In the particular case of COVID-19, several studies have indicated that chest radiography images of the infected patients show characteristic abnormalities. However, at the onset of a given pandemic, such as COVID-19, there may not be sufficient data for the affected cases to train models for their robust detection. Hence, supervised classification is ill-posed for this problem because the time spent in collecting large amounts of infected peoples' data could lead to the loss of human lives and delays in preventive interventions. Therefore, we formulate this problem within a one-class classification framework, in which the data for healthy patients is abundantly available, whereas no training data is present for the class of interest (COVID-19 in our case). To solve this problem, we present COVIDomaly, a convolutional autoencoder framework to detect unseen COVID-19 cases from the chest radiographs. We tested two settings on a publicly available dataset (COVIDx) by training the model on chest X-rays from (i) only healthy adults, and (ii) healthy and other non-COVID-19 pneumonia, and detected COVID-19 as an anomaly. After performing 3-fold cross validation, we obtain a pooled ROC-AUC of 0.7652 and 0.6902 in the two settings respectively. These results are very encouraging and pave the way towards research for ensuring emergency preparedness in future pandemics, especially the ones that could be detected from chest X-rays."}
{"original_text": "Parameterized algorithms are a very useful tool for dealing with NP-hard problems on graphs. Yet, to properly utilize parameterized algorithms it is necessary to choose the right parameter based on the type of problem and properties of the target graph class. Tree-width is an example of a very successful graph parameter, however it cannot be used on dense graph classes and there also exist problems which are hard even on graphs of bounded tree-width. Such problems can be tackled by using vertex cover as a parameter, however this places severe restrictions on admissible graph classes. Michael Lampis has recently introduced neighborhood diversity, a new graph parameter which generalizes vertex cover to dense graphs. Among other results, he has shown that simple parameterized algorithms exist for a few problems on graphs of bounded neighborhood diversity. Our article further studies this area and provides new algorithms parameterized by neighborhood diversity for the p-Vertex-Disjoint Paths, Graph Motif and Precoloring Extension problems - the latter two being hard even on graphs of bounded tree-width.", "label": "1", "is_selected": "0", "text": "Parameterized algorithms are a very useful tool for dealing with NP-hard problems on graphs. Yet, to properly utilize parameterized algorithms it is necessary to choose the right parameter based on the type of problem and properties of the target graph class. Tree-width is an example of a very successful graph parameter, however it cannot be used on dense graph classes and there also exist problems which are hard even on graphs of bounded tree-width. Such problems can be tackled by using vertex cover as a parameter, however this places severe restrictions on admissible graph classes. Michael Lampis has recently introduced neighborhood diversity, a new graph parameter which generalizes vertex cover to dense graphs. Among other results, he has shown that simple parameterized algorithms exist for a few problems on graphs of bounded neighborhood diversity. Our article further studies this area and provides new algorithms parameterized by neighborhood diversity for the p-Vertex-Disjoint Paths, Graph Motif and Precoloring Extension problems - the latter two being hard even on graphs of bounded tree-width."}
{"original_text": "We investigate GPU-based parallelization of Iterative-Deepening A (IDA). We show that straightforward thread-based parallelization techniques which were previously proposed for massively parallel SIMD processors perform poorly due to warp divergence and load imbalance. We propose Block-Parallel IDA (BPIDA), which assigns the search of a subtree to a block (a group of threads with access to fast shared memory) rather than a thread. On the 15-puzzle, BPIDA on a NVIDIA GRID K520 with 1536 CUDA cores achieves a speedup of 4.98 compared to a highly optimized sequential IDA implementation on a Xeon E5-2670 core. 1 1 footnote 1 This is an extended manuscript based on a paper accepted to appear in SoCS2017.", "label": "1", "is_selected": "0", "text": "We investigate GPU-based parallelization of Iterative-Deepening A (IDA). We show that straightforward thread-based parallelization techniques which were previously proposed for massively parallel SIMD processors perform poorly due to warp divergence and load imbalance. We propose Block-Parallel IDA (BPIDA), which assigns the search of a subtree to a block (a group of threads with access to fast shared memory) rather than a thread. On the 15-puzzle, BPIDA on a NVIDIA GRID K520 with 1536 CUDA cores achieves a speedup of 4.98 compared to a highly optimized sequential IDA implementation on a Xeon E5-2670 core. 1 1 footnote 1 This is an extended manuscript based on a paper accepted to appear in SoCS2017."}
{"original_text": "Digitisation of fruit trees using LiDAR enables analysis which can be used to better growing practices to improve yield. Sophisticated analysis requires geometric and semantic understanding of the data, including the ability to discern individual trees as well as identifying leafy and structural matter. Extraction of this information should be rapid, as should data capture, so that entire orchards can be processed, but existing methods for classification and segmentation rely on high-quality data or additional data sources like cameras. We present a method for analysis of LiDAR data specifically for individual tree location, segmentation and matter classification, which can operate on low-quality data captured by handheld or mobile LiDAR. Results demonstrate viability both on real data for avocado and mango trees and virtual data with independently controlled sensor noise and tree spacing.", "label": "1", "is_selected": "0", "text": "Digitisation of fruit trees using LiDAR enables analysis which can be used to better growing practices to improve yield. Sophisticated analysis requires geometric and semantic understanding of the data, including the ability to discern individual trees as well as identifying leafy and structural matter. Extraction of this information should be rapid, as should data capture, so that entire orchards can be processed, but existing methods for classification and segmentation rely on high-quality data or additional data sources like cameras. We present a method for analysis of LiDAR data specifically for individual tree location, segmentation and matter classification, which can operate on low-quality data captured by handheld or mobile LiDAR. Results demonstrate viability both on real data for avocado and mango trees and virtual data with independently controlled sensor noise and tree spacing."}
{"original_text": "In this paper, network of agents with identical dynamics is considered. The agents are assumed to be fed by self and neighboring output measurements, while the states are not available for feedback. The communication topology is modeled by a directed graph. The control objective is to asymptotically steer the network output to zero by means of distributed control laws which are applied at each node. The proposed control strategy is based on the dynamic average consensus algorithm. By introducing an intermediate variable in the control law, the proposed control strategy can be seen as a combination of the dynamic average consensus algorithm and distributed output feedback control law. The main result provides sufficient conditions for the control objective to be attainable in terms of linear matrix inequalities. A numerical example is presented to illustrate the effectiveness of the proposed approach.", "label": "0", "is_selected": "1", "text": "A distributed feedback control strategy has been proposed to control a network of agents in terms of a distributed control law which is applied at each node of the network."}
{"original_text": "Roundabouts in conjunction with other traffic scenarios, e.g., intersections, merging roadways, speed reduction zones, can induce congestion in a transportation network due to driver responses to various disturbances. Research efforts have shown that smoothing traffic flow and eliminating stop-and-go driving can both improve fuel efficiency of the vehicles and the throughput of a roundabout. In this paper, we validate an optimal control framework developed earlier in a multi-lane roundabout scenario using the University of Delaware's scaled smart city (UDSSC). We first provide conditions where the solution is optimal. Then, we demonstrate the feasibility of the solution using experiments at UDSSC, and show that the optimal solution completely eliminates stop-and-go driving while preserving safety.", "label": "1", "is_selected": "0", "text": "Roundabouts in conjunction with other traffic scenarios, e.g., intersections, merging roadways, speed reduction zones, can induce congestion in a transportation network due to driver responses to various disturbances. Research efforts have shown that smoothing traffic flow and eliminating stop-and-go driving can both improve fuel efficiency of the vehicles and the throughput of a roundabout. In this paper, we validate an optimal control framework developed earlier in a multi-lane roundabout scenario using the University of Delaware's scaled smart city (UDSSC). We first provide conditions where the solution is optimal. Then, we demonstrate the feasibility of the solution using experiments at UDSSC, and show that the optimal solution completely eliminates stop-and-go driving while preserving safety."}
{"original_text": "Face recognition techniques have been developed significantly in recent years. However, recognizing faces with partial occlusion is still challenging for existing face recognizers which is heavily desired in real-world applications. In this paper, we propose a novel and real-time face recognition system based on locality sensitive hashing (LSH). In order to avoid the influence of occlusion, LSH is applied to divide the feature space into several clusters. Thus, partial occlusion does not affect the face recognition process. This face recognition system consists of two parts: the training process and the face recognition process. In the training process, a novel face-relevant local feature (FRLF) is extracted based on the analysis of the structure of the face. Then, the face feature is compared with the FRLF to verify the authenticity of the face image. Finally, the LSH is used to divide the feature space and the probe image is matched with each of the clusters to recognize the identity of the face. Experimental results indicate that the proposed method can achieve high accuracy for recognizing faces with partial occlusions and show a good balance between recognition accuracy and computational efficiency.", "label": "0", "is_selected": "0", "text": "Face recognition techniques have been developed significantly in recent years. However, recognizing faces with partial occlusion is still challenging for existing face recognizers which is heavily desired in real-world applications. In this paper, we propose a novel and real-time face recognition system based on locality sensitive hashing (LSH). In order to avoid the influence of occlusion, LSH is applied to divide the feature space into several clusters. Thus, partial occlusion does not affect the face recognition process. This face recognition system consists of two parts: the training process and the face recognition process. In the training process, a novel face-relevant local feature (FRLF) is extracted based on the analysis of the structure of the face. Then, the face feature is compared with the FRLF to verify the authenticity of the face image. Finally, the LSH is used to divide the feature space and the probe image is matched with each of the clusters to recognize the identity of the face. Experimental results indicate that the proposed method can achieve high accuracy for recognizing faces with partial occlusions and show a good balance between recognition accuracy and computational efficiency."}
{"original_text": "In spite of its importance, passenger demand prediction is a highly challenging problem, because the demand is simultaneously influenced by the complex interactions among many spatial and temporal factors and other external factors such as weather. To address this problem, we propose a Spatio-TEmporal Fuzzy neural Network (STEF-Net) to accurately predict passenger demands incorporating the complex interactions of all known important factors. We design an end-to-end learning framework with different neural networks modeling different factors. Specifically, we propose to capture spatio-temporal feature interactions via a convolutional long short-term memory network and model external factors via a fuzzy neural network that handles data uncertainty significantly better than deterministic methods. To keep the temporal relations when fusing two networks and emphasize discriminative spatio-temporal feature interactions, we employ a novel feature fusion method with a convolution operation and an attention layer. As far as we know, our work is the first to fuse a deep recurrent neural network and a fuzzy neural network to model complex spatial-temporal feature interactions with additional uncertain input features for predictive learning. Experiments on a large-scale real-world dataset show that our model achieves more than 10 improvement over the state-of-the-art approaches.", "label": "1", "is_selected": "0", "text": "In spite of its importance, passenger demand prediction is a highly challenging problem, because the demand is simultaneously influenced by the complex interactions among many spatial and temporal factors and other external factors such as weather. To address this problem, we propose a Spatio-TEmporal Fuzzy neural Network (STEF-Net) to accurately predict passenger demands incorporating the complex interactions of all known important factors. We design an end-to-end learning framework with different neural networks modeling different factors. Specifically, we propose to capture spatio-temporal feature interactions via a convolutional long short-term memory network and model external factors via a fuzzy neural network that handles data uncertainty significantly better than deterministic methods. To keep the temporal relations when fusing two networks and emphasize discriminative spatio-temporal feature interactions, we employ a novel feature fusion method with a convolution operation and an attention layer. As far as we know, our work is the first to fuse a deep recurrent neural network and a fuzzy neural network to model complex spatial-temporal feature interactions with additional uncertain input features for predictive learning. Experiments on a large-scale real-world dataset show that our model achieves more than 10 improvement over the state-of-the-art approaches."}
{"original_text": "Evolving graphs arise in problems where interrelations between data change over time. We present a breadth first search (BFS) algorithm for evolving graphs that computes the most direct influences between nodes at two different times. Using simple examples, we show that naive unfoldings of adjacency matrices miscount the number of temporal paths. By mapping an evolving graph to an adjacency matrix of an equivalent static graph, we prove that our generalization of the BFS algorithm correctly accounts for paths that traverse both space and time. Finally, we demonstrate how the BFS over evolving graphs can be applied to mine citation networks.", "label": "1", "is_selected": "0", "text": "Evolving graphs arise in problems where interrelations between data change over time. We present a breadth first search (BFS) algorithm for evolving graphs that computes the most direct influences between nodes at two different times. Using simple examples, we show that naive unfoldings of adjacency matrices miscount the number of temporal paths. By mapping an evolving graph to an adjacency matrix of an equivalent static graph, we prove that our generalization of the BFS algorithm correctly accounts for paths that traverse both space and time. Finally, we demonstrate how the BFS over evolving graphs can be applied to mine citation networks."}
{"original_text": "Instance-level video segmentation requires a solid integration of spatial and temporal information. However, current methods rely mostly on domain-specific information (online learning) to produce accurate instance-level segmentations. We propose a novel framework which trains a segmentation model on instance-level pixel-level annotations and performs instance-level segmentation without any additional online learning. We exploit a novel two-stage spatial attention mechanism to propagate contextual information across different frames. We further improve the propagation of contextual information by incorporating spatial attention to the temporal attention mechanism. Furthermore, we propose a novel 3D convolutional layer which performs feature aggregation in both spatial and temporal directions. Comprehensive experiments show that our framework achieves state-of-the-art performance on several benchmarks. This work is partially supported by the National Natural Science Foundation of China under Grant No.61876218.", "label": "0", "is_selected": "1", "text": "In this paper, we propose a novel framework for instance-level video segmentation. We suggest a novel 3Dal layer which performs feature aggregation in both spatial and temporal directions."}
{"original_text": "We propose a novel vector representation that integrates lexical contrast into distributional vectors and strengthens the most salient features for determining degrees of word similarity. The improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66-0.76 across word classes (adjectives, nouns, verbs). Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model. The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex-999, and on distinguishing antonyms from synonyms.", "label": "1", "is_selected": "0", "text": "We propose a novel vector representation that integrates lexical contrast into distributional vectors and strengthens the most salient features for determining degrees of word similarity. The improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66-0.76 across word classes (adjectives, nouns, verbs). Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model. The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex-999, and on distinguishing antonyms from synonyms."}
{"original_text": "Hypothesis testing for graphs has been an important tool in applied research fields for more than two decades, and still remains a challenging problem as one often needs to draw inference from few replicates of large graphs. Recent studies in statistics and learning theory have provided some theoretical insights about such high-dimensional graph testing problems, but the practicality of the developed theoretical methods remains an open question. In this paper, we consider the problem of two-sample testing of large graphs. We demonstrate the practical merits and limitations of existing theoretical tests and their bootstrapped variants. We also propose two new tests based on asymptotic distributions. We show that these tests are computationally less expensive and, in some cases, more reliable than the existing methods.", "label": "1", "is_selected": "0", "text": "Hypothesis testing for graphs has been an important tool in applied research fields for more than two decades, and still remains a challenging problem as one often needs to draw inference from few replicates of large graphs. Recent studies in statistics and learning theory have provided some theoretical insights about such high-dimensional graph testing problems, but the practicality of the developed theoretical methods remains an open question. In this paper, we consider the problem of two-sample testing of large graphs. We demonstrate the practical merits and limitations of existing theoretical tests and their bootstrapped variants. We also propose two new tests based on asymptotic distributions. We show that these tests are computationally less expensive and, in some cases, more reliable than the existing methods."}
{"original_text": "We consider the problem of deciding the satisfiability of quantifier-free formulas in the theory of finite sets with cardinality constraints. Sets are a common high-level data structure used in programming; cardinality constraints can be used to express invariants of a program. Satisfiability in this theory is known to be decidable, but the currently known algorithms have unacceptably high complexity. The key obstacle for improving the known algorithms is the inherent algebraic complexity of cardinality constraints. We use a novel approach that exploits the algorithmic properties of the constraint system, and we show how to transfer ideas from the domain of constraint solving to improve the complexity of satisfiability algorithms for cardinality constraints.", "label": "0", "is_selected": "1", "text": "Satisfiability in the theory of finite sets with cardinality constraints is known to be decidable, but the currently known algorithms have unacceptably high complexity."}
{"original_text": "In this work, we explain the working mechanism of MixUp in terms of adversarial training. We introduce a new class of adversarial training schemes, which we refer to as directional adversarial training, or DAT. In a nutshell, a DAT scheme perturbs a training example in the direction of another example but keeps its original label as the training target. We prove that MixUp is equivalent to a special subclass of DAT, in that it has the same expected loss function and corresponds to the same optimization problem asymptotically. This understanding not only serves to explain the effectiveness of MixUp, but also reveals a more general family of MixUp schemes, which we call Untied MixUp. We prove that the family of Untied MixUp schemes is equivalent to the entire class of DAT schemes. We establish empirically the existence of Untied Mixup schemes which improve upon MixUp.", "label": "1", "is_selected": "0", "text": "In this work, we explain the working mechanism of MixUp in terms of adversarial training. We introduce a new class of adversarial training schemes, which we refer to as directional adversarial training, or DAT. In a nutshell, a DAT scheme perturbs a training example in the direction of another example but keeps its original label as the training target. We prove that MixUp is equivalent to a special subclass of DAT, in that it has the same expected loss function and corresponds to the same optimization problem asymptotically. This understanding not only serves to explain the effectiveness of MixUp, but also reveals a more general family of MixUp schemes, which we call Untied MixUp. We prove that the family of Untied MixUp schemes is equivalent to the entire class of DAT schemes. We establish empirically the existence of Untied Mixup schemes which improve upon MixUp."}
{"original_text": "This study mainly investigates two decoding problems in neural keyphrase generation: sequence length bias and beam diversity. We introduce an extension of beam search inference based on word-level and n-gram level attention score to adjust and constrain Seq2Seq prediction at test time. Results show that our proposed solution can overcome the algorithm bias to shorter and nearly identical sequences, resulting in a significant improvement of the decoding performance on generating keyphrases that are present and absent in source text.", "label": "1", "is_selected": "0", "text": "This study mainly investigates two decoding problems in neural keyphrase generation: sequence length bias and beam diversity. We introduce an extension of beam search inference based on word-level and n-gram level attention score to adjust and constrain Seq2Seq prediction at test time. Results show that our proposed solution can overcome the algorithm bias to shorter and nearly identical sequences, resulting in a significant improvement of the decoding performance on generating keyphrases that are present and absent in source text."}
{"original_text": "Pufferfish is a Bayesian privacy framework for designing and analyzing privacy mechanisms. It refines differential privacy, the current gold standard in data privacy, by allowing explicit prior knowledge in privacy analysis. Through these privacy frameworks, a number of privacy mechanisms have been developed in literature. In practice, privacy mechanisms often need be modified or adjusted to specific applications. Their privacy risks have to be re-evaluated for different circumstances. Moreover, computing devices only approximate continuous noises through floating-point computation, which is discrete in nature. Privacy proofs can thus be complicated and prone to errors. Such tedious tasks can be burdensome to average data curators. In this paper, we propose an automatic verification technique for Pufferfish privacy. We use hidden Markov models to specify and analyze discretized Pufferfish privacy mechanisms. We show that the Pufferfish verification problem in hidden Markov models is NP-hard. Using Satisfiability Modulo Theories solvers, we propose an algorithm to analyze privacy requirements. We implement our algorithm in a prototypical tool called FAIER, and present several case studies. Surprisingly, our case studies show that naive discretization of well-established privacy mechanisms often fail, witnessed by counterexamples generated by FAIER. In discretized Above Threshold, we show that it results in absolutely no privacy. Finally, we compare our approach with testing based approach on several case studies, and show that our verification technique can be combined with testing based approach for the purpose of (i) efficiently certifying counterexamples and (ii) obtaining a better lower bound for the privacy budget .", "label": "1", "is_selected": "0", "text": "Pufferfish is a Bayesian privacy framework for designing and analyzing privacy mechanisms. It refines differential privacy, the current gold standard in data privacy, by allowing explicit prior knowledge in privacy analysis. Through these privacy frameworks, a number of privacy mechanisms have been developed in literature. In practice, privacy mechanisms often need be modified or adjusted to specific applications. Their privacy risks have to be re-evaluated for different circumstances. Moreover, computing devices only approximate continuous noises through floating-point computation, which is discrete in nature. Privacy proofs can thus be complicated and prone to errors. Such tedious tasks can be burdensome to average data curators. In this paper, we propose an automatic verification technique for Pufferfish privacy. We use hidden Markov models to specify and analyze discretized Pufferfish privacy mechanisms. We show that the Pufferfish verification problem in hidden Markov models is NP-hard. Using Satisfiability Modulo Theories solvers, we propose an algorithm to analyze privacy requirements. We implement our algorithm in a prototypical tool called FAIER, and present several case studies. Surprisingly, our case studies show that naive discretization of well-established privacy mechanisms often fail, witnessed by counterexamples generated by FAIER. In discretized Above Threshold, we show that it results in absolutely no privacy. Finally, we compare our approach with testing based approach on several case studies, and show that our verification technique can be combined with testing based approach for the purpose of (i) efficiently certifying counterexamples and (ii) obtaining a better lower bound for the privacy budget ."}
{"original_text": "Ad hoc electrical networks are formed by connecting power sources and loads without pre-determining the network topology. These systems are well-suited to addressing the lack of electricity in rural areas because they can be assembled and modified by non-expert users without central oversight. There are two core aspects to ad hoc system design: 1) designing source and load units such that the microgrid formed from the arbitrary interconnection of many units is always stable and 2) developing control strategies to autonomously manage the microgrid (i.e., perform power dispatch and voltage regulation) in a decentralized manner and under large uncertainty. To address these challenges we apply a number of nonlinear control techniques - including Brayton-Moser potential theory and primal-dual dynamics - to obtain conditions under which an ad hoc dc microgrid will have a suitable and asymptotically stable equilibrium point. Further, we propose a new decentralized control scheme that coordinates many sources to achieve a specified power dispatch from each. A simulated comparison to previous research is included.", "label": "1", "is_selected": "0", "text": "Ad hoc electrical networks are formed by connecting power sources and loads without pre-determining the network topology. These systems are well-suited to addressing the lack of electricity in rural areas because they can be assembled and modified by non-expert users without central oversight. There are two core aspects to ad hoc system design: 1) designing source and load units such that the microgrid formed from the arbitrary interconnection of many units is always stable and 2) developing control strategies to autonomously manage the microgrid (i.e., perform power dispatch and voltage regulation) in a decentralized manner and under large uncertainty. To address these challenges we apply a number of nonlinear control techniques - including Brayton-Moser potential theory and primal-dual dynamics - to obtain conditions under which an ad hoc dc microgrid will have a suitable and asymptotically stable equilibrium point. Further, we propose a new decentralized control scheme that coordinates many sources to achieve a specified power dispatch from each. A simulated comparison to previous research is included."}
{"original_text": "Device-to-device (D2D) communication underlaying cellular networks allows mobile devices such as smartphones and tablets to use the licensed spectrum allocated to cellular services for direct peer-to-peer transmission. D2D communication can be seen as an extension of ad hoc communication, but with a cellular infrastructure in place for a better integration of D2D with cellular systems and a more reliable communication. It is widely believed that D2D communication will be an enabling technology for 5G networks. Mobile relaying is a key enabling technology for extending the coverage and enhancing the capacity of wireless networks. Relaying can be implemented using either dedicated or multi-hop relays. Dedicated relays, which are specifically designed for relaying, are often used in licensed spectrum but are also suitable for unlicensed spectrum if backhaul is available. On the other hand, multi-hop relays are typically devices such as smartphones and tablets which double up as relays by sharing their own downlink bandwidth with other nearby devices. Such multi-hop relays are suitable for unlicensed spectrum where there is no backhaul available. The focus of the research is in the design, analysis, and optimization of D2D and mobile relaying for wireless networks. Different approaches and technologies will be explored and compared with each other. In particular, the research will concentrate on the design and optimization of D2D and mobile relaying for both licensed and unlicensed spectrum. The study will consider both device-to-device communication and mobile relaying as well as their coexistence in one wireless network. The research is conducted in collaboration with the WirelessKTH research center.", "label": "0", "is_selected": "1", "text": "The aim of this research is to explore the design, analysis, and optimization of D2D and mobile relaying for wireless networks, in particular, unlicensed spectrum where there is no backhaul."}
{"original_text": "Context:The volume of data generated by astronomical surveys is growing rapidly. Traditional analysis techniques in spectroscopy either demand intensive human interaction or are computationally expensive. In this scenario, machine learning, and especially deep learning, techniques are promising alternatives. Aim:We study the applicability of deep learning to spectroscopic surveys, focusing on the estimation of redshifts from spectra and line flux measurements. Methods:We use two data sets from SDSS-III and BOSS and the corresponding redshift and line flux measurements. We train a convolutional neural network to map from the input spectra to the corresponding redshifts. Results:We find that deep learning can provide a fast and accurate estimate of the redshift for SDSS-III spectra with an average error of 0.0025. We also demonstrate that the deep learning model can be used to measure line fluxes, achieving errors of a few percent. We also study the performance of the model on BOSS spectra, which are lower resolution and noisier than SDSS-III spectra, and find that the neural network can provide redshift estimates with an accuracy of 0.02. We also find that the neural network trained on the higher resolution SDSS-III spectra performs better than a network trained on BOSS spectra alone. Conclusions:Deep learning can provide a fast, accurate, and robust method to estimate redshifts and line fluxes from spectra. We provide an open-source implementation of the methods described in this paper at", "label": "0", "is_selected": "1", "text": "Deep learning can provide a fast and accurate estimation of redshifts from spectra and line flux measurements in astronomical surveys, according to a study in the journal Science."}
{"original_text": "This article provides the first procedure for computing a fully data-dependent interval that traps the mixing time t mix of a finite reversible ergodic Markov chain at a prescribed confidence level. The procedure is based on the approach proposed in Cuny and Moulines, but is significantly simpler and more general. In particular, the procedure can be applied to all chains that are geometrically ergodic. The article also discusses the implementation of the procedure for a class of special chains. In this context, the procedure is extremely efficient, in the sense that the data-dependent interval improves very quickly as the chain is observed. In particular, we show that the interval becomes exact after observing O (log n) steps, where n is the number of states of the chain. The efficiency of the procedure is illustrated on a toy example and on a Markov chain modeling a queuing system.", "label": "0", "is_selected": "1", "text": "The mixing time of a finite reversible ergodic Markov chain is known as the mixing time t mix, and it is a well-known property of Markov chains that a given time-dependent interval can be fixed."}
{"original_text": "F-index of a graph is the sum of the cube of the degrees of the vertices. In this paper, we investigate the F-indices of unicyclic graphs by introducing some transformation, and characterize the unicyclic graphs with the first five largest F-indices and the unicyclic graphs with the first two smallest F-indices, respectively.", "label": "1", "is_selected": "0", "text": "F-index of a graph is the sum of the cube of the degrees of the vertices. In this paper, we investigate the F-indices of unicyclic graphs by introducing some transformation, and characterize the unicyclic graphs with the first five largest F-indices and the unicyclic graphs with the first two smallest F-indices, respectively."}
{"original_text": "The contemporary literature on cloud resource allocation is mostly focused on studying the interactions between customers and cloud managers. Nevertheless, the recent growth in the customers' demands and the emergence of private cloud providers (CPs) entice the cloud managers to rent extra resources from the CPs so as to handle their backlogged tasks and attract more customers. This also makes studying the interactions between the cloud managers and the CPs essential. In this paper, we investigate both of the mentioned interactions. For the interactions between customers and cloud managers, we adopt the options-based sequential auctions (OBSAs) to the cloud resource allocation paradigm. As compared to existing works, our framework can handle customers with heterogeneous demands, provide truthfulness as the dominant strategy, enjoy a simple winner determination, and preclude the delayed entrance issue. We also provide the performance analysis of the OBSAs, which is among the first in literature. For the interactions between cloud managers and CPs, we propose an auction-based scheme for resource gathering. Through incorporating the offered prices, we capture the heterogeneous desires of the CPs in leasing their resources. We conduct a comprehensive mathematical analysis of the two markets and identify the bidding strategy of the cloud managers.", "label": "1", "is_selected": "0", "text": "The contemporary literature on cloud resource allocation is mostly focused on studying the interactions between customers and cloud managers. Nevertheless, the recent growth in the customers' demands and the emergence of private cloud providers (CPs) entice the cloud managers to rent extra resources from the CPs so as to handle their backlogged tasks and attract more customers. This also makes studying the interactions between the cloud managers and the CPs essential. In this paper, we investigate both of the mentioned interactions. For the interactions between customers and cloud managers, we adopt the options-based sequential auctions (OBSAs) to the cloud resource allocation paradigm. As compared to existing works, our framework can handle customers with heterogeneous demands, provide truthfulness as the dominant strategy, enjoy a simple winner determination, and preclude the delayed entrance issue. We also provide the performance analysis of the OBSAs, which is among the first in literature. For the interactions between cloud managers and CPs, we propose an auction-based scheme for resource gathering. Through incorporating the offered prices, we capture the heterogeneous desires of the CPs in leasing their resources. We conduct a comprehensive mathematical analysis of the two markets and identify the bidding strategy of the cloud managers."}
{"original_text": "This document serves to complement our website which was developed with the aim of exposing the students to Gaussian Processes (GPs). GPs are non-parametric bayesian regression models that are largely used in the field of machine learning and Bayesian statistics. The objective of this document is to provide a general overview of the theory of Gaussian Processes by introducing its mathematical and statistical basis.", "label": "0", "is_selected": "1", "text": "The aim of this paper is to provide an overview of the theory of Processes by introducing its mathematical and statistical basis to the students in the Department of Statistics, University of Aberdeen."}
{"original_text": "We provide easy and readable GNU OctaveMATLAB code for the simulation of mathematical models described by ordinary differential equations and for the solution of optimal control problems through Pontryagin's maximum principle. We also explain how to use the MATLAB and GNU Octave solvers for ordinary differential equations. We explain how to use the MATLAB and GNU Octave solvers for ordinary differential equations. We provide easy and readable GNU OctaveMATLAB code for the simulation of mathematical models described by ordinary differential equations and for the solution of optimal control problems through Pontryagin's maximum principle.", "label": "0", "is_selected": "0", "text": "We provide easy and readable GNU OctaveMATLAB code for the simulation of mathematical models described by ordinary differential equations and for the solution of optimal control problems through Pontryagin's maximum principle. We also explain how to use the MATLAB and GNU Octave solvers for ordinary differential equations. We explain how to use the MATLAB and GNU Octave solvers for ordinary differential equations. We provide easy and readable GNU OctaveMATLAB code for the simulation of mathematical models described by ordinary differential equations and for the solution of optimal control problems through Pontryagin's maximum principle."}
{"original_text": "The analysis and quantification of sequence complexity is an open problem frequently encountered when defining trajectory prediction benchmarks. In order to enable a more informative assembly of a data basis, an approach for determining a dataset representation in terms of a small set of distinguishable prototypical sub-sequences is proposed. The approach employs a spatial sequence alignment, which enables a following learning vector quantization (LVQ) stage. A first proof of concept on synthetically generated and real-world datasets shows the viability of the approach.", "label": "1", "is_selected": "0", "text": "The analysis and quantification of sequence complexity is an open problem frequently encountered when defining trajectory prediction benchmarks. In order to enable a more informative assembly of a data basis, an approach for determining a dataset representation in terms of a small set of distinguishable prototypical sub-sequences is proposed. The approach employs a spatial sequence alignment, which enables a following learning vector quantization (LVQ) stage. A first proof of concept on synthetically generated and real-world datasets shows the viability of the approach."}
{"original_text": "In this paper, the authors aim to combine the latest state of the art models in image recognition with the best publicly available satellite images to create a system for automatic forest area detection, identification and monitoring. They use the Python programming language and some of the most popular libraries in image recognition and deep learning. The paper uses a neural network that can recognize the trees in the image and extract the location of the forest, and then present the result as a list of coordinates. The researchers used the YOLO (You Only Look Once) deep learning library for image recognition. The YOLO library includes a pre-trained model that allows you to recognise specific objects in an image and identify their location. The YOLO algorithm was trained on a large dataset to recognize thousands of objects, including trees, roads, buildings, etc. The paper explains how the algorithm works in detail. The paper first explains how to get the satellite images. The authors used Google Maps Engine to extract high-resolution images. In the paper, the authors used the Image API of Google Maps Engine. However, the service has been discontinued. You can try alternatives like Tilemill. The authors used a neural network to recognize the trees in the image and extract the location of the forest. This paper used the YOLO algorithm. YOLO is a deep learning framework that uses an end-to-end learning model to directly predict objects from the image. YOLO is trained on a large dataset to recognize thousands of objects, including trees, roads, buildings, etc. The algorithm consists of two parts: a feature extractor and a bounding box regressor. The feature extractor uses a CNN (Convolutional Neural Network) to extract features from the image. Then, the bounding box regressor uses the extracted features to determine the bounding boxes that contain the objects of interest in the image. The paper then explains how to train the model. First, you will need to download the YOLOv2 model from Then, you will need to train the model using the Google Cloud Vision API. The Google Cloud Vision API allows you to perform tasks like image labeling, face recognition and logo detection. You can also create a model using other models from the TensorFlow library. The TensorFlow library is an open source library for machine learning and deep learning. In this paper, the authors trained a model using the pre-trained Inception model. This model is trained to recognize 1,000 different objects in the ImageNet dataset. Once you have created the model, you can run the model and get the bounding boxes for all the objects in the image. You can get the bounding boxes for each object, as well as the class of the object. In this paper, the authors used the SciKit Learn package to get the bounding boxes for each class. You can also use the extracted bounding boxes to train a new model. In this paper, the authors used a simple linear model to predict the location of the forest based on the bounding boxes extracted by the YOLO model. Once you have extracted the bounding boxes for each class, you can use a simple linear model to predict the location of the forest. To create the model, you first need to train the model using a list of the bounding boxes and the corresponding location of the forest. In this paper, the authors used the XGBoost library to train the model. The XGBoost library is an open-source library for building decision tree ensembles. The library is widely used to train large-scale regression and classification models. Once the model is trained, you can use the model to predict the location of the forest in the image. The authors used the predictions to create a map that can be used to track the status of the forest.", "label": "0", "is_selected": "0", "text": "In this paper, the authors aim to combine the latest state of the art models in image recognition with the best publicly available satellite images to create a system for automatic forest area detection, identification and monitoring. They use the Python programming language and some of the most popular libraries in image recognition and deep learning. The paper uses a neural network that can recognize the trees in the image and extract the location of the forest, and then present the result as a list of coordinates. The researchers used the YOLO (You Only Look Once) deep learning library for image recognition. The YOLO library includes a pre-trained model that allows you to recognise specific objects in an image and identify their location. The YOLO algorithm was trained on a large dataset to recognize thousands of objects, including trees, roads, buildings, etc. The paper explains how the algorithm works in detail. The paper first explains how to get the satellite images. The authors used Google Maps Engine to extract high-resolution images. In the paper, the authors used the Image API of Google Maps Engine. However, the service has been discontinued. You can try alternatives like Tilemill. The authors used a neural network to recognize the trees in the image and extract the location of the forest. This paper used the YOLO algorithm. YOLO is a deep learning framework that uses an end-to-end learning model to directly predict objects from the image. YOLO is trained on a large dataset to recognize thousands of objects, including trees, roads, buildings, etc. The algorithm consists of two parts: a feature extractor and a bounding box regressor. The feature extractor uses a CNN (Convolutional Neural Network) to extract features from the image. Then, the bounding box regressor uses the extracted features to determine the bounding boxes that contain the objects of interest in the image. The paper then explains how to train the model. First, you will need to download the YOLOv2 model from Then, you will need to train the model using the Google Cloud Vision API. The Google Cloud Vision API allows you to perform tasks like image labeling, face recognition and logo detection. You can also create a model using other models from the TensorFlow library. The TensorFlow library is an open source library for machine learning and deep learning. In this paper, the authors trained a model using the pre-trained Inception model. This model is trained to recognize 1,000 different objects in the ImageNet dataset. Once you have created the model, you can run the model and get the bounding boxes for all the objects in the image. You can get the bounding boxes for each object, as well as the class of the object. In this paper, the authors used the SciKit Learn package to get the bounding boxes for each class. You can also use the extracted bounding boxes to train a new model. In this paper, the authors used a simple linear model to predict the location of the forest based on the bounding boxes extracted by the YOLO model. Once you have extracted the bounding boxes for each class, you can use a simple linear model to predict the location of the forest. To create the model, you first need to train the model using a list of the bounding boxes and the corresponding location of the forest. In this paper, the authors used the XGBoost library to train the model. The XGBoost library is an open-source library for building decision tree ensembles. The library is widely used to train large-scale regression and classification models. Once the model is trained, you can use the model to predict the location of the forest in the image. The authors used the predictions to create a map that can be used to track the status of the forest."}
{"original_text": "Regular decompositions are necessary for most superpixel-based object recognition or tracking applications. So far in the literature, the regularity or compactness of a superpixel shape is mainly measured by its boundary complexity, such as, the number of edges, or the boundary length. However, such a measure is incapable of detecting large holes and irregularities inside the superpixel. In this paper, we propose to use the Wasserstein distance as a measure of superpixel compactness. In particular, we show that a Wasserstein regularization term can be added to the classical entropy-based superpixel segmentation model. By doing so, we are able to regulate the superpixel compactness and the size distribution at the same time. The corresponding regularized model is then solved efficiently via a successive over-relaxation method. Experimental results demonstrate the efficiency of the proposed method in improving superpixel regularity and contour compactness. The code of our method is publicly available at", "label": "0", "is_selected": "0", "text": "Regular decompositions are necessary for most superpixel-based object recognition or tracking applications. So far in the literature, the regularity or compactness of a superpixel shape is mainly measured by its boundary complexity, such as, the number of edges, or the boundary length. However, such a measure is incapable of detecting large holes and irregularities inside the superpixel. In this paper, we propose to use the Wasserstein distance as a measure of superpixel compactness. In particular, we show that a Wasserstein regularization term can be added to the classical entropy-based superpixel segmentation model. By doing so, we are able to regulate the superpixel compactness and the size distribution at the same time. The corresponding regularized model is then solved efficiently via a successive over-relaxation method. Experimental results demonstrate the efficiency of the proposed method in improving superpixel regularity and contour compactness. The code of our method is publicly available at"}
{"original_text": "Image inpainting is one of the most challenging tasks in computer vision. Recently, generative-based image inpainting methods have been shown to produce visually plausible images. However, they still have difficulties to generate the correct structures and colors as the masked region grows large. This drawback is due to the training stability issue of the generative models. This work introduces a new curriculum-style training approach in the context of image inpainting. The proposed method increases the masked region size progressively in training time, during test time the user gives variable size and multiple holes at arbitrary locations. Incorporating such an approach in GANs may stabilize the training and provides better color consistencies and captures object continuities. We validate our approach on the MSCOCO and CelebA datasets. We report qualitative and quantitative comparisons of our training approach in different models.", "label": "1", "is_selected": "0", "text": "Image inpainting is one of the most challenging tasks in computer vision. Recently, generative-based image inpainting methods have been shown to produce visually plausible images. However, they still have difficulties to generate the correct structures and colors as the masked region grows large. This drawback is due to the training stability issue of the generative models. This work introduces a new curriculum-style training approach in the context of image inpainting. The proposed method increases the masked region size progressively in training time, during test time the user gives variable size and multiple holes at arbitrary locations. Incorporating such an approach in GANs may stabilize the training and provides better color consistencies and captures object continuities. We validate our approach on the MSCOCO and CelebA datasets. We report qualitative and quantitative comparisons of our training approach in different models."}
{"original_text": "Most computational models of analogy assume they are given a delineated source domain and often a specified target domain. These systems do not address how analogs can be isolated from other cases, which is a first-order challenge. We propose a mechanism whereby analogies can be located and evaluated without the need to specify either source or target domains in advance. Our approach is based on a model of similarity which is used to rank cases according to their relatedness to an example. The notion of relatedness is formalized in terms of cues, which are specific elements that can be used to express similarity. Cues are used to characterize the similarity between cases and a target example. A series of experiments is presented which show that the method is effective in finding the most similar cases to a target example. Citation: Sack, W. and Brglez, P. (2001) Finding the right analogy. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, 189-196.", "label": "0", "is_selected": "1", "text": "The search for the right analogy between two or more cases has become a major problem in computer science and artificial intelligence (AI), particularly in the area of machine learning."}
{"original_text": "This paper establishes theoretical bonafides for implicit concurrent multivariate effect evaluation - implicit concurrency 1 footnote 1 1 footnote 1 for short - a broad and versatile computational learning efficiency technique for minimizing memory usage by exploiting the inherent data structure of a sequential multivariate effect evaluation program. This paper provides the theoretical groundwork for analyzing the asymptotic memory usage characteristics of implicit concurrency for evaluating multivariate effects. Specifically, this paper shows that the worst-case memory usage characteristics of a program that is evaluated via implicit concurrency is upper bounded by a multiple of the worst-case memory usage characteristics of the original sequential program. This paper establishes theoretical bonafides for implicit concurrent multivariate effect evaluation - implicit concurrency 1 footnote 1 1 footnote 1 for short - a broad and versatile computational learning efficiency technique for minimizing memory usage by exploiting the inherent data structure of a sequential multivariate effect evaluation program. This paper provides the theoretical groundwork for analyzing the asymptotic memory usage characteristics of implicit concurrency for evaluating multivariate effects. Specifically, this paper shows that the worst-case memory usage characteristics of a program that is evaluated via implicit concurrency is upper bounded by a multiple of the worst-case memory usage characteristics of the original sequential program. This paper also shows that implicit concurrency is asymptotically equivalent to a sequential multivariate effect evaluation program in terms of the asymptotic complexity of the multivariate effect evaluation program. A multivariate effect evaluation program is a program that is based on a single-source multiple-target learning architecture. In the single-source multiple-target learning architecture, a single source is used to create multiple targets. In a multivariate effect evaluation program, the targets are a set of features, and the source is a set of inputs. The multivariate effect evaluation program evaluates the feature set on the input set. As a concrete example, a multivariate effect evaluation program can be used to evaluate a set of target features on a set of source inputs in a supervised learning framework. In a supervised learning framework, the input set is a set of feature-value pairs. Each pair represents a source input. Each pair is associated with a set of target features. The target features are the features that are being evaluated. The goal of the multivariate effect evaluation program is to use the feature-value pairs to compute the values of the target features. As a second concrete example, a multivariate effect evaluation program can be used to evaluate a set of target features on a set of source inputs in a reinforcement learning framework. In a reinforcement learning framework, the input set is a set of states. Each state represents a source input. Each state is associated with a set of target features. The target features are the features that are being evaluated. The goal of the multivariate effect evaluation program is to use the states to compute the values of the target features. A key feature of a multivariate effect evaluation program is that the number of values that are computed is proportional to the number of source inputs. If the number of source inputs is X, the multivariate effect evaluation program will compute a total of X values. If the number of source inputs is X, the multivariate effect evaluation program will compute a total of X values. This is because, for each source input, the multivariate effect evaluation program will compute the values of the target features that are associated with the source input. There are several reasons why the number of source inputs is important. The number of source inputs impacts the memory usage characteristics of the program, the run-time performance of the program, the power consumption characteristics of the program, the energy usage characteristics of the program, the complexity of the program, and the bandwidth usage characteristics of the program. The memory usage characteristics of the program - the amount of memory that is used by the program - can be understood in terms of the memory usage characteristics of the input set. The memory usage characteristics of the input set is the total size of the input set in bytes. If the number of source inputs is X, the memory usage characteristics of the input set is X Y, where Y is the size of the source input in bytes. The run-time performance of the program can be understood in terms of the run-time performance of the input set. The run-time performance of the input set is the total run-time that is required to process the input set. If the number of source inputs is X, the run-time performance of the input set is X Z, where Z is the run-time that is required to process a single source input. The power consumption characteristics of the program - the total amount of power that is consumed by the program - can be understood in terms of the power consumption characteristics of the input set. The", "label": "0", "is_selected": "0", "text": "This paper establishes theoretical bonafides for implicit concurrent multivariate effect evaluation - implicit concurrency 1 footnote 1 1 footnote 1 for short - a broad and versatile computational learning efficiency technique for minimizing memory usage by exploiting the inherent data structure of a sequential multivariate effect evaluation program. This paper provides the theoretical groundwork for analyzing the asymptotic memory usage characteristics of implicit concurrency for evaluating multivariate effects. Specifically, this paper shows that the worst-case memory usage characteristics of a program that is evaluated via implicit concurrency is upper bounded by a multiple of the worst-case memory usage characteristics of the original sequential program. This paper establishes theoretical bonafides for implicit concurrent multivariate effect evaluation - implicit concurrency 1 footnote 1 1 footnote 1 for short - a broad and versatile computational learning efficiency technique for minimizing memory usage by exploiting the inherent data structure of a sequential multivariate effect evaluation program. This paper provides the theoretical groundwork for analyzing the asymptotic memory usage characteristics of implicit concurrency for evaluating multivariate effects. Specifically, this paper shows that the worst-case memory usage characteristics of a program that is evaluated via implicit concurrency is upper bounded by a multiple of the worst-case memory usage characteristics of the original sequential program. This paper also shows that implicit concurrency is asymptotically equivalent to a sequential multivariate effect evaluation program in terms of the asymptotic complexity of the multivariate effect evaluation program. A multivariate effect evaluation program is a program that is based on a single-source multiple-target learning architecture. In the single-source multiple-target learning architecture, a single source is used to create multiple targets. In a multivariate effect evaluation program, the targets are a set of features, and the source is a set of inputs. The multivariate effect evaluation program evaluates the feature set on the input set. As a concrete example, a multivariate effect evaluation program can be used to evaluate a set of target features on a set of source inputs in a supervised learning framework. In a supervised learning framework, the input set is a set of feature-value pairs. Each pair represents a source input. Each pair is associated with a set of target features. The target features are the features that are being evaluated. The goal of the multivariate effect evaluation program is to use the feature-value pairs to compute the values of the target features. As a second concrete example, a multivariate effect evaluation program can be used to evaluate a set of target features on a set of source inputs in a reinforcement learning framework. In a reinforcement learning framework, the input set is a set of states. Each state represents a source input. Each state is associated with a set of target features. The target features are the features that are being evaluated. The goal of the multivariate effect evaluation program is to use the states to compute the values of the target features. A key feature of a multivariate effect evaluation program is that the number of values that are computed is proportional to the number of source inputs. If the number of source inputs is X, the multivariate effect evaluation program will compute a total of X values. If the number of source inputs is X, the multivariate effect evaluation program will compute a total of X values. This is because, for each source input, the multivariate effect evaluation program will compute the values of the target features that are associated with the source input. There are several reasons why the number of source inputs is important. The number of source inputs impacts the memory usage characteristics of the program, the run-time performance of the program, the power consumption characteristics of the program, the energy usage characteristics of the program, the complexity of the program, and the bandwidth usage characteristics of the program. The memory usage characteristics of the program - the amount of memory that is used by the program - can be understood in terms of the memory usage characteristics of the input set. The memory usage characteristics of the input set is the total size of the input set in bytes. If the number of source inputs is X, the memory usage characteristics of the input set is X Y, where Y is the size of the source input in bytes. The run-time performance of the program can be understood in terms of the run-time performance of the input set. The run-time performance of the input set is the total run-time that is required to process the input set. If the number of source inputs is X, the run-time performance of the input set is X Z, where Z is the run-time that is required to process a single source input. The power consumption characteristics of the program - the total amount of power that is consumed by the program - can be understood in terms of the power consumption characteristics of the input set. The"}
{"original_text": "The kinetic battery model is a popular model of the dynamic behavior of a conventional battery, useful to predict or optimize the time until battery depletion. The model however lacks a physical interpretation, since the reaction rate constants are not connected to the material properties of the battery. In this paper, we present a new mathematical model that connects the reaction rates in the kinetic battery model with the intrinsic reaction rates for the chemical reactions that take place in the battery. In this way, we obtain a closed system of equations that describes the dynamic behavior of a battery. The model is physically justified, and is validated by comparing its output with that of a real battery.", "label": "0", "is_selected": "0", "text": "The kinetic battery model is a popular model of the dynamic behavior of a conventional battery, useful to predict or optimize the time until battery depletion. The model however lacks a physical interpretation, since the reaction rate constants are not connected to the material properties of the battery. In this paper, we present a new mathematical model that connects the reaction rates in the kinetic battery model with the intrinsic reaction rates for the chemical reactions that take place in the battery. In this way, we obtain a closed system of equations that describes the dynamic behavior of a battery. The model is physically justified, and is validated by comparing its output with that of a real battery."}
{"original_text": "In this paper we present a novel rule-based approach for Runtime Verification of FLTL properties over finite but expanding traces. Our system exploits Horn clauses in implication form and relies on a tableau-based verification procedure. We describe the algorithms that we use to obtain FLTL tableaux from a formula and a trace. We implemented the system and tested it on a number of examples. The results show that it is feasible to verify FLTL properties over finite traces.", "label": "0", "is_selected": "1", "text": "runtime verification of finite traces of floating-line properties (FLTL) is an important problem in programming languages such as C, C++, and Java."}
{"original_text": "The long-standing byzantine agreement problem gets more attention in recent years due to the increasing demand for scalable geo-replicated Byzantine state machine replication (SMR) systems (e.g., Blockchains). To date, the key bottleneck of such systems is the communication cost of the byzantine agreement they employ as a building block, which motivates many researchers to search for low-communication byzantine agreement protocols. The conventional approach is to design deterministic protocols in the eventually synchronous communication model that are optimized to reduce the communication cost after the global stabilization time (GST). In this paper, we challenge the conventional approach and argue it is not the best fit for scalable SMR systems since it might induce an unbounded communication cost during asynchronous periods before GST, which we prove to be inherent. Instead, we forgo eventual synchrony and propose a different approach that hopes for the best (synchrony) but prepares for the worst (asynchrony). Accordingly, we design an optimistic protocol that first tries to reach an agreement via an efficient deterministic algorithm that relies on synchrony for termination, and then, only if an agreement was not reached due to asynchrony, the protocol uses a randomized asynchronous algorithm for fallback that guarantees termination with probability 1. Although randomized asynchronous algorithms are considered to be costly, we design our solution to pay this cost only when an equivalent cost has already been paid while unsuccessfully trying the synchronous protocol. We formally prove that our protocol achieves optimal communication complexity under all network conditions and failure scenarios. We first prove a lower bound of O (f t t) for synchronous deterministic agreement protocols, where t is the failure threshold, and f is the actual number of failures. Then, we present a tight upper bound and use it for our synchronous part. Finally, for the asynchronous fallback, we use a variant of the (optimal) VABA protocol, which we reconstruct to safely combine it with the synchronous part.", "label": "1", "is_selected": "0", "text": "The long-standing byzantine agreement problem gets more attention in recent years due to the increasing demand for scalable geo-replicated Byzantine state machine replication (SMR) systems (e.g., Blockchains). To date, the key bottleneck of such systems is the communication cost of the byzantine agreement they employ as a building block, which motivates many researchers to search for low-communication byzantine agreement protocols. The conventional approach is to design deterministic protocols in the eventually synchronous communication model that are optimized to reduce the communication cost after the global stabilization time (GST). In this paper, we challenge the conventional approach and argue it is not the best fit for scalable SMR systems since it might induce an unbounded communication cost during asynchronous periods before GST, which we prove to be inherent. Instead, we forgo eventual synchrony and propose a different approach that hopes for the best (synchrony) but prepares for the worst (asynchrony). Accordingly, we design an optimistic protocol that first tries to reach an agreement via an efficient deterministic algorithm that relies on synchrony for termination, and then, only if an agreement was not reached due to asynchrony, the protocol uses a randomized asynchronous algorithm for fallback that guarantees termination with probability 1. Although randomized asynchronous algorithms are considered to be costly, we design our solution to pay this cost only when an equivalent cost has already been paid while unsuccessfully trying the synchronous protocol. We formally prove that our protocol achieves optimal communication complexity under all network conditions and failure scenarios. We first prove a lower bound of O (f t t) for synchronous deterministic agreement protocols, where t is the failure threshold, and f is the actual number of failures. Then, we present a tight upper bound and use it for our synchronous part. Finally, for the asynchronous fallback, we use a variant of the (optimal) VABA protocol, which we reconstruct to safely combine it with the synchronous part."}
{"original_text": "Breast cancer screening is one of the most common radiological tasks with over 39 million exams performed each year. While breast cancer screening has been one of the most studied medical imaging applications of artificial intelligence, the development and evaluation of the algorithms are hindered due to the lack of well-annotated large-scale publicly available datasets. This is particularly an issue for digital breast tomosynthesis (DBT) which is a relatively new breast cancer screening modality. We have curated and made publicly available a large-scale dataset of digital breast tomosynthesis images. It contains 22,032 reconstructed DBT volumes belonging to 5,610 studies from 5,060 patients. This included four groups: (1) 5,129 normal studies, (2) 280 studies where additional imaging was needed but no biopsy was performed, (3) 112 benign biopsied studies, and (4) 89 studies with cancer. Our dataset included masses and architectural distortions which were annotated by two experienced radiologists. Additionally, we developed a single-phase deep learning detection model and tested it using our dataset to serve as a baseline for future research. Our model reached a sensitivity of 65 at 2 false positives per breast. Our large, diverse, and highly-curated dataset will facilitate development and evaluation of AI algorithms for breast cancer screening through providing data for training as well as common set of cases for model validation. The performance of the model developed in our study shows that the task remains challenging and will serve as a baseline for future model development. Keywords: digital breast tomosynthesis; deep learning; detection", "label": "1", "is_selected": "0", "text": "Breast cancer screening is one of the most common radiological tasks with over 39 million exams performed each year. While breast cancer screening has been one of the most studied medical imaging applications of artificial intelligence, the development and evaluation of the algorithms are hindered due to the lack of well-annotated large-scale publicly available datasets. This is particularly an issue for digital breast tomosynthesis (DBT) which is a relatively new breast cancer screening modality. We have curated and made publicly available a large-scale dataset of digital breast tomosynthesis images. It contains 22,032 reconstructed DBT volumes belonging to 5,610 studies from 5,060 patients. This included four groups: (1) 5,129 normal studies, (2) 280 studies where additional imaging was needed but no biopsy was performed, (3) 112 benign biopsied studies, and (4) 89 studies with cancer. Our dataset included masses and architectural distortions which were annotated by two experienced radiologists. Additionally, we developed a single-phase deep learning detection model and tested it using our dataset to serve as a baseline for future research. Our model reached a sensitivity of 65 at 2 false positives per breast. Our large, diverse, and highly-curated dataset will facilitate development and evaluation of AI algorithms for breast cancer screening through providing data for training as well as common set of cases for model validation. The performance of the model developed in our study shows that the task remains challenging and will serve as a baseline for future model development. Keywords: digital breast tomosynthesis; deep learning; detection"}
{"original_text": "User intent detection plays a critical role in question-answering and dialog systems. Most previous works treat intent detection as a classification problem where utterances are labeled with predefined intents. However, it is labor-intensive and time-consuming to label users' utterances as intents are diversely expressed and novel intents will continually be involved. Instead, we study the zero-shot intent detection problem, which aims to detect emerging user intents where no labeled utterances are currently available. We propose two capsule-based architectures: IntentCapsNet that extracts semantic features from utterances and aggregates them to discriminate existing intents, and IntentCapsNet-ZSL which gives IntentCapsNet the zero-shot learning ability to discriminate emerging intents via knowledge transfer from existing intents. Experiments on two real-world datasets show that our model not only can better discriminate diversely expressed existing intents, but is also able to discriminate emerging intents when no labeled utterances are available.", "label": "1", "is_selected": "0", "text": "User intent detection plays a critical role in question-answering and dialog systems. Most previous works treat intent detection as a classification problem where utterances are labeled with predefined intents. However, it is labor-intensive and time-consuming to label users' utterances as intents are diversely expressed and novel intents will continually be involved. Instead, we study the zero-shot intent detection problem, which aims to detect emerging user intents where no labeled utterances are currently available. We propose two capsule-based architectures: IntentCapsNet that extracts semantic features from utterances and aggregates them to discriminate existing intents, and IntentCapsNet-ZSL which gives IntentCapsNet the zero-shot learning ability to discriminate emerging intents via knowledge transfer from existing intents. Experiments on two real-world datasets show that our model not only can better discriminate diversely expressed existing intents, but is also able to discriminate emerging intents when no labeled utterances are available."}
{"original_text": "Given a positive integer k, a k -dominating set in a graph G is a set of vertices such that every vertex not in the set has at least k vertices in the set as neighbors. The k -domination number of G, (k) (G), is the minimum cardinality of a k -dominating set in G. A characterization is given for a graph G to have (3) (G) (k) (G) for all k 3. This result is used to show that (k) (G) (3) (G) if G is planar, 3-connected, and all blocks of G are either cycles or triangles. 2014 S. Kollipara. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.", "label": "0", "is_selected": "0", "text": "Given a positive integer k, a k -dominating set in a graph G is a set of vertices such that every vertex not in the set has at least k vertices in the set as neighbors. The k -domination number of G, (k) (G), is the minimum cardinality of a k -dominating set in G. A characterization is given for a graph G to have (3) (G) (k) (G) for all k 3. This result is used to show that (k) (G) (3) (G) if G is planar, 3-connected, and all blocks of G are either cycles or triangles. 2014 S. Kollipara. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited."}
{"original_text": "In this paper we present mono-stixels, a compact environment representation specially designed for dynamic street scenes. Mono-stixels are a novel approach to estimate stixels from a monocular camera sequence instead of stereo cameras. Stixels are 3D voxel grids which are semantically labelled with the information of free space, occupied space, height and semantic class. They represent the environment in a compact way and are optimised for an efficient processing. In addition, we propose a new approach to segment and label cars and pedestrians. The contribution of this paper is threefold: we propose a novel environment representation called mono-stixels, we describe a new algorithm to segment and label cars and pedestrians and we present a novel approach to estimate the stixel height. We demonstrate our approach on publicly available datasets and show that it is very competitive to the state-of-the-art.", "label": "0", "is_selected": "1", "text": "The spatio-temporal reconstruction of dynamic street scenes is a major challenge in the area of 3D modelling and artificial intelligence. The contribution of this paper is threefold: we propose a novel environment representation called mono-stixels, we describe a new algorithm to segment and label cars and pedestrians and we present a novel approach to estimate the stixel height."}
{"original_text": "Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm. As such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model. Unfortunately, developing and maintaining a rich dialogue corpus is costly. We propose a complementary approach to generate new dialogues via a neural language generator. The proposed generator is able to capture the content semantics of an utterance and use it to generate a new response. We propose a simple architecture called CopyCat, which relies on copying previously encountered content and adapting it to the current situation. We demonstrate the capability of CopyCat by showing that it can generate responses with reasonable coherence. Moreover, we show that it is possible to train CopyCat in an unsupervised manner using pseudo-rewards. Copyright 2017 Google Inc. All Rights Reserved.", "label": "0", "is_selected": "0", "text": "Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm. As such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model. Unfortunately, developing and maintaining a rich dialogue corpus is costly. We propose a complementary approach to generate new dialogues via a neural language generator. The proposed generator is able to capture the content semantics of an utterance and use it to generate a new response. We propose a simple architecture called CopyCat, which relies on copying previously encountered content and adapting it to the current situation. We demonstrate the capability of CopyCat by showing that it can generate responses with reasonable coherence. Moreover, we show that it is possible to train CopyCat in an unsupervised manner using pseudo-rewards. Copyright 2017 Google Inc. All Rights Reserved."}
{"original_text": "The rapidly growing field of network analytics requires data sets for use in evaluation. Real world data often lack truth and simulated data lack narrative fidelity or statistical generality. This paper presents a novel, mixed-membership, agent-based simulation model to generate activity data with narrative power while providing statistical diversity through random draws. The model generalizes to a variety of network activity types such as Internet and cellular communications, human mobility, and social network interactions. The simulated actions over all agents can then drive an application specific observational model to render measurements as one would collect in real-world experiments. We apply this framework to human mobility and demonstrate its utility in generating high fidelity traffic data for network analytics. 1 footnote 1 1 footnote 1 This work is sponsored by the Assistant Secretary of Defense for Research Engineering under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the author and are not necessarily endorsed by the United States Government", "label": "1", "is_selected": "0", "text": "The rapidly growing field of network analytics requires data sets for use in evaluation. Real world data often lack truth and simulated data lack narrative fidelity or statistical generality. This paper presents a novel, mixed-membership, agent-based simulation model to generate activity data with narrative power while providing statistical diversity through random draws. The model generalizes to a variety of network activity types such as Internet and cellular communications, human mobility, and social network interactions. The simulated actions over all agents can then drive an application specific observational model to render measurements as one would collect in real-world experiments. We apply this framework to human mobility and demonstrate its utility in generating high fidelity traffic data for network analytics. 1 footnote 1 1 footnote 1 This work is sponsored by the Assistant Secretary of Defense for Research Engineering under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the author and are not necessarily endorsed by the United States Government"}
{"original_text": "In this manuscript, we investigate the abrupt breakdown behavior of coupled distribution grids under load growth. This scenario mimics the ever-increasing customer demand and the foreseen introduction of energy hubs, which can create challenges for distribution grid operators in the future. The system is modeled using a novel method that considers both the distribution grid and the underlying transmission grid. The method combines elements from (transient) power flow analysis and the time-domain simulation of electromagnetic transients. The results are presented in a parametric study and show how the load growth and the introduction of an energy hub can affect the grid performance. Wagener, Simon Weber, Michael: Modeling of Abrupt Breakdown Scenarios of Coupled Distribution Grids under Load Growth. 2019.", "label": "0", "is_selected": "1", "text": "Distribution grids play an important role in the energy supply chain, but their performance can be affected by a wide range of factors, including load growth, the introduction of energy hubs, and the nature of the transmission grid."}
{"original_text": "Reinforcement learning with function approximation can be unstable and even divergent, especially when combined with off-policy learning and Bellman updates. In deep reinforcement learning, these issues have been dealt with by careful choice of parameters or by using reward shaping. We introduce a simple algorithm for constructing a family of unbiased and consistent off-policy function approximators. We provide a mathematical analysis of our algorithm and show that it can be used in reinforcement learning to obtain consistent and robust estimates of value functions. In off-policy learning, our algorithm is equivalent to a Bayesian bootstrap scheme in which the future value function is treated as a random variable. We show that this scheme is equivalent to the Thompson sampling algorithm of Tu et al. (2016) when applied to reinforcement learning. In policy evaluation, the algorithm corresponds to a modified importance sampling scheme.", "label": "0", "is_selected": "1", "text": "In this paper, we introduce an algorithm for constructing a family of unbiased and consistent off-policy function approximators for deep reinforcement learning and policy evaluation."}
{"original_text": "An r -identifying code in a graph G (V, E) is a subset C V such that for each u V the intersection of C and the ball of radius r centered at u is non-empty and unique. Previously, r -identifying codes have been studied in various grids. In particular, it has been shown that there exists a 2 -identifying code in the hexagonal grid with density 4 19 and that there are no 2 -identifying codes with density smaller than 2 11. Recently, the lower bound has been improved to 1 5 by Martin and Stanton (2010). In this paper, we prove that the 2 -identifying code with density 4 19 is optimal, i.e. that there does not exist a 2 -identifying code in the hexagonal grid with smaller density.", "label": "1", "is_selected": "0", "text": "An r -identifying code in a graph G (V, E) is a subset C V such that for each u V the intersection of C and the ball of radius r centered at u is non-empty and unique. Previously, r -identifying codes have been studied in various grids. In particular, it has been shown that there exists a 2 -identifying code in the hexagonal grid with density 4 19 and that there are no 2 -identifying codes with density smaller than 2 11. Recently, the lower bound has been improved to 1 5 by Martin and Stanton (2010). In this paper, we prove that the 2 -identifying code with density 4 19 is optimal, i.e. that there does not exist a 2 -identifying code in the hexagonal grid with smaller density."}
{"original_text": "Reo is an interaction-centric model of concurrency for compositional specification of communication and coordination protocols. Formal verification tools exist to ensure correctness and compliance of protocols specified in Reo, which may be written as standalone protocols, or as components of large distributed systems. In this paper, we show how one can specify hierarchical communication protocols in Reo, and use verification tools to ensure their correctness. We describe the design of a distributed architecture for high-performance computing, and its formal verification by means of Reo model-checking.", "label": "0", "is_selected": "1", "text": "We show how one can specify hierarchical communication protocols in Reo, and use formal verification tools to ensure their correctness in a distributed architecture for high-performance computing."}
{"original_text": "We present a novel family of C 1 quadrilateral finite elements, which define global C 1 spaces over a general quadrilateral mesh with vertices of arbitrary valency. The elements extend the construction by Brenner and Sung, which is based on polynomial elements of tensor-product degree p 6, to all degrees p 3. Thus, we call the family of C 1 finite elements Brenner-Sung quadrilaterals. The proposed C 1 quadrilateral can be seen as a special case of the Argyris isogeometric element of. The quadrilateral elements possess similar degrees of freedom as the classical Argyris triangles. Just as for the Argyris triangle, we additionally impose C 2 continuity at the vertices. In this paper we focus on the lower degree cases, not covered in, that may be desirable for their lower computational cost and better conditioning of the basis: We consider indeed the polynomial quadrilateral of (bi degree 5, and the polynomial degrees p 3 and p 4 by employing a splitting into x 3 3 or x 2 2 polynomial pieces, respectively. The proposed elements reproduce polynomials of total degree p. We show that the space provides optimal approximation order. Due to the interpolation properties, the error bounds are local on each element. In addition, we describe the construction of a simple, local basis and give for p {3, 4, 5 } explicit formulas for the Bezier or B-spline coefficients of the basis functions. Numerical experiments by solving the biharmonic equation demonstrate the potential of the proposed C 1 quadrilateral finite element for the numerical analysis of fourth order problems, also indicating that (for p 5) the proposed element performs comparable or in general even better than the Argyris triangle with respect to the number of degrees of freedom.", "label": "1", "is_selected": "0", "text": "We present a novel family of C 1 quadrilateral finite elements, which define global C 1 spaces over a general quadrilateral mesh with vertices of arbitrary valency. The elements extend the construction by Brenner and Sung, which is based on polynomial elements of tensor-product degree p 6, to all degrees p 3. Thus, we call the family of C 1 finite elements Brenner-Sung quadrilaterals. The proposed C 1 quadrilateral can be seen as a special case of the Argyris isogeometric element of. The quadrilateral elements possess similar degrees of freedom as the classical Argyris triangles. Just as for the Argyris triangle, we additionally impose C 2 continuity at the vertices. In this paper we focus on the lower degree cases, not covered in, that may be desirable for their lower computational cost and better conditioning of the basis: We consider indeed the polynomial quadrilateral of (bi degree 5, and the polynomial degrees p 3 and p 4 by employing a splitting into x 3 3 or x 2 2 polynomial pieces, respectively. The proposed elements reproduce polynomials of total degree p. We show that the space provides optimal approximation order. Due to the interpolation properties, the error bounds are local on each element. In addition, we describe the construction of a simple, local basis and give for p {3, 4, 5 } explicit formulas for the Bezier or B-spline coefficients of the basis functions. Numerical experiments by solving the biharmonic equation demonstrate the potential of the proposed C 1 quadrilateral finite element for the numerical analysis of fourth order problems, also indicating that (for p 5) the proposed element performs comparable or in general even better than the Argyris triangle with respect to the number of degrees of freedom."}
{"original_text": "Currently, self-driving cars rely greatly on the Global Positioning System (GPS) infrastructure, albeit there is an increasing demand for alternative methods for GPS-denied environments. One of them is known as localization, which refers to the estimation of a vehicle's position. In this paper, we propose a localization system that integrates the estimation of the vehicle's position with other vehicle data such as the steering angle and the speed. This system is based on vehicle-to-vehicle communication. We provide an evaluation of the error by running different simulations and a real implementation in the emulator CARLA. We are interested in the topic of self-driving cars, autonomous vehicles, autonomous driving, vehicle safety, and vehicle to vehicle communication. More specifically, we are interested in the estimation of the vehicle's position. We proposed a localization system that integrates the estimation of the vehicle's position with other vehicle data such as the steering angle and the speed. We evaluated the error by running different simulations and a real implementation in the emulator CARLA. We provide the entire source code of our implementation in CARLA, a video of the implementation in CARLA, and a detailed description of our methodology, including the results of the experiments. This work was funded by the Joint Program for the Promotion of Innovation of the National Autonomous University of Mexico (UNAM) and the Federal Government (PAPIT-UNAM-DGAPA).", "label": "0", "is_selected": "0", "text": "Currently, self-driving cars rely greatly on the Global Positioning System (GPS) infrastructure, albeit there is an increasing demand for alternative methods for GPS-denied environments. One of them is known as localization, which refers to the estimation of a vehicle's position. In this paper, we propose a localization system that integrates the estimation of the vehicle's position with other vehicle data such as the steering angle and the speed. This system is based on vehicle-to-vehicle communication. We provide an evaluation of the error by running different simulations and a real implementation in the emulator CARLA. We are interested in the topic of self-driving cars, autonomous vehicles, autonomous driving, vehicle safety, and vehicle to vehicle communication. More specifically, we are interested in the estimation of the vehicle's position. We proposed a localization system that integrates the estimation of the vehicle's position with other vehicle data such as the steering angle and the speed. We evaluated the error by running different simulations and a real implementation in the emulator CARLA. We provide the entire source code of our implementation in CARLA, a video of the implementation in CARLA, and a detailed description of our methodology, including the results of the experiments. This work was funded by the Joint Program for the Promotion of Innovation of the National Autonomous University of Mexico (UNAM) and the Federal Government (PAPIT-UNAM-DGAPA)."}
{"original_text": "The classical constant-sum 'silent duel' game had two antagonistic marksmen walking towards each other. A more friendly formulation has two equally skilled marksmen approaching targets at which they may silently fire their guns; the winner is the one who hits first. Such a game has no unique Nash equilibrium. We show that this game can be generalized to one in which the two players approach each other at different speeds, with different rates of firing, and with different probabilities of hitting the targets. We characterize the Nash equilibria in this generalized game. We find that there is a unique symmetric Nash equilibrium when the rate of firing is a linear function of the distance between the two players. Finally, we examine the extent to which the symmetric Nash equilibrium is robust when players deviate from this special case. Todd Kaplan Rex Pjesky, 2012. \"Silent Duel Games with Time-varying Probabilities of Hitting Targets,\" Journal of Conflict Resolution, Peace Science Society (International), vol. 56 (1), pages 1-18, January. Hillier, Derek Rust, John, 1997. \"Silent duel games,\" Economics Letters, Elsevier, vol. 56 (2), pages 167-173, August. Yan-Ni Fu Alesha Doan, 2013. \"The Potential of Biosurveillance Programs for the Infection Control of Influenza Pandemics,\" The B.E. Journal of Economic Analysis Policy, De Gruyter, vol. 13 (3), pages 1-27, June. All material on this site has been provided by the respective publishers and authors. You can help correct errors and omissions. When requesting a correction, please mention this item's handle: RePEc:bsj:jcrspp56:y:2012:i:1:p:1-18. See general information about how to correct material in RePEc.", "label": "0", "is_selected": "1", "text": "In this paper, we investigate the Nash equilibrium of a game of'silent duel'. \"Silent Duel Games with Time-varying Probabilities of Hitting Targets,\" Journal of Conflict Resolution, Peace Science Society (International), vol."}
{"original_text": "We devise a distributional variant of gradient temporal-difference (TD) learning. Distributional reinforcement learning has been demonstrated to outperform the regular one in the recent study (,). In the policy evaluation setting, we design two new algorithms called distributional GTD2 and distributional TDC using the Cramer distance on the distributional version of the Bellman error objective function, which inherits advantages of both the nonlinear gradient TD algorithms and the distributional RL approach. In the control setting, we propose the distributional Greedy-GQ using the similar derivation. We prove the asymptotic almost-sure convergence of distributional GTD2 and TDC to a local optimal solution for general smooth function approximators, which includes neural networks that have been widely used in recent study to solve the real-life RL problems. In each step, the computational complexities of above three algorithms are linear w.r.t. the number of the parameters of the function approximator, thus can be implemented efficiently for neural networks.", "label": "1", "is_selected": "0", "text": "We devise a distributional variant of gradient temporal-difference (TD) learning. Distributional reinforcement learning has been demonstrated to outperform the regular one in the recent study (,). In the policy evaluation setting, we design two new algorithms called distributional GTD2 and distributional TDC using the Cramer distance on the distributional version of the Bellman error objective function, which inherits advantages of both the nonlinear gradient TD algorithms and the distributional RL approach. In the control setting, we propose the distributional Greedy-GQ using the similar derivation. We prove the asymptotic almost-sure convergence of distributional GTD2 and TDC to a local optimal solution for general smooth function approximators, which includes neural networks that have been widely used in recent study to solve the real-life RL problems. In each step, the computational complexities of above three algorithms are linear w.r.t. the number of the parameters of the function approximator, thus can be implemented efficiently for neural networks."}
{"original_text": "Much of recent success in multiagent reinforcement learning has been in two-player zero-sum games. In these games, algorithms such as fictitious self-play and minimax tree search can converge to an equilibrium with no further training. These algorithms assume a very strong notion of learning: they assume that the opponent learns as much as they do. In this paper we explore the idea of emph{generalized Nash equilibria}, which are equilibria that emerge when the agents learn at different rates. We consider two natural questions: what is the impact of differing learning rates on convergence to an equilibrium, and how should the agents adjust their learning rates in order to converge to the best equilibrium that they can? To answer these questions we develop a new algorithm for multiagent learning, which we call fictitious opponent learning. In our setting, an agent plays against an opponent who is assumed to be much more powerful than them. We give a convergence guarantee for our algorithm, and show that it learns a better equilibrium than agents that train at the same rate. Much of recent success in multiagent reinforcement learning has been in two-player zero-sum games. In these games, algorithms such as fictitious self-play and minimax tree search can converge to an equilibrium with no further training. These algorithms assume a very strong notion of learning: they assume that the opponent learns as much as they do. In this paper we explore the idea of emph{generalized Nash equilibria}, which are equilibria that emerge when the agents learn at different rates. We consider two natural questions: what is the impact of differing learning rates on convergence to an equilibrium, and how should the agents adjust their learning rates in order to converge to the best equilibrium that they can? To answer these questions we develop a new algorithm for multiagent learning, which we call fictitious opponent learning. In our setting, an agent plays against an opponent who is assumed to be much more powerful than them. We give a convergence guarantee for our algorithm, and show that it learns a better equilibrium than agents that train at the same rate. Abstract: Much of recent success in multiagent reinforcement learning has been in two-player zero-sum games. In these games, algorithms such as fictitious self-play and minimax tree search can converge to an equilibrium with no further training. These algorithms assume a very strong notion of learning: they assume that the opponent learns as much as they do. In this paper we explore the idea of emph{generalized Nash equilibria}, which are equilibria that emerge when the agents learn at different rates. We consider two natural questions: what is the impact of differing learning rates on convergence to an equilibrium, and how should the agents adjust their learning rates in order to converge to the best equilibrium that they can? To answer these questions we develop a new algorithm for multiagent learning, which we call fictitious opponent learning. In our setting, an agent plays against an opponent who is assumed to be much more powerful than them. We give a convergence guarantee for our algorithm, and show that it learns a better equilibrium than agents that train at the same rate.", "label": "0", "is_selected": "0", "text": "Much of recent success in multiagent reinforcement learning has been in two-player zero-sum games. In these games, algorithms such as fictitious self-play and minimax tree search can converge to an equilibrium with no further training. These algorithms assume a very strong notion of learning: they assume that the opponent learns as much as they do. In this paper we explore the idea of emph{generalized Nash equilibria}, which are equilibria that emerge when the agents learn at different rates. We consider two natural questions: what is the impact of differing learning rates on convergence to an equilibrium, and how should the agents adjust their learning rates in order to converge to the best equilibrium that they can? To answer these questions we develop a new algorithm for multiagent learning, which we call fictitious opponent learning. In our setting, an agent plays against an opponent who is assumed to be much more powerful than them. We give a convergence guarantee for our algorithm, and show that it learns a better equilibrium than agents that train at the same rate. Much of recent success in multiagent reinforcement learning has been in two-player zero-sum games. In these games, algorithms such as fictitious self-play and minimax tree search can converge to an equilibrium with no further training. These algorithms assume a very strong notion of learning: they assume that the opponent learns as much as they do. In this paper we explore the idea of emph{generalized Nash equilibria}, which are equilibria that emerge when the agents learn at different rates. We consider two natural questions: what is the impact of differing learning rates on convergence to an equilibrium, and how should the agents adjust their learning rates in order to converge to the best equilibrium that they can? To answer these questions we develop a new algorithm for multiagent learning, which we call fictitious opponent learning. In our setting, an agent plays against an opponent who is assumed to be much more powerful than them. We give a convergence guarantee for our algorithm, and show that it learns a better equilibrium than agents that train at the same rate. Abstract: Much of recent success in multiagent reinforcement learning has been in two-player zero-sum games. In these games, algorithms such as fictitious self-play and minimax tree search can converge to an equilibrium with no further training. These algorithms assume a very strong notion of learning: they assume that the opponent learns as much as they do. In this paper we explore the idea of emph{generalized Nash equilibria}, which are equilibria that emerge when the agents learn at different rates. We consider two natural questions: what is the impact of differing learning rates on convergence to an equilibrium, and how should the agents adjust their learning rates in order to converge to the best equilibrium that they can? To answer these questions we develop a new algorithm for multiagent learning, which we call fictitious opponent learning. In our setting, an agent plays against an opponent who is assumed to be much more powerful than them. We give a convergence guarantee for our algorithm, and show that it learns a better equilibrium than agents that train at the same rate."}
{"original_text": "Speech and speaker recognition systems are employed in a variety of applications, from personal assistants to telephony surveillance and biometric authentication. The wide deployment of these systems has been made possible thanks to the introduction of Deep Neural Networks (DNNs). DNNs achieve excellent performance, even if they are not based on explicit models of the speech signal. For example, the spectrogram can be directly input to the network without using any spectro-temporal transformations or linear or non-linear features. However, the spectrogram is not a perfect descriptor of the speech signal. For example, a high-frequency region of the spectrum of a speech signal has a higher energy than its low-frequency counterpart. This energy imbalance is not taken into account by the spectrogram. Other forms of non-linearity, such as frequency modulation, can also be problematic. In this project, you will use DNNs to learn a spectro-temporal representation from raw waveforms, taking into account energy and frequency modulation. This representation can then be used to train speech recognition systems that can be more robust against noise and distortions. In this project, you will learn how to build your own DNNs for speech recognition and related tasks. You will use real-world data from the Linguistic Data Consortium (LDC) and the ACL M4 competition. You will train your system, compare the results with existing speech recognition systems and publish a paper. A large portion of this project will be implementing DNNs in Theano, a python package for deep learning. The project will also involve collecting data from the LDC and ACL M4 competition, training speech recognition systems and evaluating these systems using standardized metrics. Aaron Van den Oord, Oriol Vinyals, Navdeep Jaitly, Alex Graves. \"Grammar as a Foreign Language: Improving Neural Machine Translation with Morphology.\" Proc. EMNLP 2016, pp. . Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. \"Distilling the Knowledge in a Neural Network.\" Proceedings of the 31st International Conference on Machine Learning. 2014. Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. \"Explaining and Harnessing Adversarial Examples.\" arXiv:, 2014. Ian Goodfellow, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. \"Generative Adversarial Nets.\" arXiv:, 2014. D. Povey, A. Ghosal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, Y. Motlicek, P. Schwarz, J. Silovsky, G. Stemmer, K. Vesely, B. Vogel. \"The Kaldi Speech Recognition Toolkit.\" In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011. K. He, X. Zhang, S. Ren, and J. Sun. \"Deep Residual Learning for Image Recognition.\" arXiv:1512.03385, 2015. K. Simonyan and A. Zisserman. \"Very Deep Convolutional Networks for Large-Scale Image Recognition.\" arXiv:, 2014. H. Zhang, A. Lapedriza, X. Liu, A. Torralba, and A. Oliva. \"Understanding Deep Image Representations by Inverting them.\" arXiv:1606.08229, 2016. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. \"Going Deeper with Convolutions.\" arXiv:, 2014. I. Sutskever, O. Vinyals, and Q. V. Le. \"Sequence to Sequence Learning with Neural Networks.\" Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "label": "0", "is_selected": "1", "text": "The aim of this project is to train speech recognition systems that are more robust against noise and distortions, using spectrogram representation of the speech signal, as well as real-world data."}
{"original_text": "This paper addresses the problem of target detection and localisation in a limited area using multiple coordinated agents. The swarm of Unmanned Aerial Vehicles (UAVs) determines the position of the dispersion of stack effluents to a gas plume in a certain production area as fast as possible, that makes the problem challenging to model and solve, because of the time variability of the target. Three different exploration algorithms are designed and compared. Besides the exploration strategies, the paper reports a solution for quick convergence towards the actual stack position once detected by one member of the team. Both the navigation and localisation algorithms are fully distributed and based on the consensus theory. Simulations on realistic case studies are reported.", "label": "1", "is_selected": "0", "text": "This paper addresses the problem of target detection and localisation in a limited area using multiple coordinated agents. The swarm of Unmanned Aerial Vehicles (UAVs) determines the position of the dispersion of stack effluents to a gas plume in a certain production area as fast as possible, that makes the problem challenging to model and solve, because of the time variability of the target. Three different exploration algorithms are designed and compared. Besides the exploration strategies, the paper reports a solution for quick convergence towards the actual stack position once detected by one member of the team. Both the navigation and localisation algorithms are fully distributed and based on the consensus theory. Simulations on realistic case studies are reported."}
{"original_text": "How far and how fast does information spread in social media? Researchers have recently examined a number of factors that affect information diffusion in online social networks, including: the novelty of information, users' activity levels, who they pay attention to, and how they respond to friends' recommendations. Using URLs as markers of information, we carry out a detailed study of retweeting, the primary mechanism by which information spreads on the Twitter follower graph. Our empirical study examines how users respond to an incoming stimulus, i.e., a tweet (message) from a friend, and reveals that dynamically decaying visibility, which is the increasing cognitive effort required for discovering and acting upon a tweet, combined with limited attention play dominant roles in retweeting behavior. Specifically, we observe that users retweet information when it is most visible, such as when it near the top of their Twitter feed. Moreover, our measurements quantify how a user's limited attention is divided among incoming tweets, providing novel evidence that highly connected individuals are less likely to propagate an arbitrary tweet. Our study indicates that the finite ability to process incoming information constrains social contagion, and we conclude that rapid decay of visibility is the primary barrier to information propagation online.", "label": "1", "is_selected": "0", "text": "How far and how fast does information spread in social media? Researchers have recently examined a number of factors that affect information diffusion in online social networks, including: the novelty of information, users' activity levels, who they pay attention to, and how they respond to friends' recommendations. Using URLs as markers of information, we carry out a detailed study of retweeting, the primary mechanism by which information spreads on the Twitter follower graph. Our empirical study examines how users respond to an incoming stimulus, i.e., a tweet (message) from a friend, and reveals that dynamically decaying visibility, which is the increasing cognitive effort required for discovering and acting upon a tweet, combined with limited attention play dominant roles in retweeting behavior. Specifically, we observe that users retweet information when it is most visible, such as when it near the top of their Twitter feed. Moreover, our measurements quantify how a user's limited attention is divided among incoming tweets, providing novel evidence that highly connected individuals are less likely to propagate an arbitrary tweet. Our study indicates that the finite ability to process incoming information constrains social contagion, and we conclude that rapid decay of visibility is the primary barrier to information propagation online."}
{"original_text": "The Intensive Care Unit (ICU) is a hospital department where machine learning has the potential to provide valuable assistance in clinical decision making. Classical machine learning models usually only provide the final prediction of the model but not the underlying reasoning process that leads to this prediction. The way how a model arrived at a particular decision can be essential for a medical doctor to understand whether the prediction is trustworthy or not. In this work, we propose a method for explaining the reasoning process of a machine learning model that was used for predicting the survival of patients on admission to the ICU. Our method relies on the concept of the Shapley value in game theory. The Shapley value can be used to explain a model by assigning a relevance to each feature and the prediction itself. By displaying the relevance of each feature to the prediction as a bar chart, we can provide a visual overview of the reasoning process of the model.", "label": "0", "is_selected": "0", "text": "The Intensive Care Unit (ICU) is a hospital department where machine learning has the potential to provide valuable assistance in clinical decision making. Classical machine learning models usually only provide the final prediction of the model but not the underlying reasoning process that leads to this prediction. The way how a model arrived at a particular decision can be essential for a medical doctor to understand whether the prediction is trustworthy or not. In this work, we propose a method for explaining the reasoning process of a machine learning model that was used for predicting the survival of patients on admission to the ICU. Our method relies on the concept of the Shapley value in game theory. The Shapley value can be used to explain a model by assigning a relevance to each feature and the prediction itself. By displaying the relevance of each feature to the prediction as a bar chart, we can provide a visual overview of the reasoning process of the model."}
{"original_text": "Sorting, a classical combinatorial process, forms the bedrock of numerous algorithms with varied applications. A related problem involves efficiently finding the corresponding ranks of all the elements - catering to rank queries, data partitioning and allocation, etc. Although, the element ranks can be subsequently obtained by initially sorting the elements, such procedures involve O (n log n) computations and might not be suitable with large input sizes for hard real-time systems or for applications with data re-ordering constraints. This paper proposes S O N I K, a non-comparison linear time and space algorithm using bit operations inspired by radix sort for computing the ranks of all input integer elements, thereby providing implicit sorting. The element ranks are generated in-situ, i.e., directly at the corresponding element position without re-ordering or recourse to any other sorting mechanism.", "label": "1", "is_selected": "0", "text": "Sorting, a classical combinatorial process, forms the bedrock of numerous algorithms with varied applications. A related problem involves efficiently finding the corresponding ranks of all the elements - catering to rank queries, data partitioning and allocation, etc. Although, the element ranks can be subsequently obtained by initially sorting the elements, such procedures involve O (n log n) computations and might not be suitable with large input sizes for hard real-time systems or for applications with data re-ordering constraints. This paper proposes S O N I K, a non-comparison linear time and space algorithm using bit operations inspired by radix sort for computing the ranks of all input integer elements, thereby providing implicit sorting. The element ranks are generated in-situ, i.e., directly at the corresponding element position without re-ordering or recourse to any other sorting mechanism."}
{"original_text": "Numerical modeling of fluid flows based on kinetic equations provides an alternative approach for the description of complex flows simulations, and a number of kinetic methods have been developed from the past few decades. An important issue in kinetic theory is the numerical treatment of the collision integral. In this dissertation, we propose a novel discrete collision operator (DCO) with fully explicit time integration. We first present the theoretical analysis for the DCO. The numerical schemes are proved to be unconditionally stable. We then present several numerical examples to demonstrate the accuracy and efficiency of the DCO. The DCO is applied to the full Boltzmann equation and a simplified Boltzmann equation for both the pure Maxwell molecule and the hard-sphere molecule. The accuracy and efficiency of the DCO are shown through both one-dimensional and two-dimensional numerical examples. The present work represents a significant advance in the kinetic simulation of gas flows, including not only the Boltzmann equation, but also other kinetic equations.", "label": "0", "is_selected": "0", "text": "Numerical modeling of fluid flows based on kinetic equations provides an alternative approach for the description of complex flows simulations, and a number of kinetic methods have been developed from the past few decades. An important issue in kinetic theory is the numerical treatment of the collision integral. In this dissertation, we propose a novel discrete collision operator (DCO) with fully explicit time integration. We first present the theoretical analysis for the DCO. The numerical schemes are proved to be unconditionally stable. We then present several numerical examples to demonstrate the accuracy and efficiency of the DCO. The DCO is applied to the full Boltzmann equation and a simplified Boltzmann equation for both the pure Maxwell molecule and the hard-sphere molecule. The accuracy and efficiency of the DCO are shown through both one-dimensional and two-dimensional numerical examples. The present work represents a significant advance in the kinetic simulation of gas flows, including not only the Boltzmann equation, but also other kinetic equations."}
{"original_text": "The consequences of anthropogenic climate change are extensively debated through scientific papers, newspaper articles, and blogs. Newspaper articles may lack accuracy, while the severity of findings in scientific papers may be too opaque for the public to understand. Social media, however, is a forum where individuals of diverse backgrounds can share their thoughts and opinions. As consumption shifts from old media to new, Twitter has become a valuable resource for analyzing current events and headline news. In this research, we analyze tweets containing the word \"climate\" collected between September 2008 and July 2014. Through use of a previously developed sentiment measurement tool called the Hedonometer, we determine how collective sentiment varies in response to climate change news, events, and natural disasters. We find that natural disasters, climate bills, and oil-drilling can contribute to a decrease in happiness while climate rallies, a book release, and a green ideas contest can contribute to an increase in happiness. Words uncovered by our analysis suggest that responses to climate change news are predominately from climate change activists rather than climate change deniers, indicating that Twitter is a valuable resource for the spread of climate change awareness.", "label": "1", "is_selected": "0", "text": "The consequences of anthropogenic climate change are extensively debated through scientific papers, newspaper articles, and blogs. Newspaper articles may lack accuracy, while the severity of findings in scientific papers may be too opaque for the public to understand. Social media, however, is a forum where individuals of diverse backgrounds can share their thoughts and opinions. As consumption shifts from old media to new, Twitter has become a valuable resource for analyzing current events and headline news. In this research, we analyze tweets containing the word \"climate\" collected between September 2008 and July 2014. Through use of a previously developed sentiment measurement tool called the Hedonometer, we determine how collective sentiment varies in response to climate change news, events, and natural disasters. We find that natural disasters, climate bills, and oil-drilling can contribute to a decrease in happiness while climate rallies, a book release, and a green ideas contest can contribute to an increase in happiness. Words uncovered by our analysis suggest that responses to climate change news are predominately from climate change activists rather than climate change deniers, indicating that Twitter is a valuable resource for the spread of climate change awareness."}
{"original_text": "Due to their simple construction, LFSRs are commonly used as building blocks in various random number generators. Nonlinear feedforward logic is incorporated in LFSRs to increase the linear complexity of the binary sequence to a desired level. The linear complexity of the generated sequence is usually measured in terms of the maximum length of the shift register required for a given sequence. Some of the applications of LFSRs are communications, cryptography, authentication, pseudo-random number generators, etc. The 8-bit shift register can generate a maximum period of 256. The period of an LFSR depends upon the number of flip-flops (2n-1). The initial state of the shift register is established at power up. The LFSR uses a seed value to set its initial state. The random number generator also uses this seed value. To maintain security, the seed value is usually a secret. The LFSR is also used to check the integrity of a message. 8-bit LFSR with nonlinear feedback: The 8-bit LFSR can be modified to make it nonlinear. The 8-bit LFSR consists of 3 flip-flops, D, E, and F. The inputs of flip-flop E are XORed and used as a feedback input of flip-flop D. The output of flip-flop D is XORed with the input of flip-flop F. The output of flip-flop F is used as the output of the 8-bit LFSR. The nonlinear feedback is generated by using an inverter and an XOR gate. The linear complexity of the sequence is increased by using an XOR gate. 16-bit LFSR with nonlinear feedback: The 16-bit LFSR consists of 4 flip-flops, D, E, F, and G. The inputs of flip-flop G are XORed and used as a feedback input of flip-flop F. The output of flip-flop F is XORed with the input of flip-flop D. The output of flip-flop D is XORed with the input of flip-flop E. The output of flip-flop E is used as the output of the 16-bit LFSR. The nonlinear feedback is generated by using an inverter and an XOR gate. 24-bit LFSR with nonlinear feedback: The 24-bit LFSR consists of 5 flip-flops, D, E, F, G, and H. The inputs of flip-flop G are XORed and used as a feedback input of flip-flop F. The output of flip-flop F is XORed with the input of flip-flop D. The output of flip-flop D is XORed with the input of flip-flop E. The output of flip-flop E is XORed with the input of flip-flop H. The output of flip-flop H is used as the output of the 24-bit LFSR. The nonlinear feedback is generated by using an inverter and an XOR gate. The initial state of the shift register can be initialized at power up by using a seed value. The sequence generated by the LFSR is a periodic sequence. 8-bit LFSR with nonlinear feedback: The 8-bit LFSR can be modified to make it nonlinear. The 8-bit LFSR consists of 3 flip-flops, D, E, and F. The inputs of flip-flop E are XORed and used as a feedback input of flip-flop D. The output of flip-flop D is XORed with the input of flip-flop F. The output of flip-flop F is used as the output of the 8-bit LFSR. The nonlinear feedback is generated by using an inverter and an XOR gate. The sequence generated by the LFSR is a periodic sequence. The linear complexity of the sequence is increased by using an XOR gate. 16-bit LFSR with nonlinear feedback: The 16-bit LFSR consists of 4 flip-flops, D, E, F, and G. The inputs of flip-flop G are XORed and used as a feedback input of flip-flop F. The output of flip-flop F is XORed with the input of flip-flop D. The output of f", "label": "0", "is_selected": "0", "text": "Due to their simple construction, LFSRs are commonly used as building blocks in various random number generators. Nonlinear feedforward logic is incorporated in LFSRs to increase the linear complexity of the binary sequence to a desired level. The linear complexity of the generated sequence is usually measured in terms of the maximum length of the shift register required for a given sequence. Some of the applications of LFSRs are communications, cryptography, authentication, pseudo-random number generators, etc. The 8-bit shift register can generate a maximum period of 256. The period of an LFSR depends upon the number of flip-flops (2n-1). The initial state of the shift register is established at power up. The LFSR uses a seed value to set its initial state. The random number generator also uses this seed value. To maintain security, the seed value is usually a secret. The LFSR is also used to check the integrity of a message. 8-bit LFSR with nonlinear feedback: The 8-bit LFSR can be modified to make it nonlinear. The 8-bit LFSR consists of 3 flip-flops, D, E, and F. The inputs of flip-flop E are XORed and used as a feedback input of flip-flop D. The output of flip-flop D is XORed with the input of flip-flop F. The output of flip-flop F is used as the output of the 8-bit LFSR. The nonlinear feedback is generated by using an inverter and an XOR gate. The linear complexity of the sequence is increased by using an XOR gate. 16-bit LFSR with nonlinear feedback: The 16-bit LFSR consists of 4 flip-flops, D, E, F, and G. The inputs of flip-flop G are XORed and used as a feedback input of flip-flop F. The output of flip-flop F is XORed with the input of flip-flop D. The output of flip-flop D is XORed with the input of flip-flop E. The output of flip-flop E is used as the output of the 16-bit LFSR. The nonlinear feedback is generated by using an inverter and an XOR gate. 24-bit LFSR with nonlinear feedback: The 24-bit LFSR consists of 5 flip-flops, D, E, F, G, and H. The inputs of flip-flop G are XORed and used as a feedback input of flip-flop F. The output of flip-flop F is XORed with the input of flip-flop D. The output of flip-flop D is XORed with the input of flip-flop E. The output of flip-flop E is XORed with the input of flip-flop H. The output of flip-flop H is used as the output of the 24-bit LFSR. The nonlinear feedback is generated by using an inverter and an XOR gate. The initial state of the shift register can be initialized at power up by using a seed value. The sequence generated by the LFSR is a periodic sequence. 8-bit LFSR with nonlinear feedback: The 8-bit LFSR can be modified to make it nonlinear. The 8-bit LFSR consists of 3 flip-flops, D, E, and F. The inputs of flip-flop E are XORed and used as a feedback input of flip-flop D. The output of flip-flop D is XORed with the input of flip-flop F. The output of flip-flop F is used as the output of the 8-bit LFSR. The nonlinear feedback is generated by using an inverter and an XOR gate. The sequence generated by the LFSR is a periodic sequence. The linear complexity of the sequence is increased by using an XOR gate. 16-bit LFSR with nonlinear feedback: The 16-bit LFSR consists of 4 flip-flops, D, E, F, and G. The inputs of flip-flop G are XORed and used as a feedback input of flip-flop F. The output of flip-flop F is XORed with the input of flip-flop D. The output of f"}
{"original_text": "We revisit the complexity of the classical k - Coloring problem parameterized by clique-width. This is a very well-studied problem that becomes highly intractable when the number of colors k is large. However, much less is known on its complexity for small, concrete values of k. In this paper, we completely determine the complexity of k - Coloring parameterized by clique-width for any fixed k, under the SETH. Specifically, we show that for all k 3, 0, k - Coloring cannot be solved in time O 2 k 2) cw), and give an algorithm running in time O 2 k 2) cw). Thus, if the SETH is true, - 2 k 2 is the \"correct\" base of the exponent for every k. Along the way, we also consider the complexity of k - Coloring parameterized by the related parameter modular treewidth (mtw). In this case we show that the \"correct\" running time, under the SETH, is O k k 2) mtw). If we base our results on a weaker assumption (the ETH), they imply that k - Coloring cannot be solved in time n o (cw), even on instances with O (log n) colors.", "label": "1", "is_selected": "0", "text": "We revisit the complexity of the classical k - Coloring problem parameterized by clique-width. This is a very well-studied problem that becomes highly intractable when the number of colors k is large. However, much less is known on its complexity for small, concrete values of k. In this paper, we completely determine the complexity of k - Coloring parameterized by clique-width for any fixed k, under the SETH. Specifically, we show that for all k 3, 0, k - Coloring cannot be solved in time O 2 k 2) cw), and give an algorithm running in time O 2 k 2) cw). Thus, if the SETH is true, - 2 k 2 is the \"correct\" base of the exponent for every k. Along the way, we also consider the complexity of k - Coloring parameterized by the related parameter modular treewidth (mtw). In this case we show that the \"correct\" running time, under the SETH, is O k k 2) mtw). If we base our results on a weaker assumption (the ETH), they imply that k - Coloring cannot be solved in time n o (cw), even on instances with O (log n) colors."}
{"original_text": "This paper presents an adaptive randomized algorithm for computing the butterfly factorization of a x m n matrix with m n provided that both the matrix and its transpose can be accessed only by the multiplication of a vector by the matrix. The algorithm adaptively uses either sequential or parallel computations depending on the choice of the column of the matrix on which the multiplications are carried out. The algorithm is designed to use both multiprocessor (parallel) and vector computers. The butterfly factorization is used in the computation of the discrete cosine transform and the discrete Fourier transform. 1990.", "label": "0", "is_selected": "0", "text": "This paper presents an adaptive randomized algorithm for computing the butterfly factorization of a x m n matrix with m n provided that both the matrix and its transpose can be accessed only by the multiplication of a vector by the matrix. The algorithm adaptively uses either sequential or parallel computations depending on the choice of the column of the matrix on which the multiplications are carried out. The algorithm is designed to use both multiprocessor (parallel) and vector computers. The butterfly factorization is used in the computation of the discrete cosine transform and the discrete Fourier transform. 1990."}
{"original_text": "Batch normalization (BN) has become a standard technique for training the modern deep networks. However, its effectiveness diminishes when the batch size becomes smaller, since the batch statistics estimation becomes less accurate. In this paper, we propose a regularization approach to enhancing the effectiveness of batch normalization in small-batch training. Specifically, we propose to adopt the Fisher information matrix as a regularization target to encourage the batch statistics to be more representative of the population statistics. Our proposed approach does not need any extra training cost and thus is quite different from recent works on information-aware batch normalization. Extensive experiments are conducted to show the effectiveness of our approach.", "label": "0", "is_selected": "0", "text": "Batch normalization (BN) has become a standard technique for training the modern deep networks. However, its effectiveness diminishes when the batch size becomes smaller, since the batch statistics estimation becomes less accurate. In this paper, we propose a regularization approach to enhancing the effectiveness of batch normalization in small-batch training. Specifically, we propose to adopt the Fisher information matrix as a regularization target to encourage the batch statistics to be more representative of the population statistics. Our proposed approach does not need any extra training cost and thus is quite different from recent works on information-aware batch normalization. Extensive experiments are conducted to show the effectiveness of our approach."}
{"original_text": "Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.", "label": "1", "is_selected": "0", "text": "Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures."}
{"original_text": "Verification of PCTL properties of MDPs with convex uncertainties has been investigated recently by Puggelli et al. However, model checking algorithms typically suffer from state space explosion. In this paper, we show that the computation time of the verification algorithm depends polynomially on the dimension of the state space. This is achieved by a polynomial time reduction to a computation problem for a linear complementarity problem. Furthermore, a variation of the algorithm is shown to compute a decomposition of the state space into regions for which the problem of interest is true. This decomposition can be used to reduce the size of the state space to be explored by a model checker.", "label": "0", "is_selected": "0", "text": "Verification of PCTL properties of MDPs with convex uncertainties has been investigated recently by Puggelli et al. However, model checking algorithms typically suffer from state space explosion. In this paper, we show that the computation time of the verification algorithm depends polynomially on the dimension of the state space. This is achieved by a polynomial time reduction to a computation problem for a linear complementarity problem. Furthermore, a variation of the algorithm is shown to compute a decomposition of the state space into regions for which the problem of interest is true. This decomposition can be used to reduce the size of the state space to be explored by a model checker."}
{"original_text": "We design and implement an end-to-end system for real-time crime detection in low-light environments. Unlike Closed-Circuit Television, which performs reactively, the Low-Light Environment Neural Surveillance provides real time crime alerts. The system uses a low-light video feed processed in real-time by an optical-flow network, spatial and temporal networks, and a Support Vector Machine to identify shootings, assaults, and thefts. We create a low-light action-recognition dataset, LENS-4, which will be publicly available. An IoT infrastructure set up via Amazon Web Services interprets messages from the local board hosting the camera for action recognition and parses the results in the cloud to relay messages. The system achieves 71.5 accuracy at 20 FPS. The user interface is a mobile app which allows local authorities to receive notifications and to view a video of the crime scene. Citizens have a public app which enables law enforcement to push crime alerts based on user proximity.", "label": "1", "is_selected": "0", "text": "We design and implement an end-to-end system for real-time crime detection in low-light environments. Unlike Closed-Circuit Television, which performs reactively, the Low-Light Environment Neural Surveillance provides real time crime alerts. The system uses a low-light video feed processed in real-time by an optical-flow network, spatial and temporal networks, and a Support Vector Machine to identify shootings, assaults, and thefts. We create a low-light action-recognition dataset, LENS-4, which will be publicly available. An IoT infrastructure set up via Amazon Web Services interprets messages from the local board hosting the camera for action recognition and parses the results in the cloud to relay messages. The system achieves 71.5 accuracy at 20 FPS. The user interface is a mobile app which allows local authorities to receive notifications and to view a video of the crime scene. Citizens have a public app which enables law enforcement to push crime alerts based on user proximity."}
{"original_text": "There are over 1.2 million applications on the Google Play store today with a large number of competing applications for any given use or function. This creates challenges for users in selecting the right application. Moreover, some of the applications being of dubious origin, there are no mechanisms for users to understand who the applications are talking to, and to what extent. In our work, we first develop a lightweight characterization methodology that can automatically extract descriptions of application network behavior, and apply this to a large selection of applications from the Google App Store. We find several instances of overly aggressive communication with tracking websites, of excessive communication with ad related sites, and of communication with sites previously associated with malware activity. Our results underscore the need for a tool to provide users more visibility into the communication of apps installed on their mobile devices. To this end, we develop an Android application to do just this; our application monitors outgoing traffic, associates it with particular applications, and then identifies destinations in particular categories that we believe suspicious or else important to reveal to the end-user.", "label": "1", "is_selected": "0", "text": "There are over 1.2 million applications on the Google Play store today with a large number of competing applications for any given use or function. This creates challenges for users in selecting the right application. Moreover, some of the applications being of dubious origin, there are no mechanisms for users to understand who the applications are talking to, and to what extent. In our work, we first develop a lightweight characterization methodology that can automatically extract descriptions of application network behavior, and apply this to a large selection of applications from the Google App Store. We find several instances of overly aggressive communication with tracking websites, of excessive communication with ad related sites, and of communication with sites previously associated with malware activity. Our results underscore the need for a tool to provide users more visibility into the communication of apps installed on their mobile devices. To this end, we develop an Android application to do just this; our application monitors outgoing traffic, associates it with particular applications, and then identifies destinations in particular categories that we believe suspicious or else important to reveal to the end-user."}
{"original_text": "In a sponsored search auction, decisions about how to rank ads impose tradeoffs between objectives such as revenue and welfare. In this paper, we examine how these tradeoffs should be balanced, and how they affect the performance of a ranking policy. We develop a formal decision model for a sponsored search auction, allowing us to define and compute objective functions for policies that determine the ranking of ads. We prove that the expected welfare of a policy is upper-bounded by its expected revenue, and show how to derive an upper-bound on the expected welfare of a policy given its expected revenue. We then propose a framework for computing optimal policies, and use it to examine several optimal policies. Our experiments show that for some objective functions, there is a significant gap between the optimal policy and the policy that maximizes expected revenue. Our results also show that ranking policies that perform well for one objective function do not necessarily perform well for another. We conclude with a discussion of the implications of our results for sponsored search auctions.", "label": "0", "is_selected": "1", "text": "In this paper, we study the performance of ranking policies in a sponsored search auction, using a formal decision model and experiments with a number of optimal policies."}
{"original_text": "This paper deals with a complete bipartite matching problem with the objective of finding an optimal matching that maximizes a certain generic predefined utility function on the set of all possible matchings. The utility function under consideration is a function of the weights of the edges in the matching. The matching is described by a weighted bipartite graph with a weight function on the set of edges. We first show that, under certain assumptions, the optimal matching is unique, and then we present an algorithm for finding it. The algorithm is based on a weighted version of the network simplex method. We describe how the algorithm works in detail. In the second part of the paper we present an algorithm for finding a 1-optimal matching, i.e., a matching that is optimal with respect to the weight function under consideration. We prove that such a matching is unique if the weight function is monotone. We also describe a simple heuristic algorithm for finding a 1-optimal matching.", "label": "0", "is_selected": "1", "text": "The aim of this paper is to show how an algorithm can be used to find an optimal matching for a complete bipartite matching problem with the objective of finding a generic utility function on the set of all possible matchings."}
{"original_text": "Deep neural networks are highly expressive machine learning models with the ability to interpolate arbitrary datasets. Deep nets are typically optimized via first-order methods and the optimization process crucially depends on the characteristics of the network as well as the dataset. This work sheds light on the relation between the network size and the properties of the dataset with an emphasis on deep residual networks (ResNets). Our contribution is that if the network Jacobian is full rank, gradient descent for the quadratic loss and smooth activation converges to the global minima even if the network width m of the ResNet scales linearly with the sample size n, and independently from the network depth. To the best of our knowledge, this is the first work which provides a theoretical guarantee for the convergence of neural networks in the m O (n) regime.", "label": "1", "is_selected": "0", "text": "Deep neural networks are highly expressive machine learning models with the ability to interpolate arbitrary datasets. Deep nets are typically optimized via first-order methods and the optimization process crucially depends on the characteristics of the network as well as the dataset. This work sheds light on the relation between the network size and the properties of the dataset with an emphasis on deep residual networks (ResNets). Our contribution is that if the network Jacobian is full rank, gradient descent for the quadratic loss and smooth activation converges to the global minima even if the network width m of the ResNet scales linearly with the sample size n, and independently from the network depth. To the best of our knowledge, this is the first work which provides a theoretical guarantee for the convergence of neural networks in the m O (n) regime."}
{"original_text": "A source model of key sharing between three users is considered in which each pair of them wishes to agree on a secret key hidden from the remaining user. There are two models: (i) the cooperative model in which the three users are able to exchange information about their keys, and (ii) the non-cooperative model in which no information can be exchanged between any pair of users. Both the models are then extended to the case of an arbitrary number of users, and finally the case of k users each having a key to be shared with all of the others is considered. For the non-cooperative model, a condition on the set of secrets is given that is necessary and sufficient for key agreement. The condition is not necessary in the cooperative model. Keywords: key sharing, source coding, secret sharing Received January 31, 1997; revised manuscript received August 24, 1997. Communicated by D. S. Sivakumar. Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA 24061-0107, U.S.A. E-mail address: (e-mail) 1.Department of Electrical and Computer EngineeringVirginia TechBlacksburgU.S.A. Sia, H.H. IEICE Trans. Commun. (1998) 81: 28.", "label": "0", "is_selected": "1", "text": "This paper presents a new model of key sharing in which secret keys are hidden from the remaining users of a computer program, but are shared between the others."}
{"original_text": "The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA NLP models due to distribution shift in data, and propose a method for mitigating such failures. We formulate a probabilistic model for language based on word co-occurrence distributions, and use this model to design an adversarial algorithm that is capable of generating hypothetical examples for words that are likely to exist in a language, but have not been observed in training data. We experiment with generating such examples for words that are likely to be present in a language (e.g. names of cities, brands of cars), and observe that these examples generalize well. We evaluate this method in the context of two NLP tasks: Question Answering and Sentiment Analysis, where we are able to generate hypothetical examples for 17.3 and 12.1 of the words in the training data, respectively. We gratefully acknowledge support from DARPA SIMPLEX N66001-17-2-4031, DARPA LwLL N66001-17-2-4031, DARPA CRASH N66001-17-2-4031, the Samsung Advanced Institute of Technology (Next Generation Deep Learning: from Patterns to Decisions), the Swiss National Science Foundation (SNSF) under NFP75 BigDataSwiss project, and NSF-BSF grants , and .", "label": "0", "is_selected": "1", "text": "In this paper, we present a novel method for training speech-to-text (SOTA) NLP models based on word co-occurrence, which is capable of generating adversarial algorithms."}
{"original_text": "In Robot-Assisted Minimally Invasive Surgery (RAMIS), a camera assistant is normally required to control the position and zooming ratio of the laparoscope, following the surgeon's instructions. However, moving the laparoscope manually is a tedious task for the camera assistant, especially in minimally invasive surgery with a narrow field of view. In this paper, we propose an automated camera controller that uses a depth sensor to track the 3D position and orientation of surgical instruments and estimate the distance to the organ of interest, and uses a color camera to track the surgeon's hands. The camera controller then uses the information obtained from the depth sensor and the color camera to automatically control the position and zoom of the laparoscope. The experiments show that our method can track surgical tools and estimate their distance to the organ of interest with an average error of 0.8 cm. We also evaluated the performance of the proposed camera controller during a robot-assisted minimally invasive surgery on the porcine model. The results show that the proposed camera controller can automatically control the position and zoom of the laparoscope, while following the camera assistant's instructions.", "label": "0", "is_selected": "1", "text": "In this paper, we propose an automated camera controller that can automatically control the position and zoom of the laparoscope during robot-assisted minimally invasive surgery with a narrow field of view."}
{"original_text": "Synthesizing physiologically-accurate human movement in a variety of conditions can help practitioners plan surgeries, design experiments, or prototype assistive devices in simulated environments, reducing time and costs and improving treatment outcomes. Because of the large and complex solution spaces of biomechanical models, current methods are constrained to specific movements and models, requiring careful design of a controller and hindering many possible applications. We sought to discover if modern optimization methods efficiently explore these complex spaces. To do this, we posed the problem as a competition in which participants were tasked with developing a controller to enable a physiologically-based human model to navigate a complex obstacle course as quickly as possible, without using any experimental data. They were provided with a human musculoskeletal model and a physics-based simulation environment. In this paper, we discuss the design of the competition, technical difficulties, results, and analysis of the top controllers. The challenge proved that deep reinforcement learning techniques, despite their high computational cost, can be successfully employed as an optimization method for synthesizing physiologically feasible motion in high-dimensional biomechanical systems.", "label": "1", "is_selected": "0", "text": "Synthesizing physiologically-accurate human movement in a variety of conditions can help practitioners plan surgeries, design experiments, or prototype assistive devices in simulated environments, reducing time and costs and improving treatment outcomes. Because of the large and complex solution spaces of biomechanical models, current methods are constrained to specific movements and models, requiring careful design of a controller and hindering many possible applications. We sought to discover if modern optimization methods efficiently explore these complex spaces. To do this, we posed the problem as a competition in which participants were tasked with developing a controller to enable a physiologically-based human model to navigate a complex obstacle course as quickly as possible, without using any experimental data. They were provided with a human musculoskeletal model and a physics-based simulation environment. In this paper, we discuss the design of the competition, technical difficulties, results, and analysis of the top controllers. The challenge proved that deep reinforcement learning techniques, despite their high computational cost, can be successfully employed as an optimization method for synthesizing physiologically feasible motion in high-dimensional biomechanical systems."}
{"original_text": "Storage systems have a strong need for substantially improving their error correction capabilities, especially for long-term storage where the accumulating errors can exceed the decoding threshold of error-correcting codes (ECCs). In this work, a new scheme is presented that uses deep learning to perform soft decoding for noisy files based on their natural redundancy. The soft decoding result is then combined with ECCs for substantially better error correction performance. The scheme is representation-oblivious: it requires no prior knowledge on how data are represented (e.g., mapped from symbols to bits, compressed, and combined with meta data) in different types of files, which makes the solution more convenient to use for storage systems. Experimental results confirm that the scheme can substantially improve the ability to recover data for different types of files even when the bit error rates in the files have significantly exceeded the decoding threshold of the ECC. The code of this work has been publicly released. 1 1 footnote 1", "label": "1", "is_selected": "0", "text": "Storage systems have a strong need for substantially improving their error correction capabilities, especially for long-term storage where the accumulating errors can exceed the decoding threshold of error-correcting codes (ECCs). In this work, a new scheme is presented that uses deep learning to perform soft decoding for noisy files based on their natural redundancy. The soft decoding result is then combined with ECCs for substantially better error correction performance. The scheme is representation-oblivious: it requires no prior knowledge on how data are represented (e.g., mapped from symbols to bits, compressed, and combined with meta data) in different types of files, which makes the solution more convenient to use for storage systems. Experimental results confirm that the scheme can substantially improve the ability to recover data for different types of files even when the bit error rates in the files have significantly exceeded the decoding threshold of the ECC. The code of this work has been publicly released. 1 1 footnote 1"}
{"original_text": "Quantum Clustering is a powerful method to detect clusters in data with mixed density. However, it is very sensitive to a length parameter that is inherent to the Schrodinger equation. In addition, linking data points into clusters requires local estimates of covariance that are also controlled by length parameters. This raises the question of how to adjust the control parameters of the Schrodinger equation for optimal clustering. We propose a probabilistic framework that provides an objective function for the goodness-of-fit to the data, enabling the control parameters to be optimised within a Bayesian framework. This naturally yields probabilities of cluster membership and data partitions with specific numbers of clusters. The proposed framework is tested on real and synthetic data sets, assessing its validity by measuring concordance with known data structure by means of the Jaccard score (JS). This work also proposes an objective way to measure performance in unsupervised learning that correlates very well with JS.", "label": "1", "is_selected": "0", "text": "Quantum Clustering is a powerful method to detect clusters in data with mixed density. However, it is very sensitive to a length parameter that is inherent to the Schrodinger equation. In addition, linking data points into clusters requires local estimates of covariance that are also controlled by length parameters. This raises the question of how to adjust the control parameters of the Schrodinger equation for optimal clustering. We propose a probabilistic framework that provides an objective function for the goodness-of-fit to the data, enabling the control parameters to be optimised within a Bayesian framework. This naturally yields probabilities of cluster membership and data partitions with specific numbers of clusters. The proposed framework is tested on real and synthetic data sets, assessing its validity by measuring concordance with known data structure by means of the Jaccard score (JS). This work also proposes an objective way to measure performance in unsupervised learning that correlates very well with JS."}
{"original_text": "Inference on a large-scale knowledge graph (KG) is of great importance for KG applications like question answering. The path-based reasoning models can leverage much information over paths other than pure path features (i.e., semantic similarity) to obtain better performance. However, most of these models are designed for predefined paths such as shortest paths. It is difficult to leverage information on paths with arbitrary lengths because the number of paths grows exponentially with the path length. In this paper, we propose a novel model that can efficiently leverage arbitrary-length paths for KG inference. We first design a path generation framework that can generate arbitrary-length paths in a large KG. To reduce the time and space complexity of generating paths with arbitrary lengths, we propose a path sampling approach based on the subgraph isomorphism. We then introduce a path-based reasoning model, PERPLEX, that jointly models the information on paths and path features to predict the answer. PERPLEX contains a path-based embedding layer that can learn path-based representations using arbitrary-length paths, a feature aggregation layer that can aggregate path features over arbitrary-length paths, and a reasoning layer that can reason with paths and path features. Extensive experiments on four public datasets show that PERPLEX can achieve significant improvements over state-of-the-art path-based models.", "label": "0", "is_selected": "1", "text": "We propose a novel path-based reasoning model that can efficiently leverage arbitrary-length paths for knowledge graph ( KG) inference in a large subgraph."}
{"original_text": "In the domain of emergency management during hazard crises, having sufficient situational awareness information is critical. It requires capturing and integrating information from sources such as satellite images, local sensors, and other systems. The management of such data is a complex task, with information in various formats and locations. A common issue is to find and use data and information from multiple and various sources. The system should be capable of integrating data from different sources to support decision makers. The data integration problem for emergency management is very similar to the problem of data integration for other domains, such as enterprise systems. Data integration techniques from other domains can be applied to the emergency management domain, but the domain specific requirements need to be considered. This requires understanding the relevant domain information requirements. For this research, it is important to have a good understanding of the emergency management domain. The main objective of this research is to identify the data integration requirements of emergency management systems. The research will start with a literature review. The literature review will focus on emergency management, data integration, and ontology research. This will allow the development of a better understanding of the requirements of data integration in the emergency management domain. The next step of the research is to collect the requirements of data integration for emergency management systems. This will be done by interviewing emergency management system users and developers. The users will be from the emergency management community, and the developers from the University of Waikato. The interviews will be conducted to understand their data integration requirements and how data integration in emergency management systems is currently done. The information obtained from the literature review and the interviews will be used to identify the data integration requirements of emergency management systems. After the identification of the requirements, the requirements will be analysed and any issues with the requirements will be addressed. After identifying the data integration requirements, a use case will be developed for the requirements. The use case will be used to determine how the data integration requirements should be implemented. The final objective of the research is to develop a data integration prototype to satisfy the identified requirements. The prototype will be evaluated to determine if the requirements are satisfied. If the requirements are not satisfied, the prototype will be modified. The outcomes of this research are the identified data integration requirements, and the developed prototype that satisfies the identified requirements. The data integration prototype can be used by emergency management systems. It will help emergency management systems with data integration tasks.", "label": "0", "is_selected": "0", "text": "In the domain of emergency management during hazard crises, having sufficient situational awareness information is critical. It requires capturing and integrating information from sources such as satellite images, local sensors, and other systems. The management of such data is a complex task, with information in various formats and locations. A common issue is to find and use data and information from multiple and various sources. The system should be capable of integrating data from different sources to support decision makers. The data integration problem for emergency management is very similar to the problem of data integration for other domains, such as enterprise systems. Data integration techniques from other domains can be applied to the emergency management domain, but the domain specific requirements need to be considered. This requires understanding the relevant domain information requirements. For this research, it is important to have a good understanding of the emergency management domain. The main objective of this research is to identify the data integration requirements of emergency management systems. The research will start with a literature review. The literature review will focus on emergency management, data integration, and ontology research. This will allow the development of a better understanding of the requirements of data integration in the emergency management domain. The next step of the research is to collect the requirements of data integration for emergency management systems. This will be done by interviewing emergency management system users and developers. The users will be from the emergency management community, and the developers from the University of Waikato. The interviews will be conducted to understand their data integration requirements and how data integration in emergency management systems is currently done. The information obtained from the literature review and the interviews will be used to identify the data integration requirements of emergency management systems. After the identification of the requirements, the requirements will be analysed and any issues with the requirements will be addressed. After identifying the data integration requirements, a use case will be developed for the requirements. The use case will be used to determine how the data integration requirements should be implemented. The final objective of the research is to develop a data integration prototype to satisfy the identified requirements. The prototype will be evaluated to determine if the requirements are satisfied. If the requirements are not satisfied, the prototype will be modified. The outcomes of this research are the identified data integration requirements, and the developed prototype that satisfies the identified requirements. The data integration prototype can be used by emergency management systems. It will help emergency management systems with data integration tasks."}
{"original_text": "In this paper, several variants of two-stream architectures for temporal action proposal generation in long, untrimmed videos are presented. Inspired by the recent advances in the field of human action detection, we propose a two-stream architecture that uses both RGB and motion streams. Furthermore, we extend our proposed method by applying the recently introduced human body part segmentation model. Our model is based on the multi-stage cascade R-CNN object detection framework. The proposed method is evaluated on the MPII-Cooking II and J-HMDB-21 benchmarks, which resulted in improved performance over the state-of-the-art methods.", "label": "0", "is_selected": "1", "text": "Temporal action detection in video is one of the most computationally intensive areas of computer science. The proposed method is evaluated on the MPII-Cooking II and J-HMDB-21 benchmarks, which resulted in improved performance over the state-of-the-art methods."}
{"original_text": "The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the data annotation process. We collect and annotate a challenge-set we call Challenging RE (CRE), based on naturally occurring corpus examples, to benchmark this behavior. Our experiments with four state-of-the-art RE models show that they have indeed adopted shallow heuristics that do not generalize to the challenge-set data. Further, we find that alternative question answering modeling performs significantly better than the SOTA models on the challenge-set, despite worse overall TACRED performance. By adding some of the challenge data as training examples, the performance of the model improves. Finally, we provide concrete suggestion on how to improve RE data collection to alleviate this behavior.", "label": "1", "is_selected": "0", "text": "The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the data annotation process. We collect and annotate a challenge-set we call Challenging RE (CRE), based on naturally occurring corpus examples, to benchmark this behavior. Our experiments with four state-of-the-art RE models show that they have indeed adopted shallow heuristics that do not generalize to the challenge-set data. Further, we find that alternative question answering modeling performs significantly better than the SOTA models on the challenge-set, despite worse overall TACRED performance. By adding some of the challenge data as training examples, the performance of the model improves. Finally, we provide concrete suggestion on how to improve RE data collection to alleviate this behavior."}
{"original_text": "In this paper, we consider a novel cache-enabled heterogeneous network (HetNet), where macro base stations (BSs) with traditional sub-6 GHz are overlaid by dense millimeter wave (mmWave) pico BSs. These mmWave pico BSs are deployed in cell-centers and cell-edges to support the heterogeneous traffic of different mobile users. The HetNet is also equipped with base station caches to enhance the traffic offloading and the data rate. To tackle the small cell traffic and the cache-hit ratios, we propose a joint resource management scheme, which simultaneously decides on the cache content placement, the transmit power allocation, and the content-user association. In particular, the content-user association is jointly determined by both the content-to-user association and the user-to-BS association. The objective is to maximize the total expected system throughput, which is a function of the cache content placement, the transmit power allocation, and the user association. The optimization problem is formulated as a mixed-integer nonlinear programming problem which is difficult to solve. Hence, to obtain a more tractable solution, we transform it into an equivalent optimization problem in terms of the probability distributions. The equivalent optimization problem is then solved via the Lyapunov optimization technique. Finally, the performance of the proposed scheme is demonstrated by numerical simulations. 1.1 This specification covers four classes of electrical insulating cement for coating porcelain, glass, and ceramic insulators. 1.1 This specification covers four classes of electrical insulating cement for coating porcelain, glass, and ceramic insulators. 1.2 This standard does not purport to address all of the safety concerns, if any, associated with its use. It is the responsibility of the user of this standard to establish appropriate safety and health practices and determine the applicability of regulatory limitations prior to use. 1.1 This specification covers four classes of electrical insulating cement for coating porcelain, glass, and ceramic insulators. 1.2 This standard does not purport to address all of the safety concerns, if any, associated with its use. It is the responsibility of the user of this standard to establish appropriate safety and health practices and determine the applicability of regulatory limitations prior to use. 1.3 This international standard was developed in accordance with internationally recognized principles on standardization established in the Decision on Principles for the Development of International Standards, Guides and Recommendations issued by the World Trade Organization Technical Barriers to Trade (TBT) Committee.", "label": "0", "is_selected": "0", "text": "In this paper, we consider a novel cache-enabled heterogeneous network (HetNet), where macro base stations (BSs) with traditional sub-6 GHz are overlaid by dense millimeter wave (mmWave) pico BSs. These mmWave pico BSs are deployed in cell-centers and cell-edges to support the heterogeneous traffic of different mobile users. The HetNet is also equipped with base station caches to enhance the traffic offloading and the data rate. To tackle the small cell traffic and the cache-hit ratios, we propose a joint resource management scheme, which simultaneously decides on the cache content placement, the transmit power allocation, and the content-user association. In particular, the content-user association is jointly determined by both the content-to-user association and the user-to-BS association. The objective is to maximize the total expected system throughput, which is a function of the cache content placement, the transmit power allocation, and the user association. The optimization problem is formulated as a mixed-integer nonlinear programming problem which is difficult to solve. Hence, to obtain a more tractable solution, we transform it into an equivalent optimization problem in terms of the probability distributions. The equivalent optimization problem is then solved via the Lyapunov optimization technique. Finally, the performance of the proposed scheme is demonstrated by numerical simulations. 1.1 This specification covers four classes of electrical insulating cement for coating porcelain, glass, and ceramic insulators. 1.1 This specification covers four classes of electrical insulating cement for coating porcelain, glass, and ceramic insulators. 1.2 This standard does not purport to address all of the safety concerns, if any, associated with its use. It is the responsibility of the user of this standard to establish appropriate safety and health practices and determine the applicability of regulatory limitations prior to use. 1.1 This specification covers four classes of electrical insulating cement for coating porcelain, glass, and ceramic insulators. 1.2 This standard does not purport to address all of the safety concerns, if any, associated with its use. It is the responsibility of the user of this standard to establish appropriate safety and health practices and determine the applicability of regulatory limitations prior to use. 1.3 This international standard was developed in accordance with internationally recognized principles on standardization established in the Decision on Principles for the Development of International Standards, Guides and Recommendations issued by the World Trade Organization Technical Barriers to Trade (TBT) Committee."}
{"original_text": "Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays past experiences and ignores their state-dependent impact on future experience. In this paper, we propose a recurrent neural network (RNN) memory architecture for experience replay. The RNN memory generates a sequence of past experiences based on their state-dependent relevance to the current experience. The proposed architecture is effective in several online reinforcement learning tasks. It outperforms prior approaches on the Atari benchmark, and significantly outperforms the current state-of-the-art on the MuJoCo locomotion benchmark. DeepMind is releasing a new version of its AI lab. The London-based company will make the platform, DeepMind Lab (DML), open-source today at the Neural Information Processing Systems conference in Barcelona, Spain. The move is an effort to accelerate AI research, which currently relies on data sets, such as the Atari 2600 game suite, that are decades old. DML creates a 3D environment for artificial intelligence agents to learn to solve tasks and model the world. The open-source toolkit will be available to anyone, including non-researchers. For instance, the company expects game designers to use it as a tool to create new types of games and new levels. This paper proposes a novel method to control a 3D bipedal walker using only a depth camera and a single forcetorque sensor. We provide a detailed description of how the control system can be decomposed into three distinct components: a physics-based optimization system, a feedforward controller, and a feedback controller. The latter two controllers are trained using a simple reward signal provided by the physics-based system, without any further supervision. Experiments with a real robot suggest that the three-component controller is able to learn to balance the robot and walk in only a few hours of training. Humans can learn to play a complex game in a few hours, or a complex sport in a few months, but traditional reinforcement learning (RL) methods cannot solve even simple versions of these tasks. Recently, \"imitation learning\" methods have shown promising results in learning from expert demonstrations, but they struggle in tasks with sparse rewards. We present a simple, efficient, and effective method, dubbed AggreVaTe, that combines imitation learning with RL. AggreVaTe uses an RL policy to take low-quality expert actions (from the expert or from itself), and uses them as input to a \"state aggregator\" function. State aggregation transforms the current state into a new state representation which improves the value estimate of the RL policy. The state aggregator is trained by imitation learning, using high-quality expert actions as input. Experiments in the Atari 2600 domain show that AggreVaTe outperforms state-of-the-art approaches to both imitation and RL, and significantly improves over previous combination techniques. A major open question in reinforcement learning is how agents should learn to control physical systems. The difficulty of this question is exacerbated by the fact that learning algorithms are typically evaluated in simulation, where they can be exposed to a disjoint set of experiences from what is possible in the real world. In this paper, we introduce an algorithm for learning from demonstrations that works directly on hardware. Using this method, a physical robot can learn to reproduce the behavior demonstrated in a video, or can be used to gather more experiences for improving an existing policy. We prove that our algorithm achieves an exponential improvement in sample complexity over naive trial-and-error learning, and demonstrate it in a series of simulated and real robotic control tasks. We present a method for incorporating adversarial information in a reinforcement learning (RL) agent that learns to play a multi-agent game directly from image observations. Specifically, we develop a reward network that is conditioned on an adversarial predictor of the agent's behavior. The reward network is optimized in conjunction with the policy network using standard policy gradient methods, resulting in a joint policy-value-reward function. In several challenging domains (including two-player games such as Go and two-player video games such as Space Invaders), we demonstrate that our method can learn successful policies while using less experience than standard policy gradient methods. Because reinforcement learning algorithms can take a long time to learn effective policies, it is often necessary to use simulation environments to accelerate learning. However, the policies that perform well in simulation are often ineffective on the real robot. In this paper, we propose a novel", "label": "0", "is_selected": "0", "text": "Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays past experiences and ignores their state-dependent impact on future experience. In this paper, we propose a recurrent neural network (RNN) memory architecture for experience replay. The RNN memory generates a sequence of past experiences based on their state-dependent relevance to the current experience. The proposed architecture is effective in several online reinforcement learning tasks. It outperforms prior approaches on the Atari benchmark, and significantly outperforms the current state-of-the-art on the MuJoCo locomotion benchmark. DeepMind is releasing a new version of its AI lab. The London-based company will make the platform, DeepMind Lab (DML), open-source today at the Neural Information Processing Systems conference in Barcelona, Spain. The move is an effort to accelerate AI research, which currently relies on data sets, such as the Atari 2600 game suite, that are decades old. DML creates a 3D environment for artificial intelligence agents to learn to solve tasks and model the world. The open-source toolkit will be available to anyone, including non-researchers. For instance, the company expects game designers to use it as a tool to create new types of games and new levels. This paper proposes a novel method to control a 3D bipedal walker using only a depth camera and a single forcetorque sensor. We provide a detailed description of how the control system can be decomposed into three distinct components: a physics-based optimization system, a feedforward controller, and a feedback controller. The latter two controllers are trained using a simple reward signal provided by the physics-based system, without any further supervision. Experiments with a real robot suggest that the three-component controller is able to learn to balance the robot and walk in only a few hours of training. Humans can learn to play a complex game in a few hours, or a complex sport in a few months, but traditional reinforcement learning (RL) methods cannot solve even simple versions of these tasks. Recently, \"imitation learning\" methods have shown promising results in learning from expert demonstrations, but they struggle in tasks with sparse rewards. We present a simple, efficient, and effective method, dubbed AggreVaTe, that combines imitation learning with RL. AggreVaTe uses an RL policy to take low-quality expert actions (from the expert or from itself), and uses them as input to a \"state aggregator\" function. State aggregation transforms the current state into a new state representation which improves the value estimate of the RL policy. The state aggregator is trained by imitation learning, using high-quality expert actions as input. Experiments in the Atari 2600 domain show that AggreVaTe outperforms state-of-the-art approaches to both imitation and RL, and significantly improves over previous combination techniques. A major open question in reinforcement learning is how agents should learn to control physical systems. The difficulty of this question is exacerbated by the fact that learning algorithms are typically evaluated in simulation, where they can be exposed to a disjoint set of experiences from what is possible in the real world. In this paper, we introduce an algorithm for learning from demonstrations that works directly on hardware. Using this method, a physical robot can learn to reproduce the behavior demonstrated in a video, or can be used to gather more experiences for improving an existing policy. We prove that our algorithm achieves an exponential improvement in sample complexity over naive trial-and-error learning, and demonstrate it in a series of simulated and real robotic control tasks. We present a method for incorporating adversarial information in a reinforcement learning (RL) agent that learns to play a multi-agent game directly from image observations. Specifically, we develop a reward network that is conditioned on an adversarial predictor of the agent's behavior. The reward network is optimized in conjunction with the policy network using standard policy gradient methods, resulting in a joint policy-value-reward function. In several challenging domains (including two-player games such as Go and two-player video games such as Space Invaders), we demonstrate that our method can learn successful policies while using less experience than standard policy gradient methods. Because reinforcement learning algorithms can take a long time to learn effective policies, it is often necessary to use simulation environments to accelerate learning. However, the policies that perform well in simulation are often ineffective on the real robot. In this paper, we propose a novel"}
{"original_text": "Some research institutions demand researchers to distribute the incomes they earn from publishing papers to their researchers andor co-authors. In this study, we deal with the Impact Factor-based ranking journal list, i.e. the. If you are a professor, and you want to publish a paper in a journal of decent quality, you can submit it to any journal and let the editors. The Journal of Economics, a peer-reviewed journal, provides a forum for the exchange of ideas and findings of research in economics.The Journal of Economics and Finance (JEF) publishes original, unpublished manuscripts related to contemporary issues in economics and finance. Journal of Economics and Finance: Research - ISSN: 2042-8003, E-ISSN: 2042-8011. The Journal of Economics and Finance: Research (JEFR) is a peer-reviewed journal published by Sciedu Press. Journal of Economics and Finance: Research (JEFR) aims to provide a good source of reference for policy makers, government agencies, academics and researchers in the field. The Journal of Economics and Finance is a journal devoted to the publication of high-quality articles in economics and finance. Submissions are accepted in all areas of economics and finance, including applied work.", "label": "0", "is_selected": "0", "text": "Some research institutions demand researchers to distribute the incomes they earn from publishing papers to their researchers andor co-authors. In this study, we deal with the Impact Factor-based ranking journal list, i.e. the. If you are a professor, and you want to publish a paper in a journal of decent quality, you can submit it to any journal and let the editors. The Journal of Economics, a peer-reviewed journal, provides a forum for the exchange of ideas and findings of research in economics.The Journal of Economics and Finance (JEF) publishes original, unpublished manuscripts related to contemporary issues in economics and finance. Journal of Economics and Finance: Research - ISSN: 2042-8003, E-ISSN: 2042-8011. The Journal of Economics and Finance: Research (JEFR) is a peer-reviewed journal published by Sciedu Press. Journal of Economics and Finance: Research (JEFR) aims to provide a good source of reference for policy makers, government agencies, academics and researchers in the field. The Journal of Economics and Finance is a journal devoted to the publication of high-quality articles in economics and finance. Submissions are accepted in all areas of economics and finance, including applied work."}
{"original_text": "The aim of this paper is to facilitate nuanced discussion around research norms and practices to mitigate the harmful impacts of advances in machine learning (ML). We focus particularly on the ML research community, and the ways that ML research and the advancement of the field contribute to harmful outcomes. We present a framework with which to analyze ML research in terms of the ethical and societal implications of harmful outcomes. We ground our analysis in existing work in ethics, social responsibility, and societal impact, and identify a set of important ethical considerations specific to ML research, including: the role of ethical foresight in design, the value of transparency and openness, and the duty to follow through. We conclude by arguing for the development of a more comprehensive understanding of the ways in which research can contribute to harmful outcomes, and how researchers can address these harms as they work towards the goals of their research. AB - The aim of this paper is to facilitate nuanced discussion around research norms and practices to mitigate the harmful impacts of advances in machine learning (ML). We focus particularly on the ML research community, and the ways that ML research and the advancement of the field contribute to harmful outcomes. We present a framework with which to analyze ML research in terms of the ethical and societal implications of harmful outcomes. We ground our analysis in existing work in ethics, social responsibility, and societal impact, and identify a set of important ethical considerations specific to ML research, including: the role of ethical foresight in design, the value of transparency and openness, and the duty to follow through. We conclude by arguing for the development of a more comprehensive understanding of the ways in which research can contribute to harmful outcomes, and how researchers can address these harms as they work towards the goals of their research.", "label": "0", "is_selected": "0", "text": "The aim of this paper is to facilitate nuanced discussion around research norms and practices to mitigate the harmful impacts of advances in machine learning (ML). We focus particularly on the ML research community, and the ways that ML research and the advancement of the field contribute to harmful outcomes. We present a framework with which to analyze ML research in terms of the ethical and societal implications of harmful outcomes. We ground our analysis in existing work in ethics, social responsibility, and societal impact, and identify a set of important ethical considerations specific to ML research, including: the role of ethical foresight in design, the value of transparency and openness, and the duty to follow through. We conclude by arguing for the development of a more comprehensive understanding of the ways in which research can contribute to harmful outcomes, and how researchers can address these harms as they work towards the goals of their research. AB - The aim of this paper is to facilitate nuanced discussion around research norms and practices to mitigate the harmful impacts of advances in machine learning (ML). We focus particularly on the ML research community, and the ways that ML research and the advancement of the field contribute to harmful outcomes. We present a framework with which to analyze ML research in terms of the ethical and societal implications of harmful outcomes. We ground our analysis in existing work in ethics, social responsibility, and societal impact, and identify a set of important ethical considerations specific to ML research, including: the role of ethical foresight in design, the value of transparency and openness, and the duty to follow through. We conclude by arguing for the development of a more comprehensive understanding of the ways in which research can contribute to harmful outcomes, and how researchers can address these harms as they work towards the goals of their research."}
{"original_text": "A new method for estimating the relative positions of location-unaware nodes from the location-aware nodes and the received signal strength (RSS) between the nodes, in a wireless sensor network (WSN), is proposed. In the proposed method, all the location-aware nodes are used as anchors, and the location-unaware nodes are considered as un-anchored nodes. The location of a location-unaware node is estimated by using the weighted center of gravity (WCG) method, and the weighting factor is calculated by using the distance and the RSS between the location-unaware node and the anchors. The distances between the location-unaware node and the anchors are estimated by using the two-way ranging (TWR) method and the RSS values are obtained from the reference values of the RSS. The RSS values can be obtained through the field measurement or simulation. The performance of the proposed method is compared with the previously proposed methods. As a result, the location estimation error of the proposed method is improved by more than 50 compared to the RSS-based methods and by more than 10 compared to the TWR-based methods. WSNs, which are composed of spatially distributed sensor nodes, have been widely used in various fields, such as remote monitoring, military surveillance, environmental monitoring, target tracking, and healthcare. In addition, various routing protocols have been proposed to increase the reliability of data transmission between the nodes in the WSNs. Recently, the location-aware nodes have been deployed in the WSNs for the data transmission between the location-aware nodes and the location-unaware nodes. In addition, the location-aware nodes can be used as the reference for estimating the location of the location-unaware nodes. Therefore, the location estimation of the location-unaware nodes is very important for constructing the WSNs. For the location estimation of the location-unaware nodes, various methods have been proposed. Among these methods, the location estimation based on the RSS values is simple to implement, and the location estimation based on the TWR is more accurate than the RSS-based methods. In this paper, a method for estimating the location of the location-unaware nodes, which is based on the RSS values and TWR between the location-aware nodes and the location-unaware nodes, is proposed. In the proposed method, the location-unaware nodes are considered as un-anchored nodes, and all the location-aware nodes are used as anchors. As a result, the proposed method can be used in all the scenarios of the WSNs. In the proposed method, the distance between the location-unaware node and the anchors is estimated by using the TWR method, and the RSS values between the location-unaware node and the anchors are obtained from the reference values of the RSS. The distance between the location-unaware node and the anchors can be estimated by using the TWR method, which is robust to noise. The RSS values between the location-unaware node and the anchors can be obtained through the field measurement or simulation. In this paper, the performance of the proposed method is compared with the previously proposed methods. As a result, the location estimation error of the proposed method is improved by more than 50 compared to the RSS-based methods and by more than 10 compared to the TWR-based methods. The rest of this paper is organized as follows. In Section 2, the related works are described. In Section 3, the proposed method is described. In Section 4, the performance of the proposed method is analyzed through the simulation and field measurement. Finally, the conclusion of this paper is presented in Section 5. where (x, y, z) is the location of the location-unaware node, ri is the distance from the location-unaware node to the ith anchor, is the standard deviation of the RSS, and is the number of the anchors. where i is the standard deviation of the RSS between the location-unaware node and the ith anchor. where d is the distance between the location-unaware node and the ith anchor, di,j is the distance between the ith anchor and the jth anchor, N is the number of the anchors, and l is the number of the anchors between the location-unaware node and the ith anchor. where si,j is the distance between the ith anchor and the jth anchor. where m is the number of the location-aware nodes, and is the weighting factor. where and are the reference values of the RSS between the location-unaware node and the ith anchor and between the ith", "label": "0", "is_selected": "0", "text": "A new method for estimating the relative positions of location-unaware nodes from the location-aware nodes and the received signal strength (RSS) between the nodes, in a wireless sensor network (WSN), is proposed. In the proposed method, all the location-aware nodes are used as anchors, and the location-unaware nodes are considered as un-anchored nodes. The location of a location-unaware node is estimated by using the weighted center of gravity (WCG) method, and the weighting factor is calculated by using the distance and the RSS between the location-unaware node and the anchors. The distances between the location-unaware node and the anchors are estimated by using the two-way ranging (TWR) method and the RSS values are obtained from the reference values of the RSS. The RSS values can be obtained through the field measurement or simulation. The performance of the proposed method is compared with the previously proposed methods. As a result, the location estimation error of the proposed method is improved by more than 50 compared to the RSS-based methods and by more than 10 compared to the TWR-based methods. WSNs, which are composed of spatially distributed sensor nodes, have been widely used in various fields, such as remote monitoring, military surveillance, environmental monitoring, target tracking, and healthcare. In addition, various routing protocols have been proposed to increase the reliability of data transmission between the nodes in the WSNs. Recently, the location-aware nodes have been deployed in the WSNs for the data transmission between the location-aware nodes and the location-unaware nodes. In addition, the location-aware nodes can be used as the reference for estimating the location of the location-unaware nodes. Therefore, the location estimation of the location-unaware nodes is very important for constructing the WSNs. For the location estimation of the location-unaware nodes, various methods have been proposed. Among these methods, the location estimation based on the RSS values is simple to implement, and the location estimation based on the TWR is more accurate than the RSS-based methods. In this paper, a method for estimating the location of the location-unaware nodes, which is based on the RSS values and TWR between the location-aware nodes and the location-unaware nodes, is proposed. In the proposed method, the location-unaware nodes are considered as un-anchored nodes, and all the location-aware nodes are used as anchors. As a result, the proposed method can be used in all the scenarios of the WSNs. In the proposed method, the distance between the location-unaware node and the anchors is estimated by using the TWR method, and the RSS values between the location-unaware node and the anchors are obtained from the reference values of the RSS. The distance between the location-unaware node and the anchors can be estimated by using the TWR method, which is robust to noise. The RSS values between the location-unaware node and the anchors can be obtained through the field measurement or simulation. In this paper, the performance of the proposed method is compared with the previously proposed methods. As a result, the location estimation error of the proposed method is improved by more than 50 compared to the RSS-based methods and by more than 10 compared to the TWR-based methods. The rest of this paper is organized as follows. In Section 2, the related works are described. In Section 3, the proposed method is described. In Section 4, the performance of the proposed method is analyzed through the simulation and field measurement. Finally, the conclusion of this paper is presented in Section 5. where (x, y, z) is the location of the location-unaware node, ri is the distance from the location-unaware node to the ith anchor, is the standard deviation of the RSS, and is the number of the anchors. where i is the standard deviation of the RSS between the location-unaware node and the ith anchor. where d is the distance between the location-unaware node and the ith anchor, di,j is the distance between the ith anchor and the jth anchor, N is the number of the anchors, and l is the number of the anchors between the location-unaware node and the ith anchor. where si,j is the distance between the ith anchor and the jth anchor. where m is the number of the location-aware nodes, and is the weighting factor. where and are the reference values of the RSS between the location-unaware node and the ith anchor and between the ith"}
{"original_text": "3-D image registration, which involves aligning two or more images, is a critical step in a variety of medical applications from diagnosis to therapy. Image registration is commonly performed by manual methods, such as using a mouse and a computer monitor to select corresponding points on the images. Automated image registration methods are more efficient and potentially more accurate; however, many automated methods do not account for the uncertainty in the registration parameters. This uncertainty can be used to identify the regions of the images that are most likely to have been misaligned. In this paper, we present a novel image registration method that uses a measurement-error model to take into account the uncertainty in the registration parameters. We use a measurement-error model because it is easy to implement, it allows for the separation of the registration and uncertainty steps, and it is interpretable. We apply the registration method to three simulated image pairs, each with increasing complexity, and we compare our results with the results obtained from a conventional registration method. We also apply our registration method to two pairs of human head MRI images and compare our results with the results obtained from the conventional registration method. In all cases, we found that the uncertainty estimates obtained from the measurement-error model registration method were more accurate than the uncertainty estimates obtained from the conventional registration method.", "label": "0", "is_selected": "0", "text": "3-D image registration, which involves aligning two or more images, is a critical step in a variety of medical applications from diagnosis to therapy. Image registration is commonly performed by manual methods, such as using a mouse and a computer monitor to select corresponding points on the images. Automated image registration methods are more efficient and potentially more accurate; however, many automated methods do not account for the uncertainty in the registration parameters. This uncertainty can be used to identify the regions of the images that are most likely to have been misaligned. In this paper, we present a novel image registration method that uses a measurement-error model to take into account the uncertainty in the registration parameters. We use a measurement-error model because it is easy to implement, it allows for the separation of the registration and uncertainty steps, and it is interpretable. We apply the registration method to three simulated image pairs, each with increasing complexity, and we compare our results with the results obtained from a conventional registration method. We also apply our registration method to two pairs of human head MRI images and compare our results with the results obtained from the conventional registration method. In all cases, we found that the uncertainty estimates obtained from the measurement-error model registration method were more accurate than the uncertainty estimates obtained from the conventional registration method."}
{"original_text": "Automatic annotation of temporal expressions is a research challenge of great interest in the field of information extraction. In this report, I describe a novel rule-based architecture, built on top of the Apache UIMA framework, for extracting temporal information from textual corpora. This architecture is not only scalable to large corpora, but also flexible in its ability to integrate multiple types of domain knowledge. A case study in the domain of weather is presented to demonstrate the architecture, and evaluation results for this case study are also presented.", "label": "0", "is_selected": "0", "text": "Automatic annotation of temporal expressions is a research challenge of great interest in the field of information extraction. In this report, I describe a novel rule-based architecture, built on top of the Apache UIMA framework, for extracting temporal information from textual corpora. This architecture is not only scalable to large corpora, but also flexible in its ability to integrate multiple types of domain knowledge. A case study in the domain of weather is presented to demonstrate the architecture, and evaluation results for this case study are also presented."}
{"original_text": "This paper offers a multi-disciplinary review of knowledge acquisition methods in human activity systems. The review captures the degree of involvement of various types of agencies in the knowledge acquisition process, and proposes a classification with three categories of methods: the human agent, the human-inspired agent, and the autonomous machine agent methods. In the first two categories, the acquisition of knowledge is seen as a cognitive task analysis exercise, while in the third category knowledge acquisition is treated as an autonomous knowledge-discovery endeavour. The motivation for this classification stems from the continuous change over time of the structure, meaning and purpose of human activity systems, which are seen as the factor that fuelled researchers' and practitioners' efforts in knowledge acquisition for more than a century. We show through this review that the KA field is increasingly active due to the higher and higher pace of change in human activity, and conclude by discussing the emergence of a fourth category of knowledge acquisition methods, which are based on red-teaming and co-evolution.", "label": "1", "is_selected": "0", "text": "This paper offers a multi-disciplinary review of knowledge acquisition methods in human activity systems. The review captures the degree of involvement of various types of agencies in the knowledge acquisition process, and proposes a classification with three categories of methods: the human agent, the human-inspired agent, and the autonomous machine agent methods. In the first two categories, the acquisition of knowledge is seen as a cognitive task analysis exercise, while in the third category knowledge acquisition is treated as an autonomous knowledge-discovery endeavour. The motivation for this classification stems from the continuous change over time of the structure, meaning and purpose of human activity systems, which are seen as the factor that fuelled researchers' and practitioners' efforts in knowledge acquisition for more than a century. We show through this review that the KA field is increasingly active due to the higher and higher pace of change in human activity, and conclude by discussing the emergence of a fourth category of knowledge acquisition methods, which are based on red-teaming and co-evolution."}
{"original_text": "Many clustering algorithms exist that estimate a cluster centroid, such as K -means, K -medoids or mean-shift, but no algorithm seems to exist that clusters data by returning exactly K meaningful modes. We propose a natural definition of a K -modes objective function by combining the notions of density and cluster assignment. The algorithm becomes K -means and K -medoids in the limit of very large and very small scales. Computationally, it is slightly slower than K -means but much faster than mean-shift or K -medoids. Unlike K -means, it is able to find centroids that are valid patterns, truly representative of a cluster, even with nonconvex clusters, and appears robust to outliers and misspecification of the scale and number of clusters.", "label": "1", "is_selected": "0", "text": "Many clustering algorithms exist that estimate a cluster centroid, such as K -means, K -medoids or mean-shift, but no algorithm seems to exist that clusters data by returning exactly K meaningful modes. We propose a natural definition of a K -modes objective function by combining the notions of density and cluster assignment. The algorithm becomes K -means and K -medoids in the limit of very large and very small scales. Computationally, it is slightly slower than K -means but much faster than mean-shift or K -medoids. Unlike K -means, it is able to find centroids that are valid patterns, truly representative of a cluster, even with nonconvex clusters, and appears robust to outliers and misspecification of the scale and number of clusters."}
{"original_text": "Automatic software plagiarism detection tools are widely used in educational settings to ensure that submitted work was not copied. These tools have grown in use together with the rise in online education and availability of the Internet. The software is used by students who submit work to the automatic plagiarism detection system and instructors who then evaluate the report. The instructor evaluates the report based on the tool's parameters, but the tool is often seen as objective. This study's research question was: How do students and instructors perceive the use of automatic software plagiarism detection tools in educational settings? This mixed methods case study was conducted at a university in South Carolina. Purposeful sampling was used to recruit participants. Data were collected from participant observation and interviews with 12 participants. The qualitative data were coded using NVivo software. The findings were that students and instructors perceived the use of automatic software plagiarism detection tools as problematic and had some concerns with it. However, some students and instructors also acknowledged the benefits of the tools. One finding was that students who reported they plagiarized felt the tool did not catch their plagiarism. The findings from this study can be used to inform educators and administrators about the benefits and drawbacks of using automatic software plagiarism detection tools. This dissertation has been published in ProQuest; Copyright 2016 by Michelle Cooks. All rights reserved. This work is protected against unauthorized copying under Title 17, United States Code. Cooks, Michelle, \"Examining the Use of Automatic Software Plagiarism Detection Tools in Higher Education\" (2016). Dissertations. 61.", "label": "0", "is_selected": "0", "text": "Automatic software plagiarism detection tools are widely used in educational settings to ensure that submitted work was not copied. These tools have grown in use together with the rise in online education and availability of the Internet. The software is used by students who submit work to the automatic plagiarism detection system and instructors who then evaluate the report. The instructor evaluates the report based on the tool's parameters, but the tool is often seen as objective. This study's research question was: How do students and instructors perceive the use of automatic software plagiarism detection tools in educational settings? This mixed methods case study was conducted at a university in South Carolina. Purposeful sampling was used to recruit participants. Data were collected from participant observation and interviews with 12 participants. The qualitative data were coded using NVivo software. The findings were that students and instructors perceived the use of automatic software plagiarism detection tools as problematic and had some concerns with it. However, some students and instructors also acknowledged the benefits of the tools. One finding was that students who reported they plagiarized felt the tool did not catch their plagiarism. The findings from this study can be used to inform educators and administrators about the benefits and drawbacks of using automatic software plagiarism detection tools. This dissertation has been published in ProQuest; Copyright 2016 by Michelle Cooks. All rights reserved. This work is protected against unauthorized copying under Title 17, United States Code. Cooks, Michelle, \"Examining the Use of Automatic Software Plagiarism Detection Tools in Higher Education\" (2016). Dissertations. 61."}
{"original_text": "This essay argues that a new form of democracy - an \"Emergent Democracy\" - will develop as a result of the use of Internet communication tools and platforms such as weblogs and Wiki. The emergent nature of the democracy stems from the emergent nature of these new tools. The paper argues that Emergent Democracy can be thought of as a combination of the following ideas: 1. Emergent Democracy is the ability of the system to support the deliberation of issues by the entire community and the harnessing of the \"wisdom of crowds\" to create the best outcome for the community. 2. Emergent Democracy is the ability of the system to support and encourage all individuals to participate in the deliberation of issues and the creation of outcomes. 3. Emergent Democracy is the ability of the system to encourage the evolution of the deliberation system to best support the needs of the community. The paper also provides a detailed discussion of the major hurdles to overcome in order for Emergent Democracy to take hold. In addition, the paper provides a number of recommendations that should be considered by both technologists and policy makers. Emergent Democracy Home We are at an important inflection point in the history of democracy. This is a time when the use of Internet communications tools and platforms such as weblogs and Wiki are becoming more prevalent. While these technologies have been with us for some time, only recently have they started to be used in a manner that provides the ability for the Internet to support democratic discussion and deliberation. In this paper, we define a new type of democracy - an \"Emergent Democracy\" - which has the potential to become a more important form of democracy than the representative democracy we have today. Emergent Democracy differs from representative democracy in a number of important ways: Emergent Democracy is the ability of the system to support the deliberation of issues by the entire community and the harnessing of the \"wisdom of crowds\" to create the best outcome for the community. Emergent Democracy is the ability of the system to support and encourage all individuals to participate in the deliberation of issues and the creation of outcomes. Emergent Democracy is the ability of the system to encourage the evolution of the deliberation system to best support the needs of the community. The technologies which enable Emergent Democracy differ from the technologies which enable representative democracy in a number of important ways. Whereas representative democracy is enabled by technologies which enable large scale public meetings, voting and media communications, Emergent Democracy is enabled by technologies which support the following: Synchronous and Asynchronous Communications Low-to-No Barriers for Entry Extensive Self-Organization It is important to note that it is not necessary for Emergent Democracy to replace representative democracy in order for it to be a success. In fact, it is likely that the most effective forms of democracy will be a combination of representative and Emergent Democracy. The remainder of the paper is organized as follows. Section 2 provides a brief overview of representative democracy and identifies some of the shortcomings of representative democracy. Section 3 provides a brief overview of some of the technologies which are enabling Emergent Democracy. Section 4 discusses some of the challenges that must be overcome in order for Emergent Democracy to develop and flourish. Representative democracy is the type of democracy we have today. In a representative democracy, the representatives are elected by the public. Once elected, the representatives meet in a variety of bodies to discuss and deliberate on issues. Once a consensus is reached, the consensus is translated into an outcome such as a law or a regulation. There are a number of shortcomings of representative democracy: Representative democracy is not inclusive. With the exception of voting, most people are not able to participate in the deliberation of issues and the creation of outcomes. Representative democracy is not very effective at harnessing the \"wisdom of crowds.\" In fact, representative democracy is often a method used to protect the status quo and avoid change. Representative democracy is not a very effective method for ensuring the evolution of the system. In fact, representatives and the systems they participate in are often resistant to change. In recent years, the Internet has emerged as an effective medium for communications and collaboration. The Internet has a number of characteristics which make it well-suited to a wide variety of uses, including democracy. Some of these characteristics include: The Internet is decentralized", "label": "0", "is_selected": "0", "text": "This essay argues that a new form of democracy - an \"Emergent Democracy\" - will develop as a result of the use of Internet communication tools and platforms such as weblogs and Wiki. The emergent nature of the democracy stems from the emergent nature of these new tools. The paper argues that Emergent Democracy can be thought of as a combination of the following ideas: 1. Emergent Democracy is the ability of the system to support the deliberation of issues by the entire community and the harnessing of the \"wisdom of crowds\" to create the best outcome for the community. 2. Emergent Democracy is the ability of the system to support and encourage all individuals to participate in the deliberation of issues and the creation of outcomes. 3. Emergent Democracy is the ability of the system to encourage the evolution of the deliberation system to best support the needs of the community. The paper also provides a detailed discussion of the major hurdles to overcome in order for Emergent Democracy to take hold. In addition, the paper provides a number of recommendations that should be considered by both technologists and policy makers. Emergent Democracy Home We are at an important inflection point in the history of democracy. This is a time when the use of Internet communications tools and platforms such as weblogs and Wiki are becoming more prevalent. While these technologies have been with us for some time, only recently have they started to be used in a manner that provides the ability for the Internet to support democratic discussion and deliberation. In this paper, we define a new type of democracy - an \"Emergent Democracy\" - which has the potential to become a more important form of democracy than the representative democracy we have today. Emergent Democracy differs from representative democracy in a number of important ways: Emergent Democracy is the ability of the system to support the deliberation of issues by the entire community and the harnessing of the \"wisdom of crowds\" to create the best outcome for the community. Emergent Democracy is the ability of the system to support and encourage all individuals to participate in the deliberation of issues and the creation of outcomes. Emergent Democracy is the ability of the system to encourage the evolution of the deliberation system to best support the needs of the community. The technologies which enable Emergent Democracy differ from the technologies which enable representative democracy in a number of important ways. Whereas representative democracy is enabled by technologies which enable large scale public meetings, voting and media communications, Emergent Democracy is enabled by technologies which support the following: Synchronous and Asynchronous Communications Low-to-No Barriers for Entry Extensive Self-Organization It is important to note that it is not necessary for Emergent Democracy to replace representative democracy in order for it to be a success. In fact, it is likely that the most effective forms of democracy will be a combination of representative and Emergent Democracy. The remainder of the paper is organized as follows. Section 2 provides a brief overview of representative democracy and identifies some of the shortcomings of representative democracy. Section 3 provides a brief overview of some of the technologies which are enabling Emergent Democracy. Section 4 discusses some of the challenges that must be overcome in order for Emergent Democracy to develop and flourish. Representative democracy is the type of democracy we have today. In a representative democracy, the representatives are elected by the public. Once elected, the representatives meet in a variety of bodies to discuss and deliberate on issues. Once a consensus is reached, the consensus is translated into an outcome such as a law or a regulation. There are a number of shortcomings of representative democracy: Representative democracy is not inclusive. With the exception of voting, most people are not able to participate in the deliberation of issues and the creation of outcomes. Representative democracy is not very effective at harnessing the \"wisdom of crowds.\" In fact, representative democracy is often a method used to protect the status quo and avoid change. Representative democracy is not a very effective method for ensuring the evolution of the system. In fact, representatives and the systems they participate in are often resistant to change. In recent years, the Internet has emerged as an effective medium for communications and collaboration. The Internet has a number of characteristics which make it well-suited to a wide variety of uses, including democracy. Some of these characteristics include: The Internet is decentralized"}
{"original_text": "Crowd counting from unconstrained scene images is a crucial task in many real-world applications like urban surveillance and management, but it is greatly challenged by the camera's perspective that causes huge appearance variations in people's scales and rotations. Conventional methods address such challenges by resorting to fixed multi-scale architectures that are often unable to cover the largely varied scales while ignoring the rotation variations. In this paper, we propose a unified neural network framework, named Deep Recurrent Spatial-Aware Network, which adaptively addresses the two issues in a learnable spatial transform module with a region-wise refinement process. Specifically, our framework incorporates a Recurrent Spatial-Aware Refinement (RSAR) module iteratively conducting two components: i) a Spatial Transformer Network that dynamically locates an attentional region from the crowd density map and transforms it to the suitable scale and rotation for optimal crowd estimation; ii) a Local Refinement Network that refines the density map of the attended region with residual learning. Extensive experiments on four challenging benchmarks show the effectiveness of our approach. Specifically, comparing with the existing best-performing methods, we achieve an improvement of 12 on the largest dataset WorldExpo'10 and 22.8 on the most challenging dataset UCFCC50.", "label": "1", "is_selected": "0", "text": "Crowd counting from unconstrained scene images is a crucial task in many real-world applications like urban surveillance and management, but it is greatly challenged by the camera's perspective that causes huge appearance variations in people's scales and rotations. Conventional methods address such challenges by resorting to fixed multi-scale architectures that are often unable to cover the largely varied scales while ignoring the rotation variations. In this paper, we propose a unified neural network framework, named Deep Recurrent Spatial-Aware Network, which adaptively addresses the two issues in a learnable spatial transform module with a region-wise refinement process. Specifically, our framework incorporates a Recurrent Spatial-Aware Refinement (RSAR) module iteratively conducting two components: i) a Spatial Transformer Network that dynamically locates an attentional region from the crowd density map and transforms it to the suitable scale and rotation for optimal crowd estimation; ii) a Local Refinement Network that refines the density map of the attended region with residual learning. Extensive experiments on four challenging benchmarks show the effectiveness of our approach. Specifically, comparing with the existing best-performing methods, we achieve an improvement of 12 on the largest dataset WorldExpo'10 and 22.8 on the most challenging dataset UCFCC50."}
{"original_text": "With wearable devices such as smartwatches on the rise in the consumer electronics market, securing these wearables is vital. However, the current security mechanisms only focus on validating the user not the device itself. Indeed, wearables can be (1) unauthorized wearable devices with correct credentials accessing valuable systems and networks, (2) passive insiders or outsider wearable devices, or (3) information-leaking wearables devices. Fingerprinting via machine learning can provide necessary cyber threat intelligence to address all these cyber attacks. In this work, we introduce a wearable fingerprinting technique focusing on Bluetooth classic protocol, which is a common protocol used by the wearables and other IoT devices. Specifically, we propose a non-intrusive wearable device identification framework which utilizes 20 different Machine Learning (ML) algorithms in the training phase of the classification process and selects the best performing algorithm for the testing phase. Furthermore, we evaluate the performance of proposed wearable fingerprinting technique on real wearable devices, including various off-the-shelf smartwatches. Our evaluation demonstrates the feasibility of the proposed technique to provide reliable cyber threat intelligence. Specifically, our detailed accuracy results show on average 98.5, 98.3 precision and recall for identifying wearables using the Bluetooth classic protocol.", "label": "1", "is_selected": "0", "text": "With wearable devices such as smartwatches on the rise in the consumer electronics market, securing these wearables is vital. However, the current security mechanisms only focus on validating the user not the device itself. Indeed, wearables can be (1) unauthorized wearable devices with correct credentials accessing valuable systems and networks, (2) passive insiders or outsider wearable devices, or (3) information-leaking wearables devices. Fingerprinting via machine learning can provide necessary cyber threat intelligence to address all these cyber attacks. In this work, we introduce a wearable fingerprinting technique focusing on Bluetooth classic protocol, which is a common protocol used by the wearables and other IoT devices. Specifically, we propose a non-intrusive wearable device identification framework which utilizes 20 different Machine Learning (ML) algorithms in the training phase of the classification process and selects the best performing algorithm for the testing phase. Furthermore, we evaluate the performance of proposed wearable fingerprinting technique on real wearable devices, including various off-the-shelf smartwatches. Our evaluation demonstrates the feasibility of the proposed technique to provide reliable cyber threat intelligence. Specifically, our detailed accuracy results show on average 98.5, 98.3 precision and recall for identifying wearables using the Bluetooth classic protocol."}
{"original_text": "In this letter we propose the Rao test as a simpler alternative to the generalized likelihood ratio test (GLRT) for multisensor fusion. We consider sensors observing an unknown deterministic parameter with symmetric and unimodal noise. A decision fusion center (DFC) receives quantized sensor observations through error-prone binary symmetric channels and makes a global decision. We analyze the optimal quantizer thresholds and we study the performance of the Rao test in comparison to the GLRT. Also, a theoretical comparison is made and asymptotic performance is derived in a scenario with homogeneous sensors. All the results are confirmed through simulations.", "label": "1", "is_selected": "0", "text": "In this letter we propose the Rao test as a simpler alternative to the generalized likelihood ratio test (GLRT) for multisensor fusion. We consider sensors observing an unknown deterministic parameter with symmetric and unimodal noise. A decision fusion center (DFC) receives quantized sensor observations through error-prone binary symmetric channels and makes a global decision. We analyze the optimal quantizer thresholds and we study the performance of the Rao test in comparison to the GLRT. Also, a theoretical comparison is made and asymptotic performance is derived in a scenario with homogeneous sensors. All the results are confirmed through simulations."}
{"original_text": "The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sentence along with its grammatical structure, are much less common. The main reason for this is the lack of resources, in particular, the absence of large datasets of annotated sentences, and the absence of methods for obtaining them. We propose a method for the training of a syntactic linearization system using a small number of annotated sentences. The approach is based on the use of language models for generating syntactic structures from linearized sentences. It is shown that the obtained system significantly outperforms the state-of-the-art statistical models in the linearization task.", "label": "0", "is_selected": "1", "text": "The aim of this paper is to develop a method for the training of a syntactic linearization system using a small number of annotated sentences. We propose a Method for the Training of a Syntactic Linearization System using a Small Number of Annotated Sentences."}
{"original_text": "In reinforcement learning (RL), agents often operate in partially observed and uncertain environments. Model-based RL suggests that this is best achieved by learning and exploiting a probabilistic model of the world. 'Active inference' is an emerging normative framework in cognitive and computational neuroscience that offers a unifying account of how biological agents achieve this. On this framework, inference, learning and action emerge from a single imperative to maximize the Bayesian evidence for a niched model of the world. However, implementations of this process have thus far been restricted to low-dimensional and idealized situations. Here, we present a working implementation of active inference that applies to high-dimensional tasks, with proof-of-principle results demonstrating efficient exploration and an order of magnitude increase in sample efficiency over strong model-free baselines. Our results demonstrate the feasibility of applying active inference at scale and highlight the operational homologies between active inference and current model-based approaches to RL.", "label": "1", "is_selected": "0", "text": "In reinforcement learning (RL), agents often operate in partially observed and uncertain environments. Model-based RL suggests that this is best achieved by learning and exploiting a probabilistic model of the world. 'Active inference' is an emerging normative framework in cognitive and computational neuroscience that offers a unifying account of how biological agents achieve this. On this framework, inference, learning and action emerge from a single imperative to maximize the Bayesian evidence for a niched model of the world. However, implementations of this process have thus far been restricted to low-dimensional and idealized situations. Here, we present a working implementation of active inference that applies to high-dimensional tasks, with proof-of-principle results demonstrating efficient exploration and an order of magnitude increase in sample efficiency over strong model-free baselines. Our results demonstrate the feasibility of applying active inference at scale and highlight the operational homologies between active inference and current model-based approaches to RL."}
{"original_text": "Synergistic interactions are ubiquitous in the real world. Recent studies have revealed that, for a single-layer network, synergy can enhance spreading and even induce an explosive contagion. There is at the present a growing interest in behavior spreading dynamics on multiplex networks. What is the role of synergistic interactions in behavior spreading in such networked systems? To address this question, we articulate a synergistic behavior spreading model on a double layer network, where the key manifestation of the synergistic interactions is that the adoption of one behavior by a node in one layer enhances its probability of adopting the behavior in the other layer. A general result is that synergistic interactions can greatly enhance the spreading of the behaviors in both layers. A remarkable phenomenon is that the interactions can alter the nature of the phase transition associated with behavior adoption or spreading dynamics. In particular, depending on the transmission rate of one behavior in a network layer, synergistic interactions can lead to a discontinuous (first-order) or a continuous (second-order) transition in the adoption scope of the other behavior with respect to its transmission rate. A surprising two-stage spreading process can arise: due to synergy, nodes having adopted one behavior in one layer adopt the other behavior in the other layer and then prompt the remaining nodes in this layer to quickly adopt the behavior. Analytically, we develop an edge-based compartmental theory and perform a bifurcation analysis to fully understand, in the weak synergistic interaction regime where the dynamical correlation between the network layers is negligible, the role of the interactions in promoting the social behavioral spreading dynamics in the whole system.", "label": "1", "is_selected": "0", "text": "Synergistic interactions are ubiquitous in the real world. Recent studies have revealed that, for a single-layer network, synergy can enhance spreading and even induce an explosive contagion. There is at the present a growing interest in behavior spreading dynamics on multiplex networks. What is the role of synergistic interactions in behavior spreading in such networked systems? To address this question, we articulate a synergistic behavior spreading model on a double layer network, where the key manifestation of the synergistic interactions is that the adoption of one behavior by a node in one layer enhances its probability of adopting the behavior in the other layer. A general result is that synergistic interactions can greatly enhance the spreading of the behaviors in both layers. A remarkable phenomenon is that the interactions can alter the nature of the phase transition associated with behavior adoption or spreading dynamics. In particular, depending on the transmission rate of one behavior in a network layer, synergistic interactions can lead to a discontinuous (first-order) or a continuous (second-order) transition in the adoption scope of the other behavior with respect to its transmission rate. A surprising two-stage spreading process can arise: due to synergy, nodes having adopted one behavior in one layer adopt the other behavior in the other layer and then prompt the remaining nodes in this layer to quickly adopt the behavior. Analytically, we develop an edge-based compartmental theory and perform a bifurcation analysis to fully understand, in the weak synergistic interaction regime where the dynamical correlation between the network layers is negligible, the role of the interactions in promoting the social behavioral spreading dynamics in the whole system."}
{"original_text": "Existing works on control of tractor-trailers systems only consider the kinematics model without taking dynamics into account. Also, most of them treat the issue as a pure control theory problem and ignore the challenges of a practical implementation. In this paper, we present a novel control architecture for autonomous tractor-trailer platooning in an over-the-air (OTA) environment. The control architecture is based on a hierarchical control framework. The framework is composed of two layers: a high-level control layer and a low-level control layer. The former controls the speed and trajectory of the tractor-trailer systems, while the latter controls the states of the systems. The proposed control architecture has several desirable properties: the overall robustness of the platoon is increased; the centralized control system is converted to a distributed system; and a safety margin is guaranteed. In addition, we use a multi-layer perceptron neural network to approximate the trajectory of the tractor. We also present a dynamic programming based multi-objective optimization algorithm to find the optimal trajectory of the tractor-trailer systems. Finally, we present simulation results to validate the proposed control architecture and OTA control strategy.", "label": "0", "is_selected": "1", "text": "Control of tractor-trailers systems in over-the-air (OTA) environments has been a challenge for researchers for a long time. We also present a dynamic programming based multi-objective optimization algorithm to find the optimal trajectory of the tractor- trailer systems."}
{"original_text": "The current gold standard for human activity recognition (HAR) is based on the use of cameras. However, the poor scalability of camera systems renders them impractical in pursuit of the next step in human activity recognition: continuous monitoring of a population for public health and safety applications. In this paper, we propose a novel approach to human activity recognition using only WiFi signals, by exploiting the fact that human motion induces multipath components in the received WiFi signals due to reflection, diffraction and scattering from moving objects. Human motion has a very low impact on the power of the direct path component of the received signal, but a larger effect on the power of multipath components. We propose a probabilistic model for the received signal based on channel impulse response (CIR) and employ statistical features of the CIR and signal-to-noise ratio (SNR) in the time and frequency domains. Our approach is evaluated using a publicly available WiFi dataset with ground truth HAR labels. We demonstrate that our approach achieves an overall accuracy of 83.90 on four activity classes: fall down, walking, walking upstairs and walking downstairs. 2016 IEEE. Akbar, I., Chaouchi, L., Croft, B. (2016). Multipath Features for Human Activity Recognition Using WiFi. (pp. 320-325). Kona, HI, US: IEEE. Akbar, Ibrahim, Lilia Chaouchi, and Brian Croft. \"Multipath Features for Human Activity Recognition Using WiFi.\" Proceedings of the 2016 13th International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services, MOBIQUITOUS 2016, Kona, HI IEEE, 2016. 320-325.", "label": "0", "is_selected": "1", "text": "A novel approach to human activity recognition using only WiFi signals, by exploiting the fact that human motion induces multipath components in the received WiFi signals due to reflection, diffraction and scattering from moving objects."}
{"original_text": "We prove that for every proper minor-closed class M of F p -representable matroids, there exists a O (1) -competitive algorithm for the matroid secretary problem on M. This result relies on the extremely powerful matroid minor structure theory being developed by Geelen, Gerards and Whittle. We also note that for asymptotically almost all matroids, the matroid secretary algorithm that selects a random basis, ignoring weights, is (2 o (1 -competitive. In fact, assuming the conjecture that almost all matroids are paving, there is a (1 o (1 -competitive algorithm for almost all matroids.", "label": "1", "is_selected": "0", "text": "We prove that for every proper minor-closed class M of F p -representable matroids, there exists a O (1) -competitive algorithm for the matroid secretary problem on M. This result relies on the extremely powerful matroid minor structure theory being developed by Geelen, Gerards and Whittle. We also note that for asymptotically almost all matroids, the matroid secretary algorithm that selects a random basis, ignoring weights, is (2 o (1 -competitive. In fact, assuming the conjecture that almost all matroids are paving, there is a (1 o (1 -competitive algorithm for almost all matroids."}
{"original_text": "The analysis of biological sequencing data has been one of the biggest applications of string algorithms. The approaches used in many such applications are based on the analysis of k -mers, which are short fixed-length strings present in a dataset. While these approaches are rather diverse, storing and querying k -mer sets has emerged as a shared underlying component. Sets of k -mers have unique features and applications that, over the last ten years, have resulted in many specialized approaches for their representation. In this survey, we give a unified presentation and comparison of the data structures that have been proposed to store and query k -mer sets. We hope this survey will not only serve as a resource for researchers in the field but also make the area more accessible to outsiders.", "label": "1", "is_selected": "0", "text": "The analysis of biological sequencing data has been one of the biggest applications of string algorithms. The approaches used in many such applications are based on the analysis of k -mers, which are short fixed-length strings present in a dataset. While these approaches are rather diverse, storing and querying k -mer sets has emerged as a shared underlying component. Sets of k -mers have unique features and applications that, over the last ten years, have resulted in many specialized approaches for their representation. In this survey, we give a unified presentation and comparison of the data structures that have been proposed to store and query k -mer sets. We hope this survey will not only serve as a resource for researchers in the field but also make the area more accessible to outsiders."}
{"original_text": "Data augmentation in deep neural networks is the process of generating artificial data in order to reduce the variance of the classifier with the goal to reduce the number of errors. This idea has been shown to improve deep neural network's generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike in image recognition problems, data augmentation techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved, especially for small datasets that exhibit overfitting, when a data augmentation method is adopted. In this paper, we fill this gap by investigating the application of a recently proposed data augmentation technique based on the Dynamic Time Warping distance, for a deep learning model for TSC. To evaluate the potential of augmenting the training set, we performed extensive experiments using the UCR TSC benchmark. Our preliminary experiments reveal that data augmentation can drastically increase deep CNN's accuracy on some datasets and significantly improve the deep model's accuracy when the method is used in an ensemble approach.", "label": "1", "is_selected": "0", "text": "Data augmentation in deep neural networks is the process of generating artificial data in order to reduce the variance of the classifier with the goal to reduce the number of errors. This idea has been shown to improve deep neural network's generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike in image recognition problems, data augmentation techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved, especially for small datasets that exhibit overfitting, when a data augmentation method is adopted. In this paper, we fill this gap by investigating the application of a recently proposed data augmentation technique based on the Dynamic Time Warping distance, for a deep learning model for TSC. To evaluate the potential of augmenting the training set, we performed extensive experiments using the UCR TSC benchmark. Our preliminary experiments reveal that data augmentation can drastically increase deep CNN's accuracy on some datasets and significantly improve the deep model's accuracy when the method is used in an ensemble approach."}
{"original_text": "Recent research on Automatic Chord Extraction (ACE) has focused on the improvement of models based on machine learning. However, most models still fail to take into account the prior knowledge that the pitch classes are contained in chords. The aim of this paper is to use this prior knowledge in ACE models. We extend two existing chord recognition models to incorporate this prior knowledge. The extended models are compared to the original models on a dataset containing 3,293 chord annotations. The results show that the extended models perform better than the original models. The results also show that the extended models perform better for chords with at least one octave between the lowest and highest pitch class.", "label": "0", "is_selected": "1", "text": "Chord recognition is one of the most important techniques used to extract musical notes from computer generated music files. The results also show that the extended models perform better for chords with at least one octave between the lowest and highest pitch class."}
{"original_text": "The digital identity problem is a complex one in large part because it involves personal data, the algorithms which compute reputations on the data and the management of the identifiers that are linked to personal data. The reality of today is that personal data of an individual is distributed throughout the Internet, in both private and public institutions, and increasingly also on the user's devices. In order to empower individuals to have a say in who has access to their personal data and to enable individuals to make use of their data for their own purposes, a coherent and scalable access authorization architecture is required. Such an architecture must allow different data holders, data providers and user-content generators to respond to an individual's wishes with regards to consent in a federated fashion. This federation must allow an individual to easily manage access policies and provide consent as required by current and forthcoming data privacy regulations. This paper describes the User Managed Access (UMA) architecture and protocols that provide the foundation for scalable access authorization.", "label": "1", "is_selected": "0", "text": "The digital identity problem is a complex one in large part because it involves personal data, the algorithms which compute reputations on the data and the management of the identifiers that are linked to personal data. The reality of today is that personal data of an individual is distributed throughout the Internet, in both private and public institutions, and increasingly also on the user's devices. In order to empower individuals to have a say in who has access to their personal data and to enable individuals to make use of their data for their own purposes, a coherent and scalable access authorization architecture is required. Such an architecture must allow different data holders, data providers and user-content generators to respond to an individual's wishes with regards to consent in a federated fashion. This federation must allow an individual to easily manage access policies and provide consent as required by current and forthcoming data privacy regulations. This paper describes the User Managed Access (UMA) architecture and protocols that provide the foundation for scalable access authorization."}
{"original_text": "A theory explaining how deep learning works is yet to be developed. Previous work suggests that deep learning performs a coarse graining, similar in spirit to the renormalization group (RG). This idea has been explored in the setting of a local (nearest neighbor interactions) Ising spin lattice. We extend the discussion to the setting of a long range spin lattice. Markov Chain Monte Carlo (MCMC) simulations determine both the critical temperature and scaling dimensions of the system. The model is used to train both a single RBM (restricted Boltzmann machine) network, as well as a stacked RBM network. Following earlier Ising model studies, the trained weights of a single layer RBM network define a flow of lattice models. In contrast to results for nearest neighbor Ising, the RBM flow for the long ranged model does not converge to the correct values for the spin and energy scaling dimension. Further, correlation functions between visible and hidden nodes exhibit key differences between the stacked RBM and RG flows. The stacked RBM flow appears to move towards low temperatures whereas the RG flow moves towards high temperature. This again differs from results obtained for nearest neighbor Ising.", "label": "1", "is_selected": "0", "text": "A theory explaining how deep learning works is yet to be developed. Previous work suggests that deep learning performs a coarse graining, similar in spirit to the renormalization group (RG). This idea has been explored in the setting of a local (nearest neighbor interactions) Ising spin lattice. We extend the discussion to the setting of a long range spin lattice. Markov Chain Monte Carlo (MCMC) simulations determine both the critical temperature and scaling dimensions of the system. The model is used to train both a single RBM (restricted Boltzmann machine) network, as well as a stacked RBM network. Following earlier Ising model studies, the trained weights of a single layer RBM network define a flow of lattice models. In contrast to results for nearest neighbor Ising, the RBM flow for the long ranged model does not converge to the correct values for the spin and energy scaling dimension. Further, correlation functions between visible and hidden nodes exhibit key differences between the stacked RBM and RG flows. The stacked RBM flow appears to move towards low temperatures whereas the RG flow moves towards high temperature. This again differs from results obtained for nearest neighbor Ising."}
{"original_text": "Visible light communications (VLC) have emerged as strong candidates for meeting the escalating demand for high data rates. Consider a VLC network, where multiple access-points (APs) serve both energy-harvesting users (EHUs), i.e., users which harvest energy from light intensity, and information-users (IUs), i.e., users which gather data information. In order to jointly balance the achievable sum-rate at the IUs and the energy harvested by the EHUs, the paper considers maximizing a network-wide utility, which consists of a weighted-sum of the IUs sum-rate and the EHUs harvested-energy, subject to individual IU rate constraint, individual EHU harvested-energy constraint, and AP power constraints, so as to jointly determine the direct current (DC) -bias value at each AP, and the users' powers. The paper solves such a difficult non-convex optimization problem using an iterative approach which relies on inner convex approximations, and compensates for the used approximations using proper outer-loop updates. The paper further considers solving the special cases of the problem, i.e., maximizing the sum-rate, and maximizing the total harvested-energy, both subject to the same constraints. Numerical results highlight the significant performance improvement of the proposed algorithms, and illustrate the impacts of the network parameters on the performance trade-off between the sum-rate and harvested-energy.", "label": "1", "is_selected": "0", "text": "Visible light communications (VLC) have emerged as strong candidates for meeting the escalating demand for high data rates. Consider a VLC network, where multiple access-points (APs) serve both energy-harvesting users (EHUs), i.e., users which harvest energy from light intensity, and information-users (IUs), i.e., users which gather data information. In order to jointly balance the achievable sum-rate at the IUs and the energy harvested by the EHUs, the paper considers maximizing a network-wide utility, which consists of a weighted-sum of the IUs sum-rate and the EHUs harvested-energy, subject to individual IU rate constraint, individual EHU harvested-energy constraint, and AP power constraints, so as to jointly determine the direct current (DC) -bias value at each AP, and the users' powers. The paper solves such a difficult non-convex optimization problem using an iterative approach which relies on inner convex approximations, and compensates for the used approximations using proper outer-loop updates. The paper further considers solving the special cases of the problem, i.e., maximizing the sum-rate, and maximizing the total harvested-energy, both subject to the same constraints. Numerical results highlight the significant performance improvement of the proposed algorithms, and illustrate the impacts of the network parameters on the performance trade-off between the sum-rate and harvested-energy."}
{"original_text": "We consider the stochastic extensible bin packing problem (SEBP) in which n items of stochastic size are packed into m bins of unit capacity. In contrast to the classical bin packing problem, the number of bins is fixed and they can be extended at extra cost. This problem plays an important role in stochastic environments such as in surgery scheduling: Patients must be assigned to operating rooms beforehand, such that the regular capacity is fully utilized while the amount of overtime is as small as possible. This paper focuses on essential ratios between different classes of policies: First, we consider the price of non-splittability, in which we compare the optimal non-anticipatory policy against the optimal fractional assignment policy. We show that this ratio has a tight upper bound of 2. Moreover, we develop an analysis of a fixed assignment variant of the LEPT rule yielding a tight approximation ratio of (1 e - 1) 1.368. Furthermore, we prove that the price of fixed assignments, related to the benefit of adaptivity, which describes the loss when restricting to fixed assignment policies, is within the same factor. This shows that in some sense, LEPT is the best fixed assignment policy we can hope for. We also provide a lower bound on the performance of this policy comparing against an optimal fixed assignment policy. Finally, we obtain improved bounds for the case where the processing times are drawn from a particular family of distributions, with either a bounded Pietra index or when the familly is stochastically dominated at the second order.", "label": "1", "is_selected": "0", "text": "We consider the stochastic extensible bin packing problem (SEBP) in which n items of stochastic size are packed into m bins of unit capacity. In contrast to the classical bin packing problem, the number of bins is fixed and they can be extended at extra cost. This problem plays an important role in stochastic environments such as in surgery scheduling: Patients must be assigned to operating rooms beforehand, such that the regular capacity is fully utilized while the amount of overtime is as small as possible. This paper focuses on essential ratios between different classes of policies: First, we consider the price of non-splittability, in which we compare the optimal non-anticipatory policy against the optimal fractional assignment policy. We show that this ratio has a tight upper bound of 2. Moreover, we develop an analysis of a fixed assignment variant of the LEPT rule yielding a tight approximation ratio of (1 e - 1) 1.368. Furthermore, we prove that the price of fixed assignments, related to the benefit of adaptivity, which describes the loss when restricting to fixed assignment policies, is within the same factor. This shows that in some sense, LEPT is the best fixed assignment policy we can hope for. We also provide a lower bound on the performance of this policy comparing against an optimal fixed assignment policy. Finally, we obtain improved bounds for the case where the processing times are drawn from a particular family of distributions, with either a bounded Pietra index or when the familly is stochastically dominated at the second order."}
{"original_text": "While large scale pre-trained language models such as BERT have achieved great success on various natural language understanding tasks, how to efficiently and effectively incorporate them into sequence-to-sequence models and the corresponding text generation tasks remains a non-trivial problem. In this paper, we propose to address this problem by taking two different BERT models as the encoder and decoder respectively, and fine-tuning them by introducing simple and lightweight adapter modules, which are inserted between BERT layers and tuned on the task-specific dataset. In this way, we obtain a flexible and efficient model which is able to jointly leverage the information contained in the source-side and target-side BERT models, while bypassing the catastrophic forgetting problem. Each component in the framework can be considered as a plug-in unit, making the framework flexible and task agnostic. Our framework is based on a parallel sequence decoding algorithm named Mask-Predict considering the bi-directional and conditional independent nature of BERT, and can be adapted to traditional autoregressive decoding easily. We conduct extensive experiments on neural machine translation tasks where the proposed method consistently outperforms autoregressive baselines while reducing the inference latency by half, and achieves 36.49 33.57 BLEU scores on IWSLT14 German-EnglishWMT14 German-English translation. When adapted to autoregressive decoding, the proposed method achieves 30.60 43.56 BLEU scores on WMT14 English-GermanEnglish-French translation, on par with the state-of-the-art baseline models.", "label": "1", "is_selected": "0", "text": "While large scale pre-trained language models such as BERT have achieved great success on various natural language understanding tasks, how to efficiently and effectively incorporate them into sequence-to-sequence models and the corresponding text generation tasks remains a non-trivial problem. In this paper, we propose to address this problem by taking two different BERT models as the encoder and decoder respectively, and fine-tuning them by introducing simple and lightweight adapter modules, which are inserted between BERT layers and tuned on the task-specific dataset. In this way, we obtain a flexible and efficient model which is able to jointly leverage the information contained in the source-side and target-side BERT models, while bypassing the catastrophic forgetting problem. Each component in the framework can be considered as a plug-in unit, making the framework flexible and task agnostic. Our framework is based on a parallel sequence decoding algorithm named Mask-Predict considering the bi-directional and conditional independent nature of BERT, and can be adapted to traditional autoregressive decoding easily. We conduct extensive experiments on neural machine translation tasks where the proposed method consistently outperforms autoregressive baselines while reducing the inference latency by half, and achieves 36.49 33.57 BLEU scores on IWSLT14 German-EnglishWMT14 German-English translation. When adapted to autoregressive decoding, the proposed method achieves 30.60 43.56 BLEU scores on WMT14 English-GermanEnglish-French translation, on par with the state-of-the-art baseline models."}
{"original_text": "The security of cryptographic communication protocols that use X.509 certificates depends on the correctness of those certificates. This paper proposes a system that helps to ensure the correct operation of an X.509 certification authority and its registration authorities. We achieve this goal by enforcing a policy-defined, multi-party validation and authorization workflow of certificate signing requests. Besides, our system offers full accountability for this workflow for forensic purposes. As a foundation for our implementation, we leverage the distributed ledger and smart contract framework Hyperledger Fabric. Our implementation inherits the strong tamper-resistance of Fabric which strengthens the integrity of the computer processes that enforce the validation and authorization of the certificate signing request, and of the metadata collected during certificate issuance.", "label": "1", "is_selected": "0", "text": "The security of cryptographic communication protocols that use X.509 certificates depends on the correctness of those certificates. This paper proposes a system that helps to ensure the correct operation of an X.509 certification authority and its registration authorities. We achieve this goal by enforcing a policy-defined, multi-party validation and authorization workflow of certificate signing requests. Besides, our system offers full accountability for this workflow for forensic purposes. As a foundation for our implementation, we leverage the distributed ledger and smart contract framework Hyperledger Fabric. Our implementation inherits the strong tamper-resistance of Fabric which strengthens the integrity of the computer processes that enforce the validation and authorization of the certificate signing request, and of the metadata collected during certificate issuance."}
{"original_text": "This case for the Transformation Tool Contest 2013 is about evaluating the scope and usability of transformation languages and tools for a set of four tasks requiring very different capabilities. One task deals with typical model-to-model transformation problem, there's a model-to-text problem, there are two in-place transformation problems, and finally there's a task dealing with validation of models resulting from the transformations. The tasks build upon each other, but the transformation case project also provides all intermediate models, thus making it possible to skip tasks that are not suited for a particular tool, or for parallelizing the work among members of participating teams.", "label": "1", "is_selected": "0", "text": "This case for the Transformation Tool Contest 2013 is about evaluating the scope and usability of transformation languages and tools for a set of four tasks requiring very different capabilities. One task deals with typical model-to-model transformation problem, there's a model-to-text problem, there are two in-place transformation problems, and finally there's a task dealing with validation of models resulting from the transformations. The tasks build upon each other, but the transformation case project also provides all intermediate models, thus making it possible to skip tasks that are not suited for a particular tool, or for parallelizing the work among members of participating teams."}
{"original_text": "In this paper we study the problem of designing a distributed graph visualization algorithm for large graphs. The algorithm must be simple to implement and the computing infrastructure must not require major hardware or software investments. We design, implement, and experiment a force-directed algorithm in Giraph, a popular open source framework for distributed computing, based on a vertex-centric design paradigm. The algorithm is tested both on real and artificial graphs with up to million edges, by using a rather inexpensive PaaS (Platform as a Service) infrastructure of Amazon. The experiments show the scalability and effectiveness of our technique when compared to a centralized implementation of the same force-directed model. We show that graphs with about one million edges can be drawn in less than 8 minutes, by spending about 1 per drawing in the cloud computing infrastructure.", "label": "1", "is_selected": "0", "text": "In this paper we study the problem of designing a distributed graph visualization algorithm for large graphs. The algorithm must be simple to implement and the computing infrastructure must not require major hardware or software investments. We design, implement, and experiment a force-directed algorithm in Giraph, a popular open source framework for distributed computing, based on a vertex-centric design paradigm. The algorithm is tested both on real and artificial graphs with up to million edges, by using a rather inexpensive PaaS (Platform as a Service) infrastructure of Amazon. The experiments show the scalability and effectiveness of our technique when compared to a centralized implementation of the same force-directed model. We show that graphs with about one million edges can be drawn in less than 8 minutes, by spending about 1 per drawing in the cloud computing infrastructure."}
{"original_text": "Due to their simple construction, LFSRs are commonly used as building blocks in various random number generators. Nonlinear feedforward logic is incorporated in LFSRs to increase the linear complexity of the generated sequence. In this work, we extend the idea of nonlinear feedforward logic to LFSRs over arbitrary finite fields and analyze the statistical properties of the generated sequences. Further, we propose a method of applying nonlinear feedforward logic to word-based s -LFSRs and show that the proposed scheme generates vector sequences that are statistically more balanced than those generated by an existing scheme.", "label": "1", "is_selected": "0", "text": "Due to their simple construction, LFSRs are commonly used as building blocks in various random number generators. Nonlinear feedforward logic is incorporated in LFSRs to increase the linear complexity of the generated sequence. In this work, we extend the idea of nonlinear feedforward logic to LFSRs over arbitrary finite fields and analyze the statistical properties of the generated sequences. Further, we propose a method of applying nonlinear feedforward logic to word-based s -LFSRs and show that the proposed scheme generates vector sequences that are statistically more balanced than those generated by an existing scheme."}
{"original_text": "A generally intelligent agent must be able to teach itself how to solve problems in complex domains with minimal human supervision. Recently, deep reinforcement learning algorithms combined with self-play have proven highly effective in many difficult tasks, ranging from playing Atari 2600 video games to go. However, many of these algorithms require large amounts of computing power, which makes them impractical for applications where the environment can change over time and the agent must continuously adapt to these changes. We propose to attack this problem by creating a method for agents to learn to teach themselves new tasks in parallel, by acting as both student and teacher. This allows the agent to teach itself a new skill in parallel while continuing to collect experience in the environment and improve in its original task. Our work will be based on an extension of the popular deep Q-learning algorithm, which will be implemented in a video game environment, with a curriculum that will be automatically generated to teach the agent the game mechanics in a progressive way. Jean-Paul is a PhD student in computer science at the University of Alberta. He is interested in machine learning and its applications to robotics. His current work focuses on building a deep reinforcement learning agent that can teach itself the rules of a video game and eventually beat human players. He also has a keen interest in artificial general intelligence, and he would like to use his research to address some of the ethical issues surrounding this field.", "label": "0", "is_selected": "0", "text": "A generally intelligent agent must be able to teach itself how to solve problems in complex domains with minimal human supervision. Recently, deep reinforcement learning algorithms combined with self-play have proven highly effective in many difficult tasks, ranging from playing Atari 2600 video games to go. However, many of these algorithms require large amounts of computing power, which makes them impractical for applications where the environment can change over time and the agent must continuously adapt to these changes. We propose to attack this problem by creating a method for agents to learn to teach themselves new tasks in parallel, by acting as both student and teacher. This allows the agent to teach itself a new skill in parallel while continuing to collect experience in the environment and improve in its original task. Our work will be based on an extension of the popular deep Q-learning algorithm, which will be implemented in a video game environment, with a curriculum that will be automatically generated to teach the agent the game mechanics in a progressive way. Jean-Paul is a PhD student in computer science at the University of Alberta. He is interested in machine learning and its applications to robotics. His current work focuses on building a deep reinforcement learning agent that can teach itself the rules of a video game and eventually beat human players. He also has a keen interest in artificial general intelligence, and he would like to use his research to address some of the ethical issues surrounding this field."}
{"original_text": "Multi-compartment modeling of diffusion-weighted magnetic resonance imaging measurements is necessary for accurate brain connectivity analysis. Existing methods for estimating the number and orientations of fascicles in an imaging voxel either depend on non-convex optimization techniques that are sensitive to initialization and measurement noise, or are prone to predicting spurious fascicles. In this paper, we propose a machine learning-based technique that can accurately estimate the number and orientations of fascicles in a voxel. Our method can be trained with either simulated or real diffusion-weighted imaging data. Our method estimates the angle to the closest fascicle for each direction in a set of discrete directions uniformly spread on the unit sphere. This information is then processed to extract the number and orientations of fascicles in a voxel. On realistic simulated phantom data with known ground truth, our method predicts the number and orientations of crossing fascicles more accurately than several existing methods. It also leads to more accurate tractography. On real data, our method is better than or compares favorably with standard methods in terms of robustness to measurement down-sampling and also in terms of expert quality assessment of tractography results.", "label": "1", "is_selected": "0", "text": "Multi-compartment modeling of diffusion-weighted magnetic resonance imaging measurements is necessary for accurate brain connectivity analysis. Existing methods for estimating the number and orientations of fascicles in an imaging voxel either depend on non-convex optimization techniques that are sensitive to initialization and measurement noise, or are prone to predicting spurious fascicles. In this paper, we propose a machine learning-based technique that can accurately estimate the number and orientations of fascicles in a voxel. Our method can be trained with either simulated or real diffusion-weighted imaging data. Our method estimates the angle to the closest fascicle for each direction in a set of discrete directions uniformly spread on the unit sphere. This information is then processed to extract the number and orientations of fascicles in a voxel. On realistic simulated phantom data with known ground truth, our method predicts the number and orientations of crossing fascicles more accurately than several existing methods. It also leads to more accurate tractography. On real data, our method is better than or compares favorably with standard methods in terms of robustness to measurement down-sampling and also in terms of expert quality assessment of tractography results."}
{"original_text": "In this paper, we study integrated estimation and control of soft robots. A significant challenge in deploying closed loop controllers is reliable proprioception via integrated sensing in soft robots. Despite the considerable advances accomplished in fabrication, modelling, and model-based control of soft robots, integrated sensing and estimation is still in its infancy. To that end, this paper introduces a new method of estimating the degree of curvature of a soft robot using a stretchable sensing skin. The skin is a spray-coated piezoresistive sensing layer on a latex membrane. The mapping from the strain signal to the degree of curvature is estimated by using a recurrent neural network. We investigate uni-directional bending as well as bi-directional bending of a single-segment soft robot. Moreover, an adaptive controller is developed to track the degree of curvature of the soft robot in the presence of dynamic uncertainties. Subsequently, using the integrated soft sensing skin, we experimentally demonstrate successful curvature tracking control of the soft robot.", "label": "1", "is_selected": "0", "text": "In this paper, we study integrated estimation and control of soft robots. A significant challenge in deploying closed loop controllers is reliable proprioception via integrated sensing in soft robots. Despite the considerable advances accomplished in fabrication, modelling, and model-based control of soft robots, integrated sensing and estimation is still in its infancy. To that end, this paper introduces a new method of estimating the degree of curvature of a soft robot using a stretchable sensing skin. The skin is a spray-coated piezoresistive sensing layer on a latex membrane. The mapping from the strain signal to the degree of curvature is estimated by using a recurrent neural network. We investigate uni-directional bending as well as bi-directional bending of a single-segment soft robot. Moreover, an adaptive controller is developed to track the degree of curvature of the soft robot in the presence of dynamic uncertainties. Subsequently, using the integrated soft sensing skin, we experimentally demonstrate successful curvature tracking control of the soft robot."}
{"original_text": "We propose a two-layer cache mechanism to speed up dynamic WFST decoding with personalized language models. The first layer is a public cache that stores most of the static part of the graph. This is shared globally among all users. A second layer is a private cache that caches the graph that represents the personalized language model, which is only shared by the utterances from a particular user. We also propose two simple yet effective pre-initialization methods, one based on breadth-first search, and another based on a data-driven exploration of decoder states using previous utterances. Experiments with a calling speech recognition task using a personalized contact list demonstrate that the proposed public cache reduces decoding time by factor of three compared to decoding without pre-initialization. Using the private cache provides additional efficiency gains, reducing the decoding time by a factor of five.", "label": "1", "is_selected": "0", "text": "We propose a two-layer cache mechanism to speed up dynamic WFST decoding with personalized language models. The first layer is a public cache that stores most of the static part of the graph. This is shared globally among all users. A second layer is a private cache that caches the graph that represents the personalized language model, which is only shared by the utterances from a particular user. We also propose two simple yet effective pre-initialization methods, one based on breadth-first search, and another based on a data-driven exploration of decoder states using previous utterances. Experiments with a calling speech recognition task using a personalized contact list demonstrate that the proposed public cache reduces decoding time by factor of three compared to decoding without pre-initialization. Using the private cache provides additional efficiency gains, reducing the decoding time by a factor of five."}
{"original_text": "We assume that recommender systems are more successful, when they are based on a thorough understanding of how people process information. In the current paper we test this assumption in the context of social tagging systems. Cognitive research on how people assign tags has shown that they draw on two interconnected levels of knowledge in their memory: on a conceptual level of semantic fields or topics, and on a lexical level that turns patterns on the semantic level into words. Another strand of tagging research reveals a strong impact of time dependent forgetting on users' tag choices, such that recently used tags have a higher probability being reused than \"older\" tags. In this paper, we align both strands by implementing a computational theory of human memory that integrates the two-level conception and the process of forgetting in form of a tag recommender and test it in three large-scale social tagging datasets (drawn from BibSonomy, CiteULike and Flickr). As expected, our results reveal a selective effect of time: forgetting is much more pronounced on the lexical level of tags. Second, an extensive evaluation based on this observation shows that a tag recommender interconnecting both levels and integrating time dependent forgetting on the lexical level results in high accuracy predictions and outperforms other well-established algorithms, such as Collaborative Filtering, Pairwise Interaction Tensor Factorization, FolkRank and two alternative time dependent approaches. We conclude that tag recommenders can benefit from going beyond the manifest level of word co-occurrences, and from including forgetting processes on the lexical level.", "label": "1", "is_selected": "0", "text": "We assume that recommender systems are more successful, when they are based on a thorough understanding of how people process information. In the current paper we test this assumption in the context of social tagging systems. Cognitive research on how people assign tags has shown that they draw on two interconnected levels of knowledge in their memory: on a conceptual level of semantic fields or topics, and on a lexical level that turns patterns on the semantic level into words. Another strand of tagging research reveals a strong impact of time dependent forgetting on users' tag choices, such that recently used tags have a higher probability being reused than \"older\" tags. In this paper, we align both strands by implementing a computational theory of human memory that integrates the two-level conception and the process of forgetting in form of a tag recommender and test it in three large-scale social tagging datasets (drawn from BibSonomy, CiteULike and Flickr). As expected, our results reveal a selective effect of time: forgetting is much more pronounced on the lexical level of tags. Second, an extensive evaluation based on this observation shows that a tag recommender interconnecting both levels and integrating time dependent forgetting on the lexical level results in high accuracy predictions and outperforms other well-established algorithms, such as Collaborative Filtering, Pairwise Interaction Tensor Factorization, FolkRank and two alternative time dependent approaches. We conclude that tag recommenders can benefit from going beyond the manifest level of word co-occurrences, and from including forgetting processes on the lexical level."}
{"original_text": "In this work, we demonstrate that receptive fields in 3D pose estimation can be effectively specified using optical flow. We introduce adaptive receptive fields, a simple and effective method to aid receptive field selection in pose estimation models based on optical flow inference. We contrast the performance of a benchmark state-of-the-art model running on fixed receptive fields with their adaptive field counterparts. By using a reduced receptive field, our model can process slow-motion sequences (10x longer) 23 faster than the benchmark model running at regular speed. The reduction in computational cost is achieved while producing a pose prediction accuracy to within 0.36 of the benchmark model.", "label": "1", "is_selected": "0", "text": "In this work, we demonstrate that receptive fields in 3D pose estimation can be effectively specified using optical flow. We introduce adaptive receptive fields, a simple and effective method to aid receptive field selection in pose estimation models based on optical flow inference. We contrast the performance of a benchmark state-of-the-art model running on fixed receptive fields with their adaptive field counterparts. By using a reduced receptive field, our model can process slow-motion sequences (10x longer) 23 faster than the benchmark model running at regular speed. The reduction in computational cost is achieved while producing a pose prediction accuracy to within 0.36 of the benchmark model."}
{"original_text": "How can we compute the pseudoinverse of a sparse feature matrix efficiently and accurately for solving optimization problems? A pseudoinverse is a generalization of a matrix inverse, which has been used in a variety of applications, such as imputing missing data and solving least-squares problems. Fourth-year Ph.D. student Yang Du, Assistant Professor of Statistics Beidi Chen, and their collaborator Yangbo Chen at the University of Virginia have introduced the concept of the sparse pseudoinverse matrix (SPIN), which represents the pseudoinverse of a sparse matrix in the form of a sparse matrix. The main objective of their research is to explore the properties of SPIN and devise algorithms for computing it efficiently. The SPIN matrix is a generalization of the pseudoinverse of a sparse matrix. SPIN is a dense matrix that shares the same nonzero structure as the original feature matrix. \"What's important about SPIN is that it provides a way to compute the pseudoinverse of a sparse matrix without taking the inverse of the matrix,\" Chen says. The paper, titled \"Sparse Pseudoinverse Matrix (SPIN): a Generalization of Sparse Matrix Inverse,\" was recently published in the journal Annals of Statistics. It proposes a variety of algorithms for computing SPIN and investigates the theoretical properties of SPIN. The generalization is critical in solving least-squares problems. Most optimization problems can be reduced to a least-squares problem. However, there are often constraints on the solution, such as sparse constraints, which cannot be modeled as linear constraints. The researchers' proposed algorithms can efficiently solve these nonlinear optimization problems. The researchers have applied the SPIN matrix to solve machine learning problems and to fit Gaussian process models. They also envision that it can be applied to a variety of other disciplines in science and engineering. \"This is one of the first papers to introduce and study the concept of SPIN matrix,\" Chen says. The research was supported by the National Science Foundation, the Air Force Office of Scientific Research, and the Office of Naval Research. Paper: \"Sparse Pseudoinverse Matrix (SPIN): A Generalization of Sparse Matrix Inverse\" Authors: Yang Du, Beidi Chen, and Yangbo Chen Published: Annals of Statistics Penn State's College of Information Sciences and Technology (IST) is a vibrant and growing college that addresses the challenges of the information age by integrating research and education in the social and technological dimensions of information. The College is committed to its foundational vision: producing excellent IST graduates, advancing the information sciences, and serving the public good. The College's land-grant mission includes the development of new knowledge and its dissemination to the citizens of the Commonwealth, the nation, and the world.", "label": "0", "is_selected": "1", "text": "Researchers at the University of Wisconsin-Madison have come up with a new way to solve some of the most common problems in statistics, such as finding the most efficient solution to a problem."}
{"original_text": "This work is concerned with the proof of a posteriori error estimates for fully-discrete Galerkin approximations of the Allen-Cahn equation in two and three spatial dimensions. The numerical method comprises a second-order implicit time discretization in combination with the finite element discretization in space. The analysis yields error estimates for the total error in terms of the discrete H1 and L2 norms of the finite element solution and the jump of the discrete solution across element interfaces. The author gratefully acknowledges support by the DFG (German Research Foundation) through the project JA 12171-1 and through the graduate school \" Analysis, Modeling, and Computing in Science and Engineering \" .", "label": "0", "is_selected": "0", "text": "This work is concerned with the proof of a posteriori error estimates for fully-discrete Galerkin approximations of the Allen-Cahn equation in two and three spatial dimensions. The numerical method comprises a second-order implicit time discretization in combination with the finite element discretization in space. The analysis yields error estimates for the total error in terms of the discrete H1 and L2 norms of the finite element solution and the jump of the discrete solution across element interfaces. The author gratefully acknowledges support by the DFG (German Research Foundation) through the project JA 12171-1 and through the graduate school \" Analysis, Modeling, and Computing in Science and Engineering \" ."}
{"original_text": "The (classical) problem of characterizing and enumerating permutations that can be sorted using two stacks connected in series is still largely open. In the present paper we address a related problem, in which we impose restrictions both on the procedure and on the stacks. More precisely, we consider a greedy algorithm where we perform the rightmost legal operation (here \"rightmost\" refers to the usual representation of stack sorting problems). Moreover, the first stack is required to be s -avoiding, for some permutation s, meaning that, at each step, the elements maintained in the stack avoid the pattern s when read from top to bottom. Since the set of permutations which can be sorted by such a device (which we call s -machine) is not always a class, it would be interesting to understand when it happens. We will prove that the set of s -machines whose associated sortable permutations are not a class is counted by Catalan numbers. Moreover, we will analyze two specific s -machines in full details (namely when s 321 and s 123), providing for each of them a complete characterization and enumeration of sortable permutations.", "label": "1", "is_selected": "0", "text": "The (classical) problem of characterizing and enumerating permutations that can be sorted using two stacks connected in series is still largely open. In the present paper we address a related problem, in which we impose restrictions both on the procedure and on the stacks. More precisely, we consider a greedy algorithm where we perform the rightmost legal operation (here \"rightmost\" refers to the usual representation of stack sorting problems). Moreover, the first stack is required to be s -avoiding, for some permutation s, meaning that, at each step, the elements maintained in the stack avoid the pattern s when read from top to bottom. Since the set of permutations which can be sorted by such a device (which we call s -machine) is not always a class, it would be interesting to understand when it happens. We will prove that the set of s -machines whose associated sortable permutations are not a class is counted by Catalan numbers. Moreover, we will analyze two specific s -machines in full details (namely when s 321 and s 123), providing for each of them a complete characterization and enumeration of sortable permutations."}
{"original_text": "The Hotelling game consists of n servers each choosing a point on the line segment, so as to maximize the amount of clients it attracts. Clients are uniformly distributed along the line, and each client buys from the closest server. In this paper, we study a fault-prone version of the Hotelling game, where the line fails at multiple random locations. Each failure disconnects the line, blocking the passage of clients. We show that the game admits a Nash equilibrium if and only if the rate of faults exceeds a certain threshold, and calculate that threshold approximately. Moreover, when a Nash equilibrium exists we show it is unique and construct it explicitly. Hence, somewhat surprisingly, the potential occurrence of failures has a stabilizing effect on the game (provided there are enough of them). Additionally, we study the social cost of the game (measured in terms of the total transportation cost of the clients), which also seems to benefit in a certain sense from the potential presence of failures.", "label": "1", "is_selected": "0", "text": "The Hotelling game consists of n servers each choosing a point on the line segment, so as to maximize the amount of clients it attracts. Clients are uniformly distributed along the line, and each client buys from the closest server. In this paper, we study a fault-prone version of the Hotelling game, where the line fails at multiple random locations. Each failure disconnects the line, blocking the passage of clients. We show that the game admits a Nash equilibrium if and only if the rate of faults exceeds a certain threshold, and calculate that threshold approximately. Moreover, when a Nash equilibrium exists we show it is unique and construct it explicitly. Hence, somewhat surprisingly, the potential occurrence of failures has a stabilizing effect on the game (provided there are enough of them). Additionally, we study the social cost of the game (measured in terms of the total transportation cost of the clients), which also seems to benefit in a certain sense from the potential presence of failures."}
{"original_text": "We consider a point-to-point communication scenario where the receiver intends to maintain a specific linear function of a message vector over a finite field. When the value of the message is known at the transmitter, we consider the problem of finding the smallest number of channel uses necessary and sufficient to achieve the prescribed linear transformation. Our main result is a necessary and sufficient condition on the finiteness of the number of channel uses necessary and sufficient to achieve the prescribed linear transformation in terms of the rank of a matrix. We also show that the finite number of channel uses necessary and sufficient to achieve the prescribed linear transformation can be achieved by a linear encoder with minimum power. J. D. C. Cocke, \"A machine-independent set of optimizing compilers,\" Proc. ACM SIGPLAN Symp. on Compiler Construction, vol. 9, no. 3, pp. 141-156, 1974. D. A. Cox, J. L. Little, and D. O'Shea, Ideals, Varieties and Algorithms, Springer-Verlag, 3rd edition, 1998. A. M. Garg, T. Schulte-Pelkum, and M. Gastpar, \"Compress and forward for distributed source coding with correlated sources,\" IEEE Trans. Commun., vol. 59, no. 8, pp. 2104-2118, Aug. 2011. M. R. Gharan, \"On the uplink-downlink duality of linear network coding,\" IEEE Trans. Inf. Theory, vol. 58, no. 5, pp. 3039-3052, May 2012. P. Z. G. Gopalakrishnan and M. Gastpar, \"Compress-and-forward for distributed source coding with correlated sources,\" IEEE Trans. Commun., vol. 58, no. 9, pp. 2556-2566, Sep. 2010. G. A. Grimaldi, \"Compressed source-channel coding with rateless codes,\" IEEE Trans. Inf. Theory, vol. 57, no. 2, pp. 779-794, Feb. 2011. A. Hassani, M. C. Falconer, and J. Cheng, \"A study of rateless codes for communication over wireless networks,\" IEEE Trans. Wireless Commun., vol. 10, no. 9, pp. 3074-3083, Sep. 2011. F. K. Hwang, J. B. Huber, and A. G. Fraser, \"A class of rateless erasure codes,\" IEEE Trans. Commun., vol. 57, no. 10, pp. 2906-2915, Oct. 2009. G. Kramer, \"Optimal rateless codes for unbounded delays,\" IEEE Trans. Commun., vol. 59, no. 4, pp. 844-854, Apr. 2011. G. Kramer and H. Kellerer, \"On the maximum throughput of rateless codes with bounded delay,\" IEEE Trans. Inf. Theory, vol. 58, no. 2, pp. 873-883, Feb. 2012. N. Levenshtein, \"Binary codes with minimal error distances,\" IEEE Trans. Inf. Theory, vol. 2, no. 3, pp. 87-93, Sep. 1956. P. Li, S. Y. C. Sheng, and L. J. Cimini Jr., \"Rateless codes for wireless transmission over multiple channels,\" IEEE Trans. Wireless Commun., vol. 9, no. 9, pp. 2864-2874, Sep. 2010. J. Liu, X. Ma, X. Ding, and Y. Xue, \"A novel rateless codec for distributed source coding systems,\" IEEE Commun. Lett., vol. 14, no. 12, pp. , Dec. 2010. J. Liu, X. Ma, X. Ding, and Y. Xue, \"A rateless code", "label": "0", "is_selected": "1", "text": "The problem of finding the smallest number of channel uses necessary and sufficient to achieve the prescribed linear transformation in terms of the rank of a matrix is addressed in this paper."}
{"original_text": "In this work we focus on the problem of colorization for image compression. Since color information occupies a large proportion of the total storage size of an image, a method that can predict accurate color from its grayscale version can produce a dramatic reduction in image file size. But colorization for compression poses several challenges. First, while colorization for artistic purposes simply involves predicting plausible chroma, colorization for compression requires generating output colors that are as close as possible to the ground truth. Second, many objects in the real world exhibit multiple possible colors. Thus, in order to disambiguate the colorization problem some additional information must be stored to reproduce the true colors with good accuracy. To account for the multimodal color distribution of objects we propose a deep tree-structured network that generates for every pixel multiple color hypotheses, as opposed to a single color produced by most prior colorization approaches. We show how to leverage the multimodal output of our model to reproduce with high fidelity the true colors of an image by storing very little additional information. In the experiments we show that our proposed method outperforms traditional JPEG color coding by a large margin, producing colors that are nearly indistinguishable from the ground truth at the storage cost of just a few hundred bytes for high-resolution pictures!", "label": "1", "is_selected": "0", "text": "In this work we focus on the problem of colorization for image compression. Since color information occupies a large proportion of the total storage size of an image, a method that can predict accurate color from its grayscale version can produce a dramatic reduction in image file size. But colorization for compression poses several challenges. First, while colorization for artistic purposes simply involves predicting plausible chroma, colorization for compression requires generating output colors that are as close as possible to the ground truth. Second, many objects in the real world exhibit multiple possible colors. Thus, in order to disambiguate the colorization problem some additional information must be stored to reproduce the true colors with good accuracy. To account for the multimodal color distribution of objects we propose a deep tree-structured network that generates for every pixel multiple color hypotheses, as opposed to a single color produced by most prior colorization approaches. We show how to leverage the multimodal output of our model to reproduce with high fidelity the true colors of an image by storing very little additional information. In the experiments we show that our proposed method outperforms traditional JPEG color coding by a large margin, producing colors that are nearly indistinguishable from the ground truth at the storage cost of just a few hundred bytes for high-resolution pictures!"}
{"original_text": "We present an evaluation of several representative sampling-based and optimization-based motion planners, and then introduce an integrated motion planning system which incorporates recent advances in trajectory optimization into a sparse sampling-based search algorithm. We present a systematic evaluation of two important classes of motion planning algorithms: sampling-based planners and optimization-based planners. Our experiments compare the performance of a number of state-of-the-art sampling-based planners and optimization-based planners. We also present a novel hybrid motion planning system that uses sampling-based search in large parts of the state space where the dynamics are approximately known, but uses local trajectory optimization to more accurately model the vehicle dynamics in regions where the approximate model may be inaccurate. To our knowledge this represents the first work that combines sampling-based planning and local trajectory optimization into a seamless planning and control framework. Our results indicate that the integrated planning and control approach is significantly better than either component alone. J. M. Hollerbach, G. Pappas, G. S. Sukhatme and A. K. Louis, \"Integrated Planning and Control for Dynamic Systems,\" Proceedings of the 4th International Conference on Robotics and Automation (ICRA 00), San Francisco, CA, April 2000, pp. 250-257.", "label": "0", "is_selected": "1", "text": "The planning and control of dynamic systems is an important area of research in the fields of robotics and computer-aided engineering..Continued on this page."}
{"original_text": "This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of Residual Networks on the challenging ImageNet classification and COCO object detection datasets. Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions.", "label": "1", "is_selected": "0", "text": "This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of Residual Networks on the challenging ImageNet classification and COCO object detection datasets. Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions."}
{"original_text": "Wireless Sensor Networks (WSNs) with their dynamic applications gained a tremendous attention of researchers. Constant monitoring of critical situations attracted researchers to utilize WSNs at vast platforms. The main focus of this paper is to develop an algorithm which is feasible to reduce energy consumption, delay and avoid dead nodes. The algorithm is based on Location Aware Clustering (LAC) and Energy Efficient Clustering (EEC). A prototype model is proposed that shows the effectiveness of the approach. The algorithm has been implemented and the results are compared with existing techniques. The performance evaluation of the algorithm demonstrates that the proposed algorithm reduces energy consumption and delay. Due to the rapid advancement in the field of Wireless Sensor Networks (WSNs), it became a platform for researchers and scientists to work on different applications of WSNs. The WSNs are typically composed of a large number of sensing devices with constrained energy supply . The conventional wireless sensor network (WSN) systems are based on the clustering model, which is more efficient to achieve energy conservation and performance. The cluster-based sensor network (CBSN) has been widely used in many applications such as surveillance, healthcare and environmental monitoring due to their energy efficiency, scalability and less complexity . As it is clear from the figure 1, a WSN consists of a large number of nodes in an unstructured manner. Due to the large number of nodes and the fact that it has an unstructured mode of communication, the energy of the nodes is consumed at a very faster rate and this leads to the death of many nodes. The death of nodes leads to an unbalanced network that leads to degraded quality of services. This calls for an effective algorithm that would minimize the energy consumption, improve the lifetime of the nodes and increase the quality of services. The basic cluster formation algorithm forms clusters based on the minimum-energy nodes or the farthest nodes from the base station. The formation of clusters in this manner leads to unbalanced clusters. As shown in figure 2, nodes A and B are the farthest nodes from the base station but the node B has a larger energy capacity than that of the node A. So when the cluster is formed, node A is chosen instead of B due to the node B being the farther one. This leads to an unbalanced cluster. To avoid this unbalanced clustering approach, the present algorithm is based on the concept of Energy Efficient Clustering (EEC) in combination with Location Aware Clustering (LAC). The EEC uses the concept of residual energy to avoid the formation of unbalanced clusters. The LAC uses location information of the nodes to form clusters with the nodes with large residual energy. As a result, the energy consumption is minimized. Figure 2: Conventional Cluster Formation Algorithm. The remaining of the paper is organized as follows: In section II, the related work is presented. In section III, the proposed system architecture is discussed. Section IV describes the proposed approach. In section V, the results and discussions are presented. Finally, the conclusion is given in section VI. A large number of work has been done in the field of Wireless Sensor Networks. The researchers have worked on several applications of WSNs. These applications include health monitoring, vehicular traffic monitoring, environment monitoring and military applications. In this section, the focus is on the algorithms that are designed to reduce energy consumption and enhance the performance of the WSNs. The energy consumption is directly related to the number of transmissions and the distance between the source and destination nodes. To minimize the energy consumption, the clustering technique is used. The clustering approach is used to minimize energy consumption and increase the performance of the network. The WSNs are formed by placing the sensors on different locations and their energy consumption is minimized by forming clusters. The clusters are formed by placing the nodes with similar residual energy and the residual energy is decided based on the energy of the node and the distance of the node from the base station. The base station coordinates the activities of the network. The cluster heads (CHs) collect data from the nodes and then transmit it to the base station. The cluster heads use less energy when compared to that of the nodes. The cluster heads need to be replaced to avoid the death of the cluster heads. There are several algorithms that have been proposed by the researchers to minimize the energy consumption of the network. The most popular of these algorithms are LEACH , HEED , TEEN , PEGASIS , DBCS , ICPA , DBS . In , the authors have proposed the LEACH algorithm, which is a clustering algorithm where the sensors are randomly distributed in the network area. The", "label": "0", "is_selected": "1", "text": "This paper proposes an algorithm to reduce energy consumption and delay of wireless sensor network systems by clustering the sensing devices according to their location Aware (LA) and Energy Efficient Clustering (EEC)."}
{"original_text": "While significant improvements have been made in recent years in terms of end-to-end automatic speech recognition (ASR) performance, such improvements were obtained through the use of very large neural networks, most of which were trained on very large amounts of data. A similar trend is seen with other tasks in the field of artificial intelligence, including machine translation and image classification. All of these tasks can be performed with neural networks, which are a class of machine learning models inspired by the human brain. Due to the large number of parameters required to train these models, a large amount of data is needed to fit them and make them perform well. Although many neural networks are trained with millions of examples, humans are able to learn from much fewer. One way of investigating this is by performing psycholinguistic experiments to understand how humans learn new languages or tasks, and then use this understanding to design models that perform similarly to humans. In this talk, I will discuss how such psycholinguistic experiments are performed and how they can be used to develop models of language acquisition. I will also discuss how these models can be used to train neural networks that perform like humans, and how such models can be used to improve the training process for neural networks by effectively using smaller amounts of data. Hamza Idrissi is a senior software engineer at Bloomberg, where he develops software for machine learning. Hamza received his B.S. in Computer Science from the National Institute of Applied Sciences and Technology in Tangier, Morocco, in 2014. He then went on to earn his M.S. in Computer Science from New York University in 2016. During his time at NYU, he was a research assistant at the Courant Institute of Mathematical Sciences, working on the design and implementation of machine learning models of language learning and development. He also worked on the design and implementation of algorithms for the modeling of the dynamics of the retina, and how the retina responds to different kinds of stimuli. Hamza's main research interests are in the area of computational linguistics, machine learning, and natural language processing.", "label": "0", "is_selected": "0", "text": "While significant improvements have been made in recent years in terms of end-to-end automatic speech recognition (ASR) performance, such improvements were obtained through the use of very large neural networks, most of which were trained on very large amounts of data. A similar trend is seen with other tasks in the field of artificial intelligence, including machine translation and image classification. All of these tasks can be performed with neural networks, which are a class of machine learning models inspired by the human brain. Due to the large number of parameters required to train these models, a large amount of data is needed to fit them and make them perform well. Although many neural networks are trained with millions of examples, humans are able to learn from much fewer. One way of investigating this is by performing psycholinguistic experiments to understand how humans learn new languages or tasks, and then use this understanding to design models that perform similarly to humans. In this talk, I will discuss how such psycholinguistic experiments are performed and how they can be used to develop models of language acquisition. I will also discuss how these models can be used to train neural networks that perform like humans, and how such models can be used to improve the training process for neural networks by effectively using smaller amounts of data. Hamza Idrissi is a senior software engineer at Bloomberg, where he develops software for machine learning. Hamza received his B.S. in Computer Science from the National Institute of Applied Sciences and Technology in Tangier, Morocco, in 2014. He then went on to earn his M.S. in Computer Science from New York University in 2016. During his time at NYU, he was a research assistant at the Courant Institute of Mathematical Sciences, working on the design and implementation of machine learning models of language learning and development. He also worked on the design and implementation of algorithms for the modeling of the dynamics of the retina, and how the retina responds to different kinds of stimuli. Hamza's main research interests are in the area of computational linguistics, machine learning, and natural language processing."}
{"original_text": "GANs can generate photo-realistic images from the domain of their training data. However, those wanting to use them for creative purposes often want to generate imagery from a truly novel domain, a task which GANs are inherently unable to do. It is also desirable to have a level of control so that there is a degree of artistic direction rather than purely curation of random results. Here we present a method for interpolating between generative models of the StyleGAN architecture in a resolution dependent manner. This allows us to generate images from an entirely novel domain and do this with a degree of control over the nature of the output.", "label": "1", "is_selected": "0", "text": "GANs can generate photo-realistic images from the domain of their training data. However, those wanting to use them for creative purposes often want to generate imagery from a truly novel domain, a task which GANs are inherently unable to do. It is also desirable to have a level of control so that there is a degree of artistic direction rather than purely curation of random results. Here we present a method for interpolating between generative models of the StyleGAN architecture in a resolution dependent manner. This allows us to generate images from an entirely novel domain and do this with a degree of control over the nature of the output."}
{"original_text": "Internet of Things (IoT) devices have become ubiquitous and are spread across many application domains including the industry, transportation, healthcare, and households. However, the proliferation of the IoT devices has raised the concerns about their security, especially when observing that many manufacturers focus only on the core functionality of their products due to short time to market and low cost pressures, while neglecting security aspects. Moreover, it does not exist any established or standardized method for measuring and ensuring the security of IoT devices. Consequently, vulnerabilities are left untreated, allowing attackers to exploit IoT devices for various purposes, such as compromising privacy, recruiting devices into a botnet, or misusing devices to perform cryptocurrency mining. In this paper, we present a practical Host-based Anomaly DEtection System for IoT (HADES-IoT) that represents the last line of defense. HADES-IoT has proactive detection capabilities, provides tamper-proof resistance, and it can be deployed on a wide range of Linux-based IoT devices. The main advantage of HADES-IoT is its low performance overhead, which makes it suitable for the IoT domain, where state-of-the-art approaches cannot be applied due to their high-performance demands. We deployed HADES-IoT on seven IoT devices to evaluate its effectiveness and performance overhead. Our experiments show that HADES-IoT achieved 100 effectiveness in the detection of current IoT malware such as VPNFilter and IoTReaper; while on average, requiring only 5.5 of available memory and causing only a low CPU load.", "label": "1", "is_selected": "0", "text": "Internet of Things (IoT) devices have become ubiquitous and are spread across many application domains including the industry, transportation, healthcare, and households. However, the proliferation of the IoT devices has raised the concerns about their security, especially when observing that many manufacturers focus only on the core functionality of their products due to short time to market and low cost pressures, while neglecting security aspects. Moreover, it does not exist any established or standardized method for measuring and ensuring the security of IoT devices. Consequently, vulnerabilities are left untreated, allowing attackers to exploit IoT devices for various purposes, such as compromising privacy, recruiting devices into a botnet, or misusing devices to perform cryptocurrency mining. In this paper, we present a practical Host-based Anomaly DEtection System for IoT (HADES-IoT) that represents the last line of defense. HADES-IoT has proactive detection capabilities, provides tamper-proof resistance, and it can be deployed on a wide range of Linux-based IoT devices. The main advantage of HADES-IoT is its low performance overhead, which makes it suitable for the IoT domain, where state-of-the-art approaches cannot be applied due to their high-performance demands. We deployed HADES-IoT on seven IoT devices to evaluate its effectiveness and performance overhead. Our experiments show that HADES-IoT achieved 100 effectiveness in the detection of current IoT malware such as VPNFilter and IoTReaper; while on average, requiring only 5.5 of available memory and causing only a low CPU load."}
{"original_text": "Recent years have seen big advances in the field of sentence-level quality estimation (QE), largely as a result of using neural-based architectures. However, the majority of these methods work only on the language pair they are trained on and need retraining for new language pairs. This process can prove difficult from a technical point of view and is usually computationally expensive. In this paper we propose a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. Our evaluation shows that the proposed methods achieve state-of-the-art results outperforming current open-source quality estimation frameworks when trained on datasets from WMT. In addition, the framework proves very useful in transfer learning settings, especially when dealing with low-resourced languages, allowing us to obtain very competitive results.", "label": "1", "is_selected": "0", "text": "Recent years have seen big advances in the field of sentence-level quality estimation (QE), largely as a result of using neural-based architectures. However, the majority of these methods work only on the language pair they are trained on and need retraining for new language pairs. This process can prove difficult from a technical point of view and is usually computationally expensive. In this paper we propose a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. Our evaluation shows that the proposed methods achieve state-of-the-art results outperforming current open-source quality estimation frameworks when trained on datasets from WMT. In addition, the framework proves very useful in transfer learning settings, especially when dealing with low-resourced languages, allowing us to obtain very competitive results."}
{"original_text": "We consider a wireless distributed computing system, in which multiple mobile users, connected wirelessly through an access point, collaborate to perform a computation task. In particular, users communicate with each other via the access point through a broadcast channel. We first analyze the maximum transmission capacity that can be achieved by a single user in a single communication round (slot). We characterize the throughput of the transmission between a single user and the access point as a function of the SNR and the fading distribution of the wireless channel. We then present an algorithm for optimally scheduling the transmissions of the users, so as to maximize the total achievable throughput in the system. We show that this algorithm converges to the optimal schedule. Finally, we evaluate the performance of the system by means of extensive simulations.", "label": "0", "is_selected": "0", "text": "We consider a wireless distributed computing system, in which multiple mobile users, connected wirelessly through an access point, collaborate to perform a computation task. In particular, users communicate with each other via the access point through a broadcast channel. We first analyze the maximum transmission capacity that can be achieved by a single user in a single communication round (slot). We characterize the throughput of the transmission between a single user and the access point as a function of the SNR and the fading distribution of the wireless channel. We then present an algorithm for optimally scheduling the transmissions of the users, so as to maximize the total achievable throughput in the system. We show that this algorithm converges to the optimal schedule. Finally, we evaluate the performance of the system by means of extensive simulations."}
{"original_text": "We propose a new global entity disambiguation (ED) model based on contextualized embeddings of words and entities. Our model is based on a bidirectional transformer encoder (i.e., BERT) and produces entity embeddings that are contextualized by the whole document. It contains a joint entity and document representation learning, which is trained with ranking loss and evaluated in a supervised manner. Our approach outperforms state-of-the-art ED methods based on contextualized embeddings on the AIDA-CoNLL dataset, even if they use ensemble approaches, and slightly outperforms the supervised ED method that achieves the best results in a weakly-supervised setting. We also propose a new publicly available dataset that contains over 17 million entities and is obtained from Wikipedia. Our model is also evaluated on the DBpedia Spotlight dataset and achieves better results than state-of-the-art ED methods.", "label": "0", "is_selected": "0", "text": "We propose a new global entity disambiguation (ED) model based on contextualized embeddings of words and entities. Our model is based on a bidirectional transformer encoder (i.e., BERT) and produces entity embeddings that are contextualized by the whole document. It contains a joint entity and document representation learning, which is trained with ranking loss and evaluated in a supervised manner. Our approach outperforms state-of-the-art ED methods based on contextualized embeddings on the AIDA-CoNLL dataset, even if they use ensemble approaches, and slightly outperforms the supervised ED method that achieves the best results in a weakly-supervised setting. We also propose a new publicly available dataset that contains over 17 million entities and is obtained from Wikipedia. Our model is also evaluated on the DBpedia Spotlight dataset and achieves better results than state-of-the-art ED methods."}
{"original_text": "With the shortage of physicians and surgeons and increase in demand worldwide due to situations such as the COVID-19 pandemic, there is a growing interest in finding solutions to help address the problem. A solution to this problem would be to use neurotechnology to provide them augmented cognition, senses and action for optimal diagnosis and treatment. Consequently, doing so can negatively impact them and others. We argue that applying neurotechnology for human enhancement in physicians and surgeons can cause injustices, and harm to them and patients. In this paper, we will first describe the augmentations and neurotechnologies that can be used to achieve the relevant augmentations for physicians and surgeons. We will then review selected ethical concerns discussed within literature, discuss the neuroengineering behind using neurotechnology for augmentation purposes, then conclude with an analysis on outcomes and ethical issues of implementing human augmentation via neurotechnology in medical and surgical practice.", "label": "1", "is_selected": "0", "text": "With the shortage of physicians and surgeons and increase in demand worldwide due to situations such as the COVID-19 pandemic, there is a growing interest in finding solutions to help address the problem. A solution to this problem would be to use neurotechnology to provide them augmented cognition, senses and action for optimal diagnosis and treatment. Consequently, doing so can negatively impact them and others. We argue that applying neurotechnology for human enhancement in physicians and surgeons can cause injustices, and harm to them and patients. In this paper, we will first describe the augmentations and neurotechnologies that can be used to achieve the relevant augmentations for physicians and surgeons. We will then review selected ethical concerns discussed within literature, discuss the neuroengineering behind using neurotechnology for augmentation purposes, then conclude with an analysis on outcomes and ethical issues of implementing human augmentation via neurotechnology in medical and surgical practice."}
{"original_text": "Many computer vision and medical imaging problems are faced with learning from large-scale datasets, with millions of observations and features. In this paper we propose a novel efficient learning scheme that utilizes an approximate representation of the full dataset, represented as a set of representative points (i.e., k-center points). The k-center points are learned using a new density function that encourages not only the typical low-density regions to be selected, but also large local density regions. The resulting k-center points are used to construct a probability distribution that approximates the original dataset. The approximation is then used to learn a classifier that maximizes the class-conditional probabilities of the k-center points, rather than the full dataset. Our method is fully general and can be used in any supervised or semi-supervised learning setting. We apply the proposed method to the problems of medical image segmentation and object recognition, and demonstrate the advantages of our proposed method over several baseline methods.", "label": "0", "is_selected": "0", "text": "Many computer vision and medical imaging problems are faced with learning from large-scale datasets, with millions of observations and features. In this paper we propose a novel efficient learning scheme that utilizes an approximate representation of the full dataset, represented as a set of representative points (i.e., k-center points). The k-center points are learned using a new density function that encourages not only the typical low-density regions to be selected, but also large local density regions. The resulting k-center points are used to construct a probability distribution that approximates the original dataset. The approximation is then used to learn a classifier that maximizes the class-conditional probabilities of the k-center points, rather than the full dataset. Our method is fully general and can be used in any supervised or semi-supervised learning setting. We apply the proposed method to the problems of medical image segmentation and object recognition, and demonstrate the advantages of our proposed method over several baseline methods."}
{"original_text": "For sustainable growth and profitability, online game companies are constantly carrying out various events to attract new game users, to maximize return users, and to minimize churn users in online games. Because minimizing churn users is the most cost-effective method, many pieces of research are being conducted on ways to predict and to prevent churns in advance. However, there is still little research on the validity of event effects. In this study, we investigate whether game events influence the user churn rate and confirm the difference in how game users respond to events by character level, item purchasing frequency and game-playing time band.", "label": "1", "is_selected": "0", "text": "For sustainable growth and profitability, online game companies are constantly carrying out various events to attract new game users, to maximize return users, and to minimize churn users in online games. Because minimizing churn users is the most cost-effective method, many pieces of research are being conducted on ways to predict and to prevent churns in advance. However, there is still little research on the validity of event effects. In this study, we investigate whether game events influence the user churn rate and confirm the difference in how game users respond to events by character level, item purchasing frequency and game-playing time band."}
{"original_text": "Over the last 30 years, researchers have investigated connections between dimension for posets and planarity for graphs. Here we extend this line of research to the structural graph theory parameter interval dimension. We show that every poset has interval dimension at most 3, and that all graphs with interval dimension 3 have interval dimension 4. We also consider interval dimension for interval graphs, which are the class of graphs that correspond to posets with interval dimension at most 3. We use interval dimension to show that interval graphs have a closed interval of domination numbers. interval dimension, posets, interval graphs, domination numbers", "label": "0", "is_selected": "1", "text": "We show that every poset has interval dimension at most 3, and that all graphs with interval dimension 4 have interval dimensions 4 and 5. We show and use interval dimension to show that interval graphs have a closed interval of domination numbers."}
{"original_text": "Eigenvector continuation is a computational method that finds the extremal eigenvalues and eigenvectors of a Hamiltonian matrix with one or more control parameters. It does this by projection onto a subspace of eigenvectors corresponding to selected training values of the control parameters. The method has proven to be very efficient and accurate for interpolating and extrapolating eigenvectors. However, almost nothing is known about how the method converges, and its rapid convergence properties have remained mysterious. In this letter we present the first study of the convergence of eigenvector continuation. In order to perform the mathematical analysis, we introduce a new variant of eigenvector continuation that we call vector continuation. We first prove that eigenvector continuation and vector continuation have identical convergence properties and then analyze the convergence of vector continuation. Our analysis shows that, in general, eigenvector continuation converges more rapidly than perturbation theory. The faster convergence is achieved by eliminating a phenomenon that we call differential folding, the interference between non-orthogonal vectors appearing at different orders in perturbation theory. From our analysis we can predict how eigenvector continuation converges both inside and outside the radius of convergence of perturbation theory. While eigenvector continuation is a non-perturbative method, we show that its rate of convergence can be deduced from power series expansions of the eigenvectors. Our results also yield new insights into the nature of divergences in perturbation theory.", "label": "1", "is_selected": "0", "text": "Eigenvector continuation is a computational method that finds the extremal eigenvalues and eigenvectors of a Hamiltonian matrix with one or more control parameters. It does this by projection onto a subspace of eigenvectors corresponding to selected training values of the control parameters. The method has proven to be very efficient and accurate for interpolating and extrapolating eigenvectors. However, almost nothing is known about how the method converges, and its rapid convergence properties have remained mysterious. In this letter we present the first study of the convergence of eigenvector continuation. In order to perform the mathematical analysis, we introduce a new variant of eigenvector continuation that we call vector continuation. We first prove that eigenvector continuation and vector continuation have identical convergence properties and then analyze the convergence of vector continuation. Our analysis shows that, in general, eigenvector continuation converges more rapidly than perturbation theory. The faster convergence is achieved by eliminating a phenomenon that we call differential folding, the interference between non-orthogonal vectors appearing at different orders in perturbation theory. From our analysis we can predict how eigenvector continuation converges both inside and outside the radius of convergence of perturbation theory. While eigenvector continuation is a non-perturbative method, we show that its rate of convergence can be deduced from power series expansions of the eigenvectors. Our results also yield new insights into the nature of divergences in perturbation theory."}
{"original_text": "Characterization of the relationship between a kidney tumor's appearance on cross-sectional imaging and it's treatment outcomes is a promising direction for informing treatement decisions and improving patient outcomes. Unfortunately, the accuracy and robustness of automated methods for characterizing tumor type on cross-sectional imaging remains a significant challenge. The primary focus of our research is to develop new methods for automated characterization of kidney tumors on cross-sectional imaging. In this project, we are developing new methods for characterizing renal tumor types using diffusion-weighted MRI and CT. We are developing methods to characterize the extent of renal tumor necrosis using diffusion-weighted MRI and CT. In this project, we are developing new methods for characterizing renal tumor types using single-energy and dual-energy CT. We are developing methods to characterize the extent of renal tumor necrosis using single-energy and dual-energy CT. In this project, we are developing new methods for characterizing renal tumor types using single-energy and dual-energy CT. In this project, we are developing new methods for characterizing renal tumor types using single-energy CT. In this project, we are developing new methods for characterizing renal tumor types using diffusion-weighted MRI and CT. We are developing methods to characterize the extent of renal tumor necrosis using single-energy CT. In this project, we are developing new methods for characterizing renal tumor types using single-energy CT. We are developing methods to characterize the extent of renal tumor necrosis using single-energy CT. In this project, we are developing new methods for characterizing renal tumor types using diffusion-weighted MRI and CT. We are developing methods to characterize the extent of renal tumor necrosis using diffusion-weighted MRI and CT. In this project, we are developing new methods for characterizing renal tumor types using single-energy CT. We are developing methods to characterize the extent of renal tumor necrosis using single-energy CT. In this project, we are developing new methods for characterizing renal tumor types using single-energy and dual-energy CT. In this project, we are developing new methods for characterizing renal tumor types using single-energy and dual-energy CT and diffusion-weighted MRI. In this project, we are developing new methods for characterizing renal tumor types using single-energy and dual-energy CT. We are developing methods to characterize the extent of renal tumor necrosis using single-energy and dual-energy CT. In this project, we are developing new methods for characterizing renal tumor types using single-energy CT and diffusion-weighted MRI. We are developing methods to characterize the extent of renal tumor necrosis using single-energy CT and diffusion-weighted MRI. In this project, we are developing new methods for characterizing renal tumor types using single-energy and dual-energy CT and diffusion-weighted MRI. We are developing methods to characterize the extent of renal tumor necrosis using single-energy and dual-energy CT and diffusion-weighted MRI. In this project, we are developing new methods for characterizing renal tumor types using single-energy CT and diffusion-weighted MRI. We are developing methods to characterize the extent of renal tumor necrosis using single-energy CT and diffusion-weighted MRI. In this project, we are developing new methods for characterizing renal tumor types using single-energy CT. We are developing methods to characterize the extent of renal tumor necrosis using single-energy CT. In this project, we are developing new methods for characterizing renal tumor types using single-energy and dual-energy CT and diffusion-weighted MRI and DCE-MRI. We are developing methods to characterize the extent of renal tumor necrosis using single-energy and dual-energy CT and diffusion-weighted MRI and DCE-MRI. In this project, we are developing new methods for characterizing renal tumor types using single-energy and dual-energy CT. We are developing methods to characterize the extent of renal tumor necrosis using single-energy and dual-energy CT. In this project, we are developing new methods for characterizing renal tumor types using single-energy CT. We are developing methods to characterize the extent of renal tum", "label": "0", "is_selected": "1", "text": "Cross-sectional imaging of the kidney is an important tool in the treatment of kidney cancer and has been shown to be predictive of outcome in patients with the disease."}
{"original_text": "We consider the problem of learning a non-deterministic probabilistic system consistent with a given finite set of positive and negative tree samples. Consistency is defined with respect to strong simulation of probabilistic Kripke structures. We prove that the probabilistic system cannot be inferred without prior knowledge of the class of probabilistic Kripke structures that the system can generate. We discuss different ways to obtain such prior information, and derive a method based on clustering of the tree samples. The resulting method is applied on a number of illustrative examples, including the pennies example of Shalizi.", "label": "0", "is_selected": "0", "text": "We consider the problem of learning a non-deterministic probabilistic system consistent with a given finite set of positive and negative tree samples. Consistency is defined with respect to strong simulation of probabilistic Kripke structures. We prove that the probabilistic system cannot be inferred without prior knowledge of the class of probabilistic Kripke structures that the system can generate. We discuss different ways to obtain such prior information, and derive a method based on clustering of the tree samples. The resulting method is applied on a number of illustrative examples, including the pennies example of Shalizi."}
{"original_text": "In this paper, we address an issue that the visually impaired commonly face while crossing intersections and propose a solution that takes form as a mobile application. The application utilizes a deep learning convolutional neural network model, LytNetV2, to output necessary information that the visually impaired may lack when without human companions or guide-dogs. A prototype of the application runs on iOS devices of versions 11 or above. It is designed for comprehensiveness, concision, accuracy, and computational efficiency through delivering the two most important pieces of information, pedestrian traffic light color and direction, required to cross the road in real-time. Furthermore, it is specifically aimed to support those facing financial burden as the solution takes the form of a free mobile application. Through the modification and utilization of key principles in MobileNetV3 such as depthwise seperable convolutions and squeeze-excite layers, the deep neural network model achieves a classification accuracy of 96 and average angle error of 6.15deg, while running at a frame rate of 16.34 frames per second. Additionally, the model is trained as an image classifier, allowing for a faster and more accurate model. The network is able to outperform other methods such as object detection and non-deep learning algorithms in both accuracy and thoroughness. The information is delivered through both auditory signals and vibrations, and it has been tested on seven visually impaired and has received above satisfactory responses.", "label": "1", "is_selected": "0", "text": "In this paper, we address an issue that the visually impaired commonly face while crossing intersections and propose a solution that takes form as a mobile application. The application utilizes a deep learning convolutional neural network model, LytNetV2, to output necessary information that the visually impaired may lack when without human companions or guide-dogs. A prototype of the application runs on iOS devices of versions 11 or above. It is designed for comprehensiveness, concision, accuracy, and computational efficiency through delivering the two most important pieces of information, pedestrian traffic light color and direction, required to cross the road in real-time. Furthermore, it is specifically aimed to support those facing financial burden as the solution takes the form of a free mobile application. Through the modification and utilization of key principles in MobileNetV3 such as depthwise seperable convolutions and squeeze-excite layers, the deep neural network model achieves a classification accuracy of 96 and average angle error of 6.15deg, while running at a frame rate of 16.34 frames per second. Additionally, the model is trained as an image classifier, allowing for a faster and more accurate model. The network is able to outperform other methods such as object detection and non-deep learning algorithms in both accuracy and thoroughness. The information is delivered through both auditory signals and vibrations, and it has been tested on seven visually impaired and has received above satisfactory responses."}
{"original_text": "Based on the hurricane struking Puerto Rico in 2017, we developed a transportable disaster response system \"DroneGo\" featuring a drone fleet capable of delivering medical package and videoing roads. Covering a wide range of disaster response applications from medical package delivery to videoing roads, we aimed to provide a comprehensive solution for emergency managements. Over 2 years of research and development, our team developed and produced several prototypes. We tested our system in different kinds of emergency scenarios to ensure the reliability of the system. We tested our system in different kinds of emergency scenarios to ensure the reliability of the system. The DroneGo system provides emergency managements with aerial medical package delivery and road videoing capabilities. By installing camera to a drone and providing the flight path to DroneGo's central control, we can get a live video of the affected area. DroneGo's innovative, modular payload design enables us to deliver medical packages to a certain location by just a few simple steps. We developed our own system to collect and process different data from different sensors. Different kinds of sensors were installed on the drone to monitor the status of the drone's motor, battery, payload, and so on. A drone's GPS signal is not precise enough for package delivery, thus we developed our own image-based localization system to accurately localize the drone in real time. Sometimes we need to deliver a small package to a certain location. We can simply install a camera on a drone to check the status of the road and send a video to emergency managements. When there is an emergency, we can deliver medical package to victims in a short period of time. We developed a system to guide the drone to fly autonomously to the right location. The drone can carry out different kinds of tasks by switching different kinds of payloads. Adaptive \"Road-Mapping\" Flight Path Planning. DroneGo can accurately localize the drone using only a single image from a camera installed on the drone. The system can automatically detect and avoid obstacles on the road by using a depth sensor. The DroneGo system can be easily scaled to meet different needs of emergency managements. We can install a camera on the drone to check the status of the road. The DroneGo system can send a live video of the disaster area to emergency managements. DroneGo can deliver medical package to victims in a short period of time. Different kinds of payloads can be installed on the drone to carry out different tasks. Our system can accurately localize the drone using only a single image from a camera installed on the drone. The system can automatically detect and avoid obstacles on the road by using a depth sensor and computer vision technology. The DroneGo system can be easily scaled to meet different needs of emergency managements. More drones can be added to the fleet when necessary. The DroneGo system can be remotely operated from the control center. It can be used by any qualified personnel. The DroneGo system can work well in a low-bandwidth environment by compressing the video feed from the drone. The DroneGo system can provide the GPS locations of the drone to the control center in real time. The DroneGo system can work well in a low-bandwidth environment by compressing the video feed from the drone. The compressed video feed can be transmitted by WiFi signal. The DroneGo system can provide the GPS locations of the drone to the control center in real time. The drone's location can be precisely calculated by using a lightweight computer vision algorithm. The DroneGo system is designed to be easily transported by a small vehicle. It can be quickly deployed in case of emergency. DroneGo can be transported by a small vehicle, which can be easily deployed in case of emergency. When there is an emergency, a team can be sent to the emergency site to set up the DroneGo system. The DroneGo system can be easily deployed in case of emergency. The DroneGo system can be quickly deployed in case of emergency. The DroneGo system can be deployed in case of emergency. All the components of the DroneGo system are designed to be portable. The DroneGo system can be easily deployed in case of emergency. We developed a lightweight \"Road-Mapping\" flight path planning algorithm. Our algorithm is much more accurate than traditional GPS based localization.", "label": "0", "is_selected": "1", "text": "Our team developed and produced several prototypes of a disaster response system featuring a drone fleet capable of delivering medical package and videoing roads. Covering a wide range of disaster response applications from medical package delivery to videoingroads, we aimed to provide a comprehensive solution for emergency managements."}
{"original_text": "Recent GAN-based architectures have been able to deliver impressive performance on the general task of image-to-image translation. In particular, it was shown that a wide variety of image translation operators can be learned to effectively transfer images between different domains. In this paper, we show that GAN-based image translation models are also able to learn domain-specific operations, thus enabling the application of such models to domain-specific image translation tasks. We demonstrate the generality and the high performance of such models on a variety of tasks, including face-to-face translation, age progression, and translation of weather conditions. We also show how these GAN-based models can be used to perform image translation in a self-supervised setting.", "label": "0", "is_selected": "0", "text": "Recent GAN-based architectures have been able to deliver impressive performance on the general task of image-to-image translation. In particular, it was shown that a wide variety of image translation operators can be learned to effectively transfer images between different domains. In this paper, we show that GAN-based image translation models are also able to learn domain-specific operations, thus enabling the application of such models to domain-specific image translation tasks. We demonstrate the generality and the high performance of such models on a variety of tasks, including face-to-face translation, age progression, and translation of weather conditions. We also show how these GAN-based models can be used to perform image translation in a self-supervised setting."}
{"original_text": "This chapter provides a comprehensive survey of the researches and motivations for hardware implementation of reservoir computing (RC) on neuromorphic electronic systems. Due to its computational efficiency and the fact that it has been successfully implemented on many platforms, RC is considered as one of the most promising algorithms for processing and analyzing high-dimensional, nonlinear and non-stationary data. Recent years have seen a growing interest in the hardware implementation of RC, especially on neuromorphic computing platforms. In this chapter, we provide an overview of the available electronic implementations of RC, including the field-programmable gate array, digital signal processor, application-specific integrated circuit, programmable logic device and neuromorphic system. We also review the most important RC electronic platforms from different perspectives, including the processing element, the number of processing elements, the data type, and the number of layers. Furthermore, we discuss the open research problems and challenges that should be resolved to improve the performance of hardware RC platforms in the future.", "label": "0", "is_selected": "0", "text": "This chapter provides a comprehensive survey of the researches and motivations for hardware implementation of reservoir computing (RC) on neuromorphic electronic systems. Due to its computational efficiency and the fact that it has been successfully implemented on many platforms, RC is considered as one of the most promising algorithms for processing and analyzing high-dimensional, nonlinear and non-stationary data. Recent years have seen a growing interest in the hardware implementation of RC, especially on neuromorphic computing platforms. In this chapter, we provide an overview of the available electronic implementations of RC, including the field-programmable gate array, digital signal processor, application-specific integrated circuit, programmable logic device and neuromorphic system. We also review the most important RC electronic platforms from different perspectives, including the processing element, the number of processing elements, the data type, and the number of layers. Furthermore, we discuss the open research problems and challenges that should be resolved to improve the performance of hardware RC platforms in the future."}
{"original_text": "We introduce a novel class of adjustment rules for a collection of beliefs. This is an extension of Lewis' imaging to absorb probabilistic evidence in generalized settings. Unlike standard tools for belief revision, our proposal may be used when information is inconsistent with an agent's belief base. We show that the functionals we introduce are based on the imaginary counterpart of probability kinematics for standard belief revision, and prove that, under certain conditions, all standard postulates for belief revision are satisfied.", "label": "1", "is_selected": "0", "text": "We introduce a novel class of adjustment rules for a collection of beliefs. This is an extension of Lewis' imaging to absorb probabilistic evidence in generalized settings. Unlike standard tools for belief revision, our proposal may be used when information is inconsistent with an agent's belief base. We show that the functionals we introduce are based on the imaginary counterpart of probability kinematics for standard belief revision, and prove that, under certain conditions, all standard postulates for belief revision are satisfied."}
{"original_text": "As interest in quantum computing grows, there is a pressing need for standardized API's so that algorithm designers, circuit designers, and physicists can be provided a common reference frame for the development of quantum computing software. The Universal Quantum Computer API (UQC) is a proposed standard for quantum computing API's. This talk describes the initial design of the UQC API, and how it can be used to quickly develop quantum computing applications that are portable across both quantum computing simulators and hardware architectures.", "label": "0", "is_selected": "0", "text": "As interest in quantum computing grows, there is a pressing need for standardized API's so that algorithm designers, circuit designers, and physicists can be provided a common reference frame for the development of quantum computing software. The Universal Quantum Computer API (UQC) is a proposed standard for quantum computing API's. This talk describes the initial design of the UQC API, and how it can be used to quickly develop quantum computing applications that are portable across both quantum computing simulators and hardware architectures."}
{"original_text": "Speech processing systems rely on robust feature extraction to handle phonetic and semantic variations found in natural language. While techniques exist for desensitizing features to common noise patterns produced by signal perturbation, natural human variability is a more difficult problem. A solution to this problem is to utilize a multi-class Hidden Markov Model (HMM) framework for speech recognition. We propose a general multi-class HMM structure with a broad application in speech recognition. By incorporating multi-class HMMs as part of the speech recognition framework, the model can be trained to accommodate the variability of the speech signal. This multi-class HMM framework can also be incorporated into an existing speech recognition system to improve overall performance. Features extracted from the speech signal can be transformed into a vector form for use in the HMM framework. The multi-class HMMs are trained on a corpus of speech which consists of various speaker and environmental noise characteristics. The multi-class HMMs are then used to assign a phoneme label to a feature vector extracted from a speech sample. A speech recognition system which incorporates the multi-class HMM framework is shown to outperform a baseline speech recognition system. The multi-class HMM framework allows the speech recognition system to generalize better to unseen data. This increased robustness to perturbations in the speech signal translates to better performance in real-world applications.", "label": "0", "is_selected": "0", "text": "Speech processing systems rely on robust feature extraction to handle phonetic and semantic variations found in natural language. While techniques exist for desensitizing features to common noise patterns produced by signal perturbation, natural human variability is a more difficult problem. A solution to this problem is to utilize a multi-class Hidden Markov Model (HMM) framework for speech recognition. We propose a general multi-class HMM structure with a broad application in speech recognition. By incorporating multi-class HMMs as part of the speech recognition framework, the model can be trained to accommodate the variability of the speech signal. This multi-class HMM framework can also be incorporated into an existing speech recognition system to improve overall performance. Features extracted from the speech signal can be transformed into a vector form for use in the HMM framework. The multi-class HMMs are trained on a corpus of speech which consists of various speaker and environmental noise characteristics. The multi-class HMMs are then used to assign a phoneme label to a feature vector extracted from a speech sample. A speech recognition system which incorporates the multi-class HMM framework is shown to outperform a baseline speech recognition system. The multi-class HMM framework allows the speech recognition system to generalize better to unseen data. This increased robustness to perturbations in the speech signal translates to better performance in real-world applications."}
{"original_text": "Skills like computational thinking, problem solving, handling complexity, team-work and project management are essential for future careers and needs to be taught to students at the elementary level itself. Computer programming can teach these skills in a fun way. The Robo Sapien's mission is to make every school, college and university in India a coding school. We believe that every student in India should know how to code and Robo Sapien will help them to achieve it. The Robo Sapien will teach the basics of programming to all students. We are building a team of teachers who will be able to teach programming to students of all ages. Our partners will make sure that students in all schools, colleges and universities in their region get to know programming and get opportunities to build amazing projects. The Robo Sapien is a non-profit initiative. We are creating a system in which schools can become a part of our community and we will help them setup a Robo Sapien club in their school. The Robo Sapien club will have a dedicated lab for all their Robo Sapien programming related activities. The Robo Sapien club will have access to all the resources and mentors from our community. Students who join the Robo Sapien club will learn how to code through a series of workshops. After they become certified coders they can work on various Robo Sapien projects and get hands on experience on real world problems. All the projects and workshops will be offered to students for free or at subsidized costs. Students who pass the Robo Sapien certification exam will be able to get internships and jobs in the Robo Sapien community. The Robo Sapien club will be supervised by a dedicated teacher and a community manager. Students from our partner schools will be able to participate in the Robo Sapien events. The Robo Sapien will conduct workshops, hackathons and various events in schools and colleges. The Robo Sapien will provide students with opportunities to learn and build projects in our events and workshops. The Robo Sapien will help students publish their projects on GitHub and get a headstart in their career. The Robo Sapien will arrange internships and jobs for students in various companies. We are building a community of teachers who can teach programming to students. We are also building a network of partner schools, colleges and universities in all regions of India. Our partners will help us setup a Robo Sapien club in their school and guide us in recruiting teachers. A community of students who love to code and build amazing projects. A network of teachers who are experts in teaching programming and love to mentor students. A network of schools, colleges and universities who love to teach coding to their students. A team of community managers who will manage the Robo Sapien clubs and events. Partners who can help us achieve our mission and help us build the Robo Sapien community. We are looking for partners to help us achieve our mission and to create the Robo Sapien community. If you are a school and interested in becoming a partner school or a teacher and interested in teaching coding, or a community manager interested in managing Robo Sapien clubs, please fill the form below.", "label": "0", "is_selected": "1", "text": "The Robo Sapien is an initiative to teach students in India how to code using computer programming languages like Java, Python, C, Ruby and others."}
{"original_text": "One of the roadmap plans for quantum computers is an integration within HPC ecosystems assigning them a role of accelerators for a variety of computationally hard tasks. However, in the current state-of-the-art, the size of quantum circuits in use is not yet comparable to the one of classical circuits, and the communication protocols for large-scale quantum computations are not yet experimentally implemented. One of the ways to overcome these limitations is a hybrid HPC-QC architectures, where a classical computer is used as a \"quantum simulator\" for a quantum system of interest. One of the examples is the \"Quantum Optics Simulator\" presented at the SC18 booth (see our previous post). This architecture is used to simulate the behaviour of the quantum system using a set of classical qubits, each of which representing a part of the quantum system. This is equivalent to the real experiment where a photon emitted from a quantum source interacts with the system of interest. The probability of the photon absorption, fluorescence and transmission are used to construct the overall system response. The Quantum Optics Simulator architecture allows an access to the photon behaviour at any time during the experiment, which is not possible in a real experiment. Also, it allows a fast and efficient simulation of the photon behaviour in the system of interest for a variety of input parameters. The simulator is used in the experiment where the photon beam excites the atoms in the quantum system. This process is equivalent to a measurement of the photon behaviour. The information about the system response is then sent back to the computer for further processing. Simulation of a Quantum Optics Experiment The Quantum Optics Simulator is a software running on a classical computer, with a set of classical qubits emulating the photon behaviour. The state of each classical qubit is determined by the state of the corresponding photon. For example, if the photon is emitted at the time 1, then the qubit value is set to 1. The simulation progresses in time steps, where the value of each qubit is updated to the next time step based on the previously set value. If the photon is absorbed by the system during the simulation, its value is set to 0, and the corresponding classical qubit is not further used in the simulation. If the photon is emitted after the interaction with the system, it is reflected and transmitted to the next time step. If the photon is absorbed by the system during the simulation, its value is set to 0, and the corresponding classical qubit is not further used in the simulation. The simulation of the system response is constructed as a combination of a set of classical states. Each of these states is created by a classical circuit, which is determined by the input parameters of the simulation. In the case of the Quantum Optics Simulator, the photon behaviour is modelled by a classical circuit, where each of the classical qubits represents a photon. A classical circuit modeling the photon behaviour During the simulation, each of the classical qubits is updated from one time step to the next time step, and the circuit is applied to the set of classical qubits. This process is repeated until the end of the simulation. The overall simulation response is then determined by a combination of the results of all these circuits. The simulation response is a combination of several classical states, each of which representing a state of a part of the quantum system. The state of each classical qubit is determined by the state of the corresponding photon. For example, if the photon is emitted at the time 1, then the qubit value is set to 1. The simulation progresses in time steps, where the value of each qubit is updated to the next time step based on the previously set value. If the photon is absorbed by the system during the simulation, its value is set to 0, and the corresponding classical qubit is not further used in the simulation. If the photon is emitted after the interaction with the system, it is reflected and transmitted to the next time step. If the photon is absorbed by the system during the simulation, its value is set to 0, and the corresponding classical qubit is not further used in the simulation. The simulation of the system response is constructed as a combination of a set of classical states. Each of these states is created by a classical circuit, which is determined by the input parameters of the simulation. In the case of the Quantum Optics Simulator, the photon behaviour is modelled by a classical circuit, where each of the classical qubits represents a photon. During the simulation, each of the classical qubits is updated from one time step to the next time step, and the circuit is applied to the set of classical qubits", "label": "0", "is_selected": "1", "text": "A hybrid HPC-QC architecture, where a classical computer is used as a \"quantum simulator\" for a quantum system of interest, has been presented at the SC18 booth."}
{"original_text": "We consider channels affected by intersymbol interference with reduced-complexity, mutual information optimized, channel-shortening detection. For such settings, we optimize the transmit filter, taking into consideration the reduced receiver complexity constraint. We show that, in this case, the transmit filter is not necessarily optimal for the average mutual information. To overcome this suboptimality, we propose a novel technique for finding the channel shortening filter for the case where the transmit filter is a linear filter and the receiver filter is a single tap. We show that the optimal channel shortening filter maximizes the average mutual information over the class of channels for which the channel shortening filter is a single tap. We derive the optimal single-tap channel shortening filter and compare it with the optimal filter for the case where the receiver is assumed to be a matched filter.", "label": "0", "is_selected": "0", "text": "We consider channels affected by intersymbol interference with reduced-complexity, mutual information optimized, channel-shortening detection. For such settings, we optimize the transmit filter, taking into consideration the reduced receiver complexity constraint. We show that, in this case, the transmit filter is not necessarily optimal for the average mutual information. To overcome this suboptimality, we propose a novel technique for finding the channel shortening filter for the case where the transmit filter is a linear filter and the receiver filter is a single tap. We show that the optimal channel shortening filter maximizes the average mutual information over the class of channels for which the channel shortening filter is a single tap. We derive the optimal single-tap channel shortening filter and compare it with the optimal filter for the case where the receiver is assumed to be a matched filter."}
{"original_text": "Abundant data is the key to successful machine learning. However, supervised learning requires annotated data that are often hard to obtain. In a classification task with limited resources, Active Learning (AL) promises to guide annotators to examples that bring the most value for a classifier. AL can be successfully combined with self-training, i.e., extending a training set with the unlabelled examples for which a classifier is the most certain. We report our experiences on using AL in a systematic manner to train an SVM classifier for Stack Overflow posts discussing performance of software components. We show that the training examples deemed as the most valuable to the classifier are also the most difficult for humans to annotate. Despite carefully evolved annotation criteria, we report low inter-rater agreement, but we also propose mitigation strategies. Finally, based on one annotator's work, we show that self-training can improve the classification accuracy. We conclude the paper by discussing implication for future text miners aspiring to use AL and self-training.", "label": "1", "is_selected": "0", "text": "Abundant data is the key to successful machine learning. However, supervised learning requires annotated data that are often hard to obtain. In a classification task with limited resources, Active Learning (AL) promises to guide annotators to examples that bring the most value for a classifier. AL can be successfully combined with self-training, i.e., extending a training set with the unlabelled examples for which a classifier is the most certain. We report our experiences on using AL in a systematic manner to train an SVM classifier for Stack Overflow posts discussing performance of software components. We show that the training examples deemed as the most valuable to the classifier are also the most difficult for humans to annotate. Despite carefully evolved annotation criteria, we report low inter-rater agreement, but we also propose mitigation strategies. Finally, based on one annotator's work, we show that self-training can improve the classification accuracy. We conclude the paper by discussing implication for future text miners aspiring to use AL and self-training."}
{"original_text": "Aiming to minimize service delay, we propose a new random caching scheme in device-to-device (D2D) -assisted heterogeneous network. To support diversified viewing qualities of multimedia video services, each video file is encoded into a base layer (BL) and multiple enhancement layers (ELs) by scalable video coding (SVC). A super layer, including the BL and several ELs, is transmitted to every user. We define and quantify the service delay of multi-quality videos by deriving successful transmission probabilities when a user is served by a D2D helper, a small-cell base station (SBS) and a macro-cell base station (MBS). We formulate a delay minimization problem subject to the limited cache sizes of D2D helpers and SBSs. The structure of the optimal solutions to the problem is revealed, and then an improved standard gradient projection method is designed to effectively obtain the solutions. Both theoretical analysis and Monte-Carlo simulations validate the successful transmission probabilities. Compared with three benchmark caching policies, the proposed SVC-based random caching scheme is superior in terms of reducing the service delay.", "label": "1", "is_selected": "0", "text": "Aiming to minimize service delay, we propose a new random caching scheme in device-to-device (D2D) -assisted heterogeneous network. To support diversified viewing qualities of multimedia video services, each video file is encoded into a base layer (BL) and multiple enhancement layers (ELs) by scalable video coding (SVC). A super layer, including the BL and several ELs, is transmitted to every user. We define and quantify the service delay of multi-quality videos by deriving successful transmission probabilities when a user is served by a D2D helper, a small-cell base station (SBS) and a macro-cell base station (MBS). We formulate a delay minimization problem subject to the limited cache sizes of D2D helpers and SBSs. The structure of the optimal solutions to the problem is revealed, and then an improved standard gradient projection method is designed to effectively obtain the solutions. Both theoretical analysis and Monte-Carlo simulations validate the successful transmission probabilities. Compared with three benchmark caching policies, the proposed SVC-based random caching scheme is superior in terms of reducing the service delay."}
{"original_text": "Summary: Genome-to-genome comparisons require designating anchor points, which are given by Maximum Exact Matches (MEMs) between their sequences. For large genomes this is a challenging problem and the performance of current MEM algorithms is not satisfying, due to their exponential time complexity. The number of MEMs of two sequences is linearly dependent on their edit distance. Therefore, edit distance bounds are used to reduce the amount of computational effort. Such edit distance bounds can be estimated by employing k-mismatch arrays. If k is a multiple of 3, the computational effort is independent of the k-mismatch array construction. Unfortunately, edit distance bounds based on k-mismatch arrays are often too conservative. In this paper we present an alternative way to estimate edit distance bounds that is independent of k, resulting in more liberal edit distance bounds. We present an efficient algorithm to compute these bounds and discuss the influence of the MEMs found by our approach on the downstream sequence alignment algorithms. The experimental results show that our novel approach is more efficient than the state-of-the-art in terms of the runtime and the number of MEMs found.", "label": "0", "is_selected": "1", "text": "We present an efficient algorithm to estimate edit distance bounds independent of k-mismatch arrays, resulting in more liberal edit distance alignment algorithms for genome-to-genome comparisons."}
{"original_text": "A lot of research has been focused on secure outsourcing of biometric identification in the context of cloud computing. In such schemes, both the encrypted biometric database and the identification process are outsourced to the cloud. The ultimate goal is to protect the security and privacy of the biometric database and the query templates. Security analysis shows that previous schemes suffer from the enrolment attack and unnecessarily expose more information than needed. In this paper, we propose a new secure outsourcing scheme aims at enhancing the security from these two aspects. First, besides all the attacks discussed in previous schemes, our proposed scheme is also secure against the enrolment attack. Second, we model the identification process as a fixed radius similarity query problem instead of the kNN search problem. Such a modelling is able to reduce the exposed information thus enhancing the privacy of the biometric database. Our comprehensive security and complexity analysis show that our scheme is able to enhance the security and privacy of the biometric database and query templates while maintaining the same computational savings from outsourcing.", "label": "1", "is_selected": "0", "text": "A lot of research has been focused on secure outsourcing of biometric identification in the context of cloud computing. In such schemes, both the encrypted biometric database and the identification process are outsourced to the cloud. The ultimate goal is to protect the security and privacy of the biometric database and the query templates. Security analysis shows that previous schemes suffer from the enrolment attack and unnecessarily expose more information than needed. In this paper, we propose a new secure outsourcing scheme aims at enhancing the security from these two aspects. First, besides all the attacks discussed in previous schemes, our proposed scheme is also secure against the enrolment attack. Second, we model the identification process as a fixed radius similarity query problem instead of the kNN search problem. Such a modelling is able to reduce the exposed information thus enhancing the privacy of the biometric database. Our comprehensive security and complexity analysis show that our scheme is able to enhance the security and privacy of the biometric database and query templates while maintaining the same computational savings from outsourcing."}
{"original_text": "Understanding the formation of subjective human traits, such as preference and opinions, is an important, but poorly explored problem. An essential aspect is that traits collectively evolve under the repeated action of social influence interactions, which is the focus of many quantitative studies of cultural dynamics. In this paradigm, dynamical models require that all traits are fixed when specifying the \"initial cultural state.\" Typically, this initial state is randomly generated, from a uniform distribution over the set of possible combinations of traits. However, recent work has shown that the outcome of social influence dynamics strongly depends on the nature of the initial state: if this is sampled from empirical data instead of being generated in a uniformly random way, a higher level of cultural diversity is found after long-term dynamics, for the same level of propensity towards collective behavior in the short-term; moreover, if the initial state is obtained by shuffling the empirical traits among people, the level of long-term cultural diversity is in-between those obtained for the empirical and random counterparts. The current study repeats the analysis for multiple empirical data sets, showing that the results are remarkably similar, although the matrix of correlations between cultural variables clearly differs across data sets. This points towards robust structural properties inherent in empirical cultural states, likely due to universal laws governing the dynamics of culture in the real world. The analysis suggests, first, that this dynamics operates close to criticality and second, that it is driven by more than just social influence, implications which were not recognized previously.", "label": "1", "is_selected": "0", "text": "Understanding the formation of subjective human traits, such as preference and opinions, is an important, but poorly explored problem. An essential aspect is that traits collectively evolve under the repeated action of social influence interactions, which is the focus of many quantitative studies of cultural dynamics. In this paradigm, dynamical models require that all traits are fixed when specifying the \"initial cultural state.\" Typically, this initial state is randomly generated, from a uniform distribution over the set of possible combinations of traits. However, recent work has shown that the outcome of social influence dynamics strongly depends on the nature of the initial state: if this is sampled from empirical data instead of being generated in a uniformly random way, a higher level of cultural diversity is found after long-term dynamics, for the same level of propensity towards collective behavior in the short-term; moreover, if the initial state is obtained by shuffling the empirical traits among people, the level of long-term cultural diversity is in-between those obtained for the empirical and random counterparts. The current study repeats the analysis for multiple empirical data sets, showing that the results are remarkably similar, although the matrix of correlations between cultural variables clearly differs across data sets. This points towards robust structural properties inherent in empirical cultural states, likely due to universal laws governing the dynamics of culture in the real world. The analysis suggests, first, that this dynamics operates close to criticality and second, that it is driven by more than just social influence, implications which were not recognized previously."}
{"original_text": "We give a (2) -approximation algorithm for minimizing total weighted completion time on a single machine under release time and precedence constraints. This settles a recent conjecture made in , which was left open due to a technical gap in the proof of their (2) -approximation algorithm. Our algorithm also runs in polynomial time.", "label": "0", "is_selected": "0", "text": "We give a (2) -approximation algorithm for minimizing total weighted completion time on a single machine under release time and precedence constraints. This settles a recent conjecture made in , which was left open due to a technical gap in the proof of their (2) -approximation algorithm. Our algorithm also runs in polynomial time."}
{"original_text": "An uplink system with a single antenna transmitter and a single receiver with a large number of antennas is considered. We propose an energy-detection-based single-shot noncoherent communication scheme which does not use the instantaneous channel state information (CSI), but rather only the knowledge of the channel statistics. The suggested system uses a transmitter that modulates information on the power of the symbols, and a receiver which measures only the average energy across the antennas. We propose constellation designs which are asymptotically optimal with respect to symbol error rate (SER) with an increasing number of antennas, for any finite signal to noise ratio (SNR) at the receiver, under different assumptions on the availability of CSI statistics (exact channel fading distribution or the first few moments of the channel fading distribution). We also consider the case of imperfect knowledge of the channel statistics and describe in detail the case when there is a bounded uncertainty on the moments of the fading distribution. We present numerical results on the SER performance achieved by these designs in typical scenarios and find that they may outperform existing noncoherent constellations, e.g., conventional Amplitude Shift Keying (ASK), and pilot-based schemes, e.g., Pulse Amplitude Modulation (PAM). We also observe that an optimized constellation for a specific channel distribution makes it very sensitive to uncertainties in the channel statistics. In particular, constellation designs based on optimistic channel conditions could lead to significant performance degradation in terms of the achieved symbol error rates.", "label": "1", "is_selected": "0", "text": "An uplink system with a single antenna transmitter and a single receiver with a large number of antennas is considered. We propose an energy-detection-based single-shot noncoherent communication scheme which does not use the instantaneous channel state information (CSI), but rather only the knowledge of the channel statistics. The suggested system uses a transmitter that modulates information on the power of the symbols, and a receiver which measures only the average energy across the antennas. We propose constellation designs which are asymptotically optimal with respect to symbol error rate (SER) with an increasing number of antennas, for any finite signal to noise ratio (SNR) at the receiver, under different assumptions on the availability of CSI statistics (exact channel fading distribution or the first few moments of the channel fading distribution). We also consider the case of imperfect knowledge of the channel statistics and describe in detail the case when there is a bounded uncertainty on the moments of the fading distribution. We present numerical results on the SER performance achieved by these designs in typical scenarios and find that they may outperform existing noncoherent constellations, e.g., conventional Amplitude Shift Keying (ASK), and pilot-based schemes, e.g., Pulse Amplitude Modulation (PAM). We also observe that an optimized constellation for a specific channel distribution makes it very sensitive to uncertainties in the channel statistics. In particular, constellation designs based on optimistic channel conditions could lead to significant performance degradation in terms of the achieved symbol error rates."}
{"original_text": "We present five variants of the standard Long Short-term Memory (LSTM) recurrent neural networks by uniformly reducing blocks of adaptive parameters in the gating mechanisms. For simplicity, we refer to these variants as emph{compressed} LSTM. We apply this compression technique to six different datasets: a character-level language modeling task, a next-word prediction task on three different corpora (Enron, Penn Treebank and One Billion Words), a multilingual character-level language modeling task, and the Google Speech Commands dataset. Our experiments show that the original LSTM model is overparameterized with respect to the training data available, and the compressed variants achieve comparable or better performance. This result is especially surprising for the multilingual and Google Speech Commands datasets, which contain a much smaller amount of training data.", "label": "0", "is_selected": "1", "text": "In this paper, we present a novel compression technique for recurrent neural networks, which achieves better performance than the original model in a wide range of training datasets."}
{"original_text": "The Social Internet of Things (SIoT), integration of Internet of Things and Social networks paradigms, has been introduced to build a network of smart nodes which are capable of establishing social relationships with each other. The SIoT applications have been used in different domains like healthcare, environmental monitoring, energy management, and location-based services. However, the security and privacy of the SIoT applications is one of the major challenges. The current research work in this domain mainly focuses on the data security. This paper presents a lightweight secure communication scheme, named Randomized Physical Authentication-based Secure Clustering (RPASC), for the SIoT. The proposed scheme is based on a lightweight authentication scheme using randomized physical unclonable functions (RPUFs). The proposed scheme uses the physical properties of the devices to perform secure clustering and to prevent the malicious devices from joining the network. The RPUFs are generated randomly using the random number generator and the RFID tag. The experimentation results show that the proposed scheme has less processing overhead and higher authentication accuracy compared to the existing schemes.", "label": "0", "is_selected": "0", "text": "The Social Internet of Things (SIoT), integration of Internet of Things and Social networks paradigms, has been introduced to build a network of smart nodes which are capable of establishing social relationships with each other. The SIoT applications have been used in different domains like healthcare, environmental monitoring, energy management, and location-based services. However, the security and privacy of the SIoT applications is one of the major challenges. The current research work in this domain mainly focuses on the data security. This paper presents a lightweight secure communication scheme, named Randomized Physical Authentication-based Secure Clustering (RPASC), for the SIoT. The proposed scheme is based on a lightweight authentication scheme using randomized physical unclonable functions (RPUFs). The proposed scheme uses the physical properties of the devices to perform secure clustering and to prevent the malicious devices from joining the network. The RPUFs are generated randomly using the random number generator and the RFID tag. The experimentation results show that the proposed scheme has less processing overhead and higher authentication accuracy compared to the existing schemes."}
{"original_text": "We present a polynomial-space algorithm that computes the number of independent sets of any input graph in time O (1.1389 n) for graphs with maximum degree 3 and in time O (1.2356 n) for general graphs, where n is the number of vertices. Together with the inclusion-exclusion approach of Bjorklund, Husfeldt, and Koivisto [SIAM J. Comput. 2009], this leads to a faster polynomial-space algorithm for the graph coloring problem with running time O (2.2356 n). As a byproduct, we also obtain an exponential-space O (1.2330 n) time algorithm for counting independent sets. Our main algorithm counts independent sets in graphs with maximum degree 3 and no vertex with three neighbors of degree 3. This polynomial-space algorithm is analyzed using the recently introduced Separate, Measure and Conquer approach [Gaspers Sorkin, ICALP 2015]. Using Wahlstrom's compound measure approach, this improvement in running time for small degree graphs is then bootstrapped to larger degrees, giving the improvement for general graphs. Combining both approaches leads to some inflexibility in choosing vertices to branch on for the small-degree cases, which we counter by structural graph properties. The main complication is to upper bound the number of times the algorithm has to branch on vertices all of whose neighbors have degree 2, while still decreasing the size of the separator each time the algorithm branches.", "label": "1", "is_selected": "0", "text": "We present a polynomial-space algorithm that computes the number of independent sets of any input graph in time O (1.1389 n) for graphs with maximum degree 3 and in time O (1.2356 n) for general graphs, where n is the number of vertices. Together with the inclusion-exclusion approach of Bjorklund, Husfeldt, and Koivisto [SIAM J. Comput. 2009], this leads to a faster polynomial-space algorithm for the graph coloring problem with running time O (2.2356 n). As a byproduct, we also obtain an exponential-space O (1.2330 n) time algorithm for counting independent sets. Our main algorithm counts independent sets in graphs with maximum degree 3 and no vertex with three neighbors of degree 3. This polynomial-space algorithm is analyzed using the recently introduced Separate, Measure and Conquer approach [Gaspers Sorkin, ICALP 2015]. Using Wahlstrom's compound measure approach, this improvement in running time for small degree graphs is then bootstrapped to larger degrees, giving the improvement for general graphs. Combining both approaches leads to some inflexibility in choosing vertices to branch on for the small-degree cases, which we counter by structural graph properties. The main complication is to upper bound the number of times the algorithm has to branch on vertices all of whose neighbors have degree 2, while still decreasing the size of the separator each time the algorithm branches."}
{"original_text": "Roundabouts in conjunction with other traffic scenarios, e.g., intersections, merging roadways, speed reduction zones, can induce congestion in a transportation network due to driver responses to various disturbances. Research efforts at the University of Maryland, University College, have resulted in a validated simulation model of an urban arterial roadway with roundabouts. This model predicts flows on the network, and more importantly, captures queue formation in the network. We present a survey of the roundabouts in the National Capital Region (NCR) and study the NCR network to identify the bottleneck locations. We then simulate the network in the presence of roundabouts and study the congestion formation. We analyze the impact of the roundabouts on the travel time on the network and study the bottlenecks in the network. Syed, S.A. Chatterjee, A. (2017). Roundabouts: Impact on Urban Transportation Networks and Traffic Congestion. 2017 International Conference on Intelligent Transportation (ICIT) (pp. 438-445). Institute of Electrical and Electronics Engineers Inc..", "label": "0", "is_selected": "1", "text": "The impact of roundabouts on traffic congestion in an urban area has been studied in detail in a number of papers published in journals such as the International Journal of Intelligent Transportation (ICIT) and the Journal of Transportation Research (JTR)."}
{"original_text": "A theory explaining how deep learning works is yet to be developed. Previous work suggests that deep learning performs a coarse graining, similar in spirit to the renormalization group (RG). We show that this analogy is correct, and use the correspondence to build neural networks with desirable properties. Specifically, we show that a given architecture can be trained by choosing an RG flow to solve a certain variational problem. We illustrate the approach on MNIST and CIFAR10, where we find that a single model can perform well across different regimes. The workshop on the Physics of Machine Learning will be held on June 24-26 at the Perimeter Institute for Theoretical Physics in Waterloo, Ontario. The goal of the workshop is to bring together experts from theoretical physics and machine learning to discuss the use of concepts and tools from physics in machine learning. The workshop will consist of invited talks, contributed talks, and a poster session. If you are interested in contributing a talk or poster, please submit an abstract here. If you are interested in attending the workshop, please register here. For questions, please contact the workshop organizers at . This workshop is generously supported by the Perimeter Institute for Theoretical Physics, the Centre for the Universe and the Google DeepMind Quantum AI Team.", "label": "0", "is_selected": "1", "text": "In our work, we show that deep learning can be trained across different regimes, and that a single model can perform well across regimes. The goal of the workshop is to bring together experts from theoretical physics and machine learning to discuss the use of concepts and tools from physics in machine learning."}
{"original_text": "Forecasting stock market direction is always an amazing but challenging problem in finance. Although many popular shallow computational methods (such as Backpropagation Network and Support Vector Machine) have extensively been applied to tackle this problem, these methods are known to be time-consuming in training, difficult in parameter tuning and need lots of computational resources. However, these methods have been criticized as only suitable for classification problems. Deep Learning methods have been introduced to address the drawbacks of shallow networks in recent years. In this work, a Recurrent Neural Network with Long-Short Term Memory (LSTM) cells is proposed to address the stock market direction forecasting problem. The proposed method has three major benefits over shallow networks: the ability to deal with sequential data (in this case stock market series), the ability to automatically learn long-term dependencies in the data, and the ability to be optimized via backpropagation which is a powerful and well-known optimization technique. The performance of the proposed method is evaluated on a large dataset (the Standard Poor's 500 Index) from 2014 to 2017. The results show that the proposed method outperforms both shallow networks and other deep neural networks in terms of mean-squared error and accuracy. Phung, Dai Diem, \"Forecasting Stock Market Direction Using Recurrent Neural Networks with Long-Short Term Memory Cells\" (2017). Master's Theses. 4748.", "label": "0", "is_selected": "0", "text": "Forecasting stock market direction is always an amazing but challenging problem in finance. Although many popular shallow computational methods (such as Backpropagation Network and Support Vector Machine) have extensively been applied to tackle this problem, these methods are known to be time-consuming in training, difficult in parameter tuning and need lots of computational resources. However, these methods have been criticized as only suitable for classification problems. Deep Learning methods have been introduced to address the drawbacks of shallow networks in recent years. In this work, a Recurrent Neural Network with Long-Short Term Memory (LSTM) cells is proposed to address the stock market direction forecasting problem. The proposed method has three major benefits over shallow networks: the ability to deal with sequential data (in this case stock market series), the ability to automatically learn long-term dependencies in the data, and the ability to be optimized via backpropagation which is a powerful and well-known optimization technique. The performance of the proposed method is evaluated on a large dataset (the Standard Poor's 500 Index) from 2014 to 2017. The results show that the proposed method outperforms both shallow networks and other deep neural networks in terms of mean-squared error and accuracy. Phung, Dai Diem, \"Forecasting Stock Market Direction Using Recurrent Neural Networks with Long-Short Term Memory Cells\" (2017). Master's Theses. 4748."}
{"original_text": "Shallow Convolution Neural Network (CNN) is a time-tested tool for the information extraction from cancer pathology reports. Shallow CNN performs competitively on this task to other deep learning models including BERT, which holds the state-of-the-art for many NLP tasks. The main insight behind this eccentric phenomenon is that the information extraction from cancer pathology reports require only a small number of domain-specific text segments to perform the task, thus making the most of the texts and contexts excessive for the task. Shallow CNN model is well-suited to identify these key short text segments from the labeled training set; however, the identified text segments remain obscure to humans. In this study, we fill this gap by developing a model reduction tool to make a reliable connection between CNN filters and relevant text segments by discarding the spurious connections. We reduce the complexity of shallow CNN representation by approximating it with a linear transformation of n-gram presence representation with a non-negativity and sparsity prior on the transformation weights to obtain an interpretable model. Our approach bridge the gap between the conventionally perceived tradeoff boundary between accuracy on the one side and explainability on the other by model reduction.", "label": "1", "is_selected": "0", "text": "Shallow Convolution Neural Network (CNN) is a time-tested tool for the information extraction from cancer pathology reports. Shallow CNN performs competitively on this task to other deep learning models including BERT, which holds the state-of-the-art for many NLP tasks. The main insight behind this eccentric phenomenon is that the information extraction from cancer pathology reports require only a small number of domain-specific text segments to perform the task, thus making the most of the texts and contexts excessive for the task. Shallow CNN model is well-suited to identify these key short text segments from the labeled training set; however, the identified text segments remain obscure to humans. In this study, we fill this gap by developing a model reduction tool to make a reliable connection between CNN filters and relevant text segments by discarding the spurious connections. We reduce the complexity of shallow CNN representation by approximating it with a linear transformation of n-gram presence representation with a non-negativity and sparsity prior on the transformation weights to obtain an interpretable model. Our approach bridge the gap between the conventionally perceived tradeoff boundary between accuracy on the one side and explainability on the other by model reduction."}
{"original_text": "In the domain of emergency management during hazard crises, having sufficient situational awareness information is critical. It requires capturing and integrating information from sources such as satellite images, local sensors and social media content generated by local people. A bold obstacle to capturing, representing and integrating such heterogeneous and diverse information is lack of a proper ontology which properly conceptualizes this domain, aggregates and unifies datasets. Thus, in this paper, we introduce empathi ontology which conceptualizes the core concepts concerning with the domain of emergency managing and planning of hazard crises. Although empathi has a coarse-grained view, it considers the necessary concepts and relations being essential in this domain. This ontology is available at", "label": "1", "is_selected": "0", "text": "In the domain of emergency management during hazard crises, having sufficient situational awareness information is critical. It requires capturing and integrating information from sources such as satellite images, local sensors and social media content generated by local people. A bold obstacle to capturing, representing and integrating such heterogeneous and diverse information is lack of a proper ontology which properly conceptualizes this domain, aggregates and unifies datasets. Thus, in this paper, we introduce empathi ontology which conceptualizes the core concepts concerning with the domain of emergency managing and planning of hazard crises. Although empathi has a coarse-grained view, it considers the necessary concepts and relations being essential in this domain. This ontology is available at"}
{"original_text": "The fuzzy K -means problem is a generalization of the classical K -means problem to soft clusterings, i.e. clusterings where each points belongs to each cluster to some degree. Although it has been proved that this problem is NP-complete, a growing number of algorithms have been proposed to solve it in practice. However, there is no known polynomial time algorithm which is able to find a global minimum of this problem. In this paper we investigate the difficulty of the fuzzy K -means problem. We first prove that the fuzzy K -means problem is weakly NP-hard, and then, that it is even weakly NP-hard on the class of trees. In addition, we show that even if one assumes the clusters to be spherical, and that all distances between points and cluster centers are integers, there is still no polynomial time algorithm that is able to find a global minimum of the fuzzy K -means problem. We also prove that the problem is weakly NP-hard on this class of instances.", "label": "0", "is_selected": "1", "text": "The fuzzy K -means problem is one of the most difficult problems to solve in the field of machine learning.<n> We first prove that the fuzzy K-meansproblem is weakly NP-hard, and that it is even weakly NP -hard on the class of trees."}
{"original_text": "We show how Markov mixed membership models (MMMM) can be used to predict the degradation of assets. We model the degradation path of individual assets, to predict overall failure rates. The MMMM has the advantage that the latent class structure is shared across assets. This enables us to borrow information from similar assets to improve the accuracy of the degradation predictions for individual assets.", "label": "0", "is_selected": "0", "text": "We show how Markov mixed membership models (MMMM) can be used to predict the degradation of assets. We model the degradation path of individual assets, to predict overall failure rates. The MMMM has the advantage that the latent class structure is shared across assets. This enables us to borrow information from similar assets to improve the accuracy of the degradation predictions for individual assets."}
